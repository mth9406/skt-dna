{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPrJF9KiZYUe6QKlqCQISqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/gdrvie/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWovWtEE7h1v","executionInfo":{"status":"ok","timestamp":1666713500844,"user_tz":-540,"elapsed":18181,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"4656d635-6925-4cfe-8b0a-ada85c3ecafe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrvie/\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/gdrvie/MyDrive/SKT\")\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"zCrQG3WU7mhr","executionInfo":{"status":"ok","timestamp":1666713500845,"user_tz":-540,"elapsed":6,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"f55e98b6-5d96-41b2-9fef-21d136be5680"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/gdrvie/MyDrive/SKT'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#오류가 어디서 났는지 자세히 보여주게끔 하는 코드\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"metadata":{"id":"ZCabc_nWy9BA","executionInfo":{"status":"ok","timestamp":1666713500845,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zgrS8uvM5m5W","executionInfo":{"status":"ok","timestamp":1666713506241,"user_tz":-540,"elapsed":5400,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"outputs":[],"source":["#패키지들\n","import os\n","import sys \n","import argparse\n","import json \n","\n","import pandas as pd\n","from time import time\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","from utils.utils import * \n","from utils.torchUtils import *\n","from layers.models import *\n","from utils.dataloader import * \n"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mwx_xbyP7qe2","executionInfo":{"status":"ok","timestamp":1666713506787,"user_tz":-540,"elapsed":549,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"972271de-fc86-422b-af87-f4b5064f4edd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Oct 25 15:58:25 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   28C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["#args\n","args = argparse.Namespace(\n","    # data path\n","    data_type='skt',\n","    data_path='./data/skt',\n","    pred_steps=3,\n","    tr=0.7,\n","    val=0.2,\n","    standardize=True, #action='store_true'\n","    exclude_TA=True,  #action='store_true'\n","    lag=7,\n","    cache_file='./data/cache.pickle',\n","    # training options\n","    batch_size=2,\n","    fine_tunning_every=12, #nri multistep에 처음생김\n","    epoch=30,\n","    epoch_online=30, #nri multistep에 처음생김\n","    lr=0.001,\n","    kl_loss_penalty=0.01,\n","    patience=5,\n","    delta=0.01,\n","    print_log_option=10,\n","    verbose=True,  #action='store_true'\n","    reg_loss_penalty=1e-2,\n","    kl_weight=0.1,\n","    gradient_max_norm=5,\n","    train_ar = True, #action='store_true'이고 아마 이게 True\n","    train_online = False, #action='store_true'이고 아마 이게 False\n","    # reg_loss_penalty=1e-2,\n","    # kl_weight=0.1,\n","    # gradient_max_norm=5,\n","\n","    # model options\n","    model_path='./data/skt/multi_mtgnn',\n","    model_configs = './data/skt/multi_mtgnn/commandline_args.txt',\n","    num_blocks=20,\n","    k=1,\n","    top_k=2,\n","    embedding_dim=256,\n","    alpha=3,\n","    beta=0.5,\n","    tau=0.1,\n","    model_name='latest_checkpoint.pth.tar',\n","    n_hid_encoder=256,\n","    msg_hid=256,\n","    msg_out=256,\n","    n_hid_decoder=256,\n","    model_file='latest_checkpoint.pth.tar',\n","    model_type='proto',\n","    num_folds=1,\n","    test=False,\n","    save_result = True #action='store_true'\n","    #hard=True,\n","    # To test\n","    #test=False,   #학습하고 싶을땐 True로 바꿔\n","    #model_file='latest_checkpoint.pth.tar',\n","    #model_type='proto',\n","    #num_folds=1\n","\n",")"],"metadata":{"id":"6yjhZial7rwP","executionInfo":{"status":"ok","timestamp":1666713805092,"user_tz":-540,"elapsed":260,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"u6CuMebJ9ZCz","executionInfo":{"status":"ok","timestamp":1666713805572,"user_tz":-540,"elapsed":1,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# make a path to save a model\n","if not os.path.exists(args.model_path):\n","    print(\"Making a path to save the model...\")\n","    os.makedirs(args.model_path, exist_ok=True)\n","else:\n","    print(\"The path already exists, skip making the path...\")\n","\n","print(f'saving the commandline arguments in the path: {args.model_path}...')\n","args_file = os.path.join(args.model_path, 'commandline_args.txt')\n","with open(args_file, 'w') as f:\n","    json.dump(args.__dict__, f, indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8pJkz-Fk9ip2","executionInfo":{"status":"ok","timestamp":1666713805898,"user_tz":-540,"elapsed":2,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"38c2180f-538c-4b8f-e154-a7aecdcef821"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["The path already exists, skip making the path...\n","saving the commandline arguments in the path: ./data/skt/multi_mtgnn...\n"]}]},{"cell_type":"code","source":["def main(args):\n","    # read data\n","    print(\"Loading data...\")\n","    if args.data_type == 'skt':\n","        # load gestures-data\n","        # data는 data['train'] 이 X와 M으로 나뉘는데 X가 (enb개수, 관측시점개수,col개수)임.\n","        data = load_skt(args) if not args.exclude_TA else load_skt_without_TA(args)\n","    else:\n","        print(\"Unkown data type, data type should be \\\"skt\\\"\")\n","        sys.exit()\n","\n","    # define training, validation, test datasets and their dataloaders respectively\n","    train_data, valid_data, test_data \\\n","        = TimeSeriesDataset(*data['train'], lag= args.lag,pred_steps=args.pred_steps),\\\n","          TimeSeriesDataset(*data['valid'], lag= args.lag,pred_steps=args.pred_steps),\\\n","          TimeSeriesDataset(*data['test'], lag= args.lag,pred_steps=args.pred_steps)\n","    train_loader, valid_loader, test_loader \\\n","        = DataLoader(train_data, batch_size = args.batch_size, shuffle = False),\\\n","            DataLoader(valid_data, batch_size = args.batch_size, shuffle = False),\\\n","            DataLoader(test_data, batch_size = args.batch_size, shuffle = False)\n","\n","    print(\"Loading data done!\")\n","\n","    model = Multistep_MTGNN(\n","        num_heteros=args.num_heteros,\n","        num_ts=args.num_ts,\n","        time_lags=args.lag,\n","        num_blocks=args.num_blocks,\n","        k=args.k,\n","        embedding_dim=args.embedding_dim,\n","        device=device,\n","        alpha=args.alpha,\n","        top_k=args.top_k,\n","        pred_steps=args.pred_steps\n","    ).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), args.lr)\n","    early_stopping = EarlyStopping(\n","        patience=args.patience,\n","        delta=args.delta,\n","        path=args.model_path)\n","\n","    if args.test:\n","        model_file = os.path.join(args.model_path, args.model_file)  # .data/skt/model/latest_checkpoint.path.tar 이 되는거임.\n","        ckpt = torch.load(model_file)\n","        model.load_state_dict(ckpt['state_dict'])\n","    else:\n","        train(args, model, train_loader, valid_loader, optimizer, criterion, early_stopping, device)\n","\n","    print(f\"Testing the model...\")\n","\n","    #test  \n","    te_mse = [[] for _ in range(args.pred_steps)]; te_r2 = [[] for _ in range(args.pred_steps)]; \n","    te_mae= [[] for _ in range(args.pred_steps)]\n","    weights = []\n","    time_ellapsed = []\n","\n","    criterion_mask = nn.BCELoss()\n","    # test_loader_iter = iter(test_loader)\n","\n","    predictions = []\n","    labels = []\n","    graphs = []\n","\n","    for batch_idx, x in enumerate(test_loader): \n","        x['input'], x['mask'], x['label'], x['label_mask'] \\\n","        = x['input'].to(device), x['mask'].to(device), x['label'].to(device), x['label_mask'].to(device)\n","        \n","        # test\n","        model.eval() \n","        with torch.no_grad():\n","            out = model(x, args.beta)\n","            preds = out['preds'].detach().cpu().numpy()\n","            label = x['label'].detach().cpu().numpy()\n","\n","            for t in range(args.pred_steps):\n","                te_mse[t].append(mean_squared_error(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","                te_mae[t].append(mean_absolute_error(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","                te_r2[t].append(r2_score(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","            weights.append(len(out['preds']))\n","\n","            # record labels and predictions \n","            predictions.append(out['preds'].detach().cpu()) # bs, c, t, n\n","            labels.append(x['label'].detach().cpu()) # bs, c, t, n\n","            if out['adj_mat'] is not None: \n","                graphs.append(out['adj_mat'].detach().cpu()) # bs, c, n, n or bs, n, n\n","\n","    te_mse = np.array(te_mse)\n","    te_mae = np.array(te_mae)\n","    te_r2 = np.array(te_r2)\n","    time_ellapsed = np.array(time_ellapsed) if args.train_online else float('nan')\n","   \n","    te_mse_mean = np.average(te_mse, weights= weights, axis= 1)\n","    te_r2_mean  = np.average(te_r2, weights= weights, axis= 1)\n","    te_mae_mean  = np.average(te_mae, weights=weights, axis= 1)\n","    time_ellapsed_mean = np.average(time_ellapsed, weights=weights) if args.train_online else float('nan')\n","\n","    te_mse_std = np.average((te_mse-te_mse_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    te_r2_std = np.average((te_r2-te_r2_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    te_mae_std = np.average((te_mae-te_mae_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    time_ellapsed_std = np.average((time_ellapsed-time_ellapsed_mean)**2, weights=weights) if args.train_online else float('nan')\n","    \n","    perf = {}\n","    for t in range(args.pred_steps):\n","        perf[f'r2_{t}'] = [te_r2_mean[t]]\n","        perf[f'mae_{t}'] = [te_mae_mean[t]]\n","        perf[f'mse_{t}'] = [te_mse_mean[t]]\n","        perf[f'r2_std_{t}'] = [te_r2_std[t]]\n","        perf[f'mae_std_{t}'] = [te_mae_std[t]]\n","        perf[f'te_mse_std_{t}'] = [te_mse_std[t]]\n","    perf['mean_fine_tunning_time'] = [time_ellapsed_mean]\n","    perf['std_fine_tunning_time'] = [time_ellapsed_std]\n","\n","    print(perf)\n","\n","    if args.save_result: \n","        \n","        print('saving the predictions...')\n","\n","        predictions = torch.concat(predictions, dim=0) # num_obs, num_cells, preds_steps, num_time_series\n","        labels = torch.concat(labels, dim=0) # num_obs, num_cells, preds_steps, num_time_series   \n","\n","        #예측값 저장 및 figure 저장\n","\n","        for t in range(args.pred_steps):\n","            p = torch.permute(predictions[:,:,t, :], (1, 0, 2)) # num_cells, num_obs, num_time_series \n","            p = p.numpy()\n","            if args.cache is not None: \n","                # preds = inv_min_max_scaler(preds, args.cache, args.columns)\n","                p = inv_min_max_scaler_ver2(p, args.cache, args.columns)\n","\n","            l = torch.permute(labels[:,:,t, :], (1, 0, 2)) # num_cells, num_obs, num_time_series\n","            l = l.numpy()\n","            num_cells = l.shape[0]\n","            if args.cache is not None: \n","                # labels = inv_min_max_scaler(labels, args.cache, args.columns)\n","                l = inv_min_max_scaler_ver2(l, args.cache, args.columns)\n","        \n","            # saving figures: predictions vs labels\n","            for i in tqdm(range(num_cells), total= num_cells):\n","                enb_id = args.decoder.get(i)\n","                write_csv(args, f'test/predictions_{t}_step', f'predictions_{enb_id}.csv', p[i, ...], args.columns)\n","                write_csv(args, f'test/labels_{t}_step', f'labels_{enb_id}.csv', l[i, ...], args.columns)   \n","                \n","                fig, axes = plt.subplots(len(args.columns), 1, figsize= (10,3*len(args.columns)))\n","\n","                for j in range(len(args.columns)):\n","                    col_name = args.columns[j]\n","                    fig.axes[j].set_title(f'time-seris plot: {col_name}')\n","                    fig.axes[j].plot(p[i,:,j], label= 'prediction')\n","                    fig.axes[j].plot(l[i,:,j], label= 'label')\n","                    fig.axes[j].legend()\n","                \n","                fig.suptitle(f\"Prediction and True label plot of {enb_id}\", fontsize=20, position= (0.5, 1.0+0.05))\n","                fig.tight_layout()\n","                # make a path to save a figures \n","                fig_path = os.path.join(args.model_path, f'test/figures/{t}_step')\n","                if not os.path.exists(fig_path):\n","                    # print(\"Making a path to save figures...\")\n","                    print(f\"{fig_path}\")\n","                    os.makedirs(fig_path, exist_ok= True)\n","                # else:\n","                #     print(\"The path to save figures already exists, skip making the path...\")\n","                fig_file = os.path.join(fig_path, f'figure_{enb_id}.png')\n","                fig.savefig(fig_file)\n","                plt.close('all')\n","\n","    return perf \n","\n","\n","\n"],"metadata":{"id":"lv4Gwkj7JTy7","executionInfo":{"status":"ok","timestamp":1666713806366,"user_tz":-540,"elapsed":1,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# #MTGNN hyper parameters\n","# num_blocks = [5,15,20,30,40]\n","# ks = [1,2]\n","# top_ks = [1,2,3,4,5]\n","# lags = [7,12,24,36,48]\n","# betas = [0.1,0.5,1.0]"],"metadata":{"id":"ZZwHfBOv09K2","executionInfo":{"status":"ok","timestamp":1666713806838,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#single step의 optimal hyper parameter로 결과 내기\n","print(f'saving the commandline arguments in the path: {args.model_path}...')\n","args_file = os.path.join(args.model_path, 'commandline_args.txt')\n","with open(args_file, 'w') as f:\n","    json.dump(args.__dict__, f, indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTqmZUzI3PKE","executionInfo":{"status":"ok","timestamp":1666713807158,"user_tz":-540,"elapsed":12,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"5524efc4-c682-4927-e986-05cf7444116e"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["saving the commandline arguments in the path: ./data/skt/multi_mtgnn...\n"]}]},{"cell_type":"code","source":["perf = main(args)\n","print(\"Test done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HEi8SH9JVRa","executionInfo":{"status":"ok","timestamp":1666749876086,"user_tz":-540,"elapsed":25190843,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"4e05d433-3eae-4732-8209-985c3162e6a4"},"execution_count":19,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading data...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 306/306 [00:02<00:00, 102.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["the shape of X       : (306, 2293, 9)\n","Loading data done!\n","Start training...\n","Epoch [1/30] Batch [10/798]:                     loss = 2273.134765625\n","Epoch [1/30] Batch [20/798]:                     loss = 1873.1575927734375\n","Epoch [1/30] Batch [30/798]:                     loss = 1771.9208984375\n","Epoch [1/30] Batch [40/798]:                     loss = 1733.7275390625\n","Epoch [1/30] Batch [50/798]:                     loss = 2533.9296875\n","Epoch [1/30] Batch [60/798]:                     loss = 3903.59814453125\n","Epoch [1/30] Batch [70/798]:                     loss = 5935.75146484375\n","Epoch [1/30] Batch [80/798]:                     loss = 6422.2783203125\n","Epoch [1/30] Batch [90/798]:                     loss = 6901.7275390625\n","Epoch [1/30] Batch [100/798]:                     loss = 6155.05712890625\n","Epoch [1/30] Batch [110/798]:                     loss = 5256.9111328125\n","Epoch [1/30] Batch [120/798]:                     loss = 3826.41259765625\n","Epoch [1/30] Batch [130/798]:                     loss = 3172.4794921875\n","Epoch [1/30] Batch [140/798]:                     loss = 2176.192626953125\n","Epoch [1/30] Batch [150/798]:                     loss = 2016.1142578125\n","Epoch [1/30] Batch [160/798]:                     loss = 1820.822021484375\n","Epoch [1/30] Batch [170/798]:                     loss = 1574.3092041015625\n","Epoch [1/30] Batch [180/798]:                     loss = 1454.5291748046875\n","Epoch [1/30] Batch [190/798]:                     loss = 1662.6514892578125\n","Epoch [1/30] Batch [200/798]:                     loss = 2322.37744140625\n","Epoch [1/30] Batch [210/798]:                     loss = 3398.22607421875\n","Epoch [1/30] Batch [220/798]:                     loss = 4484.7998046875\n","Epoch [1/30] Batch [230/798]:                     loss = 4714.8359375\n","Epoch [1/30] Batch [240/798]:                     loss = 4695.94091796875\n","Epoch [1/30] Batch [250/798]:                     loss = 4170.27392578125\n","Epoch [1/30] Batch [260/798]:                     loss = 3126.986328125\n","Epoch [1/30] Batch [270/798]:                     loss = 2483.009521484375\n","Epoch [1/30] Batch [280/798]:                     loss = 1912.00244140625\n","Epoch [1/30] Batch [290/798]:                     loss = 1832.1912841796875\n","Epoch [1/30] Batch [300/798]:                     loss = 1624.43701171875\n","Epoch [1/30] Batch [310/798]:                     loss = 1504.2294921875\n","Epoch [1/30] Batch [320/798]:                     loss = 1697.9927978515625\n","Epoch [1/30] Batch [330/798]:                     loss = 2553.0537109375\n","Epoch [1/30] Batch [340/798]:                     loss = 6153.7099609375\n","Epoch [1/30] Batch [350/798]:                     loss = 7883.3896484375\n","Epoch [1/30] Batch [360/798]:                     loss = 9368.302734375\n","Epoch [1/30] Batch [370/798]:                     loss = 10095.296875\n","Epoch [1/30] Batch [380/798]:                     loss = 9855.6845703125\n","Epoch [1/30] Batch [390/798]:                     loss = 8709.712890625\n","Epoch [1/30] Batch [400/798]:                     loss = 7182.94482421875\n","Epoch [1/30] Batch [410/798]:                     loss = 4646.20361328125\n","Epoch [1/30] Batch [420/798]:                     loss = 3234.228271484375\n","Epoch [1/30] Batch [430/798]:                     loss = 2547.091552734375\n","Epoch [1/30] Batch [440/798]:                     loss = 1945.317626953125\n","Epoch [1/30] Batch [450/798]:                     loss = 1749.7589111328125\n","Epoch [1/30] Batch [460/798]:                     loss = 1668.23974609375\n","Epoch [1/30] Batch [470/798]:                     loss = 1950.8736572265625\n","Epoch [1/30] Batch [480/798]:                     loss = 4768.99267578125\n","Epoch [1/30] Batch [490/798]:                     loss = 7614.71337890625\n","Epoch [1/30] Batch [500/798]:                     loss = 9470.5341796875\n","Epoch [1/30] Batch [510/798]:                     loss = 9246.0615234375\n","Epoch [1/30] Batch [520/798]:                     loss = 9973.904296875\n","Epoch [1/30] Batch [530/798]:                     loss = 8920.7099609375\n","Epoch [1/30] Batch [540/798]:                     loss = 7635.5859375\n","Epoch [1/30] Batch [550/798]:                     loss = 5007.42578125\n","Epoch [1/30] Batch [560/798]:                     loss = 3455.888671875\n","Epoch [1/30] Batch [570/798]:                     loss = 2372.75830078125\n","Epoch [1/30] Batch [580/798]:                     loss = 2180.04736328125\n","Epoch [1/30] Batch [590/798]:                     loss = 1831.064453125\n","Epoch [1/30] Batch [600/798]:                     loss = 1613.256591796875\n","Epoch [1/30] Batch [610/798]:                     loss = 1461.531494140625\n","Epoch [1/30] Batch [620/798]:                     loss = 1562.3970947265625\n","Epoch [1/30] Batch [630/798]:                     loss = 2097.333740234375\n","Epoch [1/30] Batch [640/798]:                     loss = 2984.773681640625\n","Epoch [1/30] Batch [650/798]:                     loss = 3590.60791015625\n","Epoch [1/30] Batch [660/798]:                     loss = 3918.822265625\n","Epoch [1/30] Batch [670/798]:                     loss = 4059.154296875\n","Epoch [1/30] Batch [680/798]:                     loss = 3846.383544921875\n","Epoch [1/30] Batch [690/798]:                     loss = 3152.8564453125\n","Epoch [1/30] Batch [700/798]:                     loss = 2625.248046875\n","Epoch [1/30] Batch [710/798]:                     loss = 1951.6214599609375\n","Epoch [1/30] Batch [720/798]:                     loss = 1895.23779296875\n","Epoch [1/30] Batch [730/798]:                     loss = 1667.0181884765625\n","Epoch [1/30] Batch [740/798]:                     loss = 1526.62548828125\n","Epoch [1/30] Batch [750/798]:                     loss = 1549.2796630859375\n","Epoch [1/30] Batch [760/798]:                     loss = 2062.726318359375\n","Epoch [1/30] Batch [770/798]:                     loss = 5109.38330078125\n","Epoch [1/30] Batch [780/798]:                     loss = 5468.1484375\n","Epoch [1/30] Batch [790/798]:                     loss = 6481.94873046875\n","Epoch [1/30]: training loss= 3903.299091, training mse loss= 3903.299091, training bce loss= nan\n","            : validation loss= 3426.184088, validation mse loss= 3426.184088, validation bce loss= nan\n","Epoch [2/30] Batch [10/798]:                     loss = 2271.10546875\n","Epoch [2/30] Batch [20/798]:                     loss = 1872.9910888671875\n","Epoch [2/30] Batch [30/798]:                     loss = 1771.89501953125\n","Epoch [2/30] Batch [40/798]:                     loss = 1733.72998046875\n","Epoch [2/30] Batch [50/798]:                     loss = 2533.942138671875\n","Epoch [2/30] Batch [60/798]:                     loss = 3903.595947265625\n","Epoch [2/30] Batch [70/798]:                     loss = 5935.6328125\n","Epoch [2/30] Batch [80/798]:                     loss = 6422.26123046875\n","Epoch [2/30] Batch [90/798]:                     loss = 6901.7119140625\n","Epoch [2/30] Batch [100/798]:                     loss = 6155.04052734375\n","Epoch [2/30] Batch [110/798]:                     loss = 5256.90234375\n","Epoch [2/30] Batch [120/798]:                     loss = 3826.4091796875\n","Epoch [2/30] Batch [130/798]:                     loss = 3172.4775390625\n","Epoch [2/30] Batch [140/798]:                     loss = 2176.190185546875\n","Epoch [2/30] Batch [150/798]:                     loss = 2016.118408203125\n","Epoch [2/30] Batch [160/798]:                     loss = 1820.8240966796875\n","Epoch [2/30] Batch [170/798]:                     loss = 1574.315185546875\n","Epoch [2/30] Batch [180/798]:                     loss = 1454.537109375\n","Epoch [2/30] Batch [190/798]:                     loss = 1662.6519775390625\n","Epoch [2/30] Batch [200/798]:                     loss = 2322.373779296875\n","Epoch [2/30] Batch [210/798]:                     loss = 3398.219482421875\n","Epoch [2/30] Batch [220/798]:                     loss = 4484.79248046875\n","Epoch [2/30] Batch [230/798]:                     loss = 4714.83203125\n","Epoch [2/30] Batch [240/798]:                     loss = 4695.93701171875\n","Epoch [2/30] Batch [250/798]:                     loss = 4170.26904296875\n","Epoch [2/30] Batch [260/798]:                     loss = 3126.983154296875\n","Epoch [2/30] Batch [270/798]:                     loss = 2483.009033203125\n","Epoch [2/30] Batch [280/798]:                     loss = 1912.005615234375\n","Epoch [2/30] Batch [290/798]:                     loss = 1832.1939697265625\n","Epoch [2/30] Batch [300/798]:                     loss = 1624.438232421875\n","Epoch [2/30] Batch [310/798]:                     loss = 1504.22705078125\n","Epoch [2/30] Batch [320/798]:                     loss = 1697.995849609375\n","Epoch [2/30] Batch [330/798]:                     loss = 2553.068603515625\n","Epoch [2/30] Batch [340/798]:                     loss = 6153.70751953125\n","Epoch [2/30] Batch [350/798]:                     loss = 7883.38525390625\n","Epoch [2/30] Batch [360/798]:                     loss = 9368.294921875\n","Epoch [2/30] Batch [370/798]:                     loss = 10095.29296875\n","Epoch [2/30] Batch [380/798]:                     loss = 9855.6826171875\n","Epoch [2/30] Batch [390/798]:                     loss = 8709.708984375\n","Epoch [2/30] Batch [400/798]:                     loss = 7182.9423828125\n","Epoch [2/30] Batch [410/798]:                     loss = 4646.201171875\n","Epoch [2/30] Batch [420/798]:                     loss = 3234.224853515625\n","Epoch [2/30] Batch [430/798]:                     loss = 2547.0927734375\n","Epoch [2/30] Batch [440/798]:                     loss = 1945.3114013671875\n","Epoch [2/30] Batch [450/798]:                     loss = 1749.7611083984375\n","Epoch [2/30] Batch [460/798]:                     loss = 1668.2423095703125\n","Epoch [2/30] Batch [470/798]:                     loss = 1950.8721923828125\n","Epoch [2/30] Batch [480/798]:                     loss = 4768.9951171875\n","Epoch [2/30] Batch [490/798]:                     loss = 7614.71484375\n","Epoch [2/30] Batch [500/798]:                     loss = 9470.53125\n","Epoch [2/30] Batch [510/798]:                     loss = 9246.0615234375\n","Epoch [2/30] Batch [520/798]:                     loss = 9973.9052734375\n","Epoch [2/30] Batch [530/798]:                     loss = 8920.7099609375\n","Epoch [2/30] Batch [540/798]:                     loss = 7635.58740234375\n","Epoch [2/30] Batch [550/798]:                     loss = 5007.42822265625\n","Epoch [2/30] Batch [560/798]:                     loss = 3455.88720703125\n","Epoch [2/30] Batch [570/798]:                     loss = 2372.759033203125\n","Epoch [2/30] Batch [580/798]:                     loss = 2180.049072265625\n","Epoch [2/30] Batch [590/798]:                     loss = 1831.063720703125\n","Epoch [2/30] Batch [600/798]:                     loss = 1613.2576904296875\n","Epoch [2/30] Batch [610/798]:                     loss = 1461.5367431640625\n","Epoch [2/30] Batch [620/798]:                     loss = 1562.398193359375\n","Epoch [2/30] Batch [630/798]:                     loss = 2097.33447265625\n","Epoch [2/30] Batch [640/798]:                     loss = 2984.777587890625\n","Epoch [2/30] Batch [650/798]:                     loss = 3590.607666015625\n","Epoch [2/30] Batch [660/798]:                     loss = 3918.822265625\n","Epoch [2/30] Batch [670/798]:                     loss = 4059.154296875\n","Epoch [2/30] Batch [680/798]:                     loss = 3846.383544921875\n","Epoch [2/30] Batch [690/798]:                     loss = 3152.857421875\n","Epoch [2/30] Batch [700/798]:                     loss = 2625.247802734375\n","Epoch [2/30] Batch [710/798]:                     loss = 1951.6214599609375\n","Epoch [2/30] Batch [720/798]:                     loss = 1895.23876953125\n","Epoch [2/30] Batch [730/798]:                     loss = 1667.016845703125\n","Epoch [2/30] Batch [740/798]:                     loss = 1526.627197265625\n","Epoch [2/30] Batch [750/798]:                     loss = 1549.2808837890625\n","Epoch [2/30] Batch [760/798]:                     loss = 2062.723876953125\n","Epoch [2/30] Batch [770/798]:                     loss = 5109.38525390625\n","Epoch [2/30] Batch [780/798]:                     loss = 5468.1494140625\n","Epoch [2/30] Batch [790/798]:                     loss = 6481.94921875\n","Epoch [2/30]: training loss= 3903.130144, training mse loss= 3903.130144, training bce loss= nan\n","            : validation loss= 3426.162390, validation mse loss= 3426.162390, validation bce loss= nan\n","Epoch [3/30] Batch [10/798]:                     loss = 2271.043212890625\n","Epoch [3/30] Batch [20/798]:                     loss = 1872.9566650390625\n","Epoch [3/30] Batch [30/798]:                     loss = 1771.8646240234375\n","Epoch [3/30] Batch [40/798]:                     loss = 1733.7042236328125\n","Epoch [3/30] Batch [50/798]:                     loss = 2533.926025390625\n","Epoch [3/30] Batch [60/798]:                     loss = 3903.58154296875\n","Epoch [3/30] Batch [70/798]:                     loss = 5935.61767578125\n","Epoch [3/30] Batch [80/798]:                     loss = 6422.25341796875\n","Epoch [3/30] Batch [90/798]:                     loss = 6901.7041015625\n","Epoch [3/30] Batch [100/798]:                     loss = 6155.03076171875\n","Epoch [3/30] Batch [110/798]:                     loss = 5256.89794921875\n","Epoch [3/30] Batch [120/798]:                     loss = 3826.40478515625\n","Epoch [3/30] Batch [130/798]:                     loss = 3172.47412109375\n","Epoch [3/30] Batch [140/798]:                     loss = 2176.1875\n","Epoch [3/30] Batch [150/798]:                     loss = 2016.113037109375\n","Epoch [3/30] Batch [160/798]:                     loss = 1820.8226318359375\n","Epoch [3/30] Batch [170/798]:                     loss = 1574.3101806640625\n","Epoch [3/30] Batch [180/798]:                     loss = 1454.5303955078125\n","Epoch [3/30] Batch [190/798]:                     loss = 1662.6541748046875\n","Epoch [3/30] Batch [200/798]:                     loss = 2322.373779296875\n","Epoch [3/30] Batch [210/798]:                     loss = 3398.220703125\n","Epoch [3/30] Batch [220/798]:                     loss = 4484.7919921875\n","Epoch [3/30] Batch [230/798]:                     loss = 4714.83251953125\n","Epoch [3/30] Batch [240/798]:                     loss = 4695.93701171875\n","Epoch [3/30] Batch [250/798]:                     loss = 4170.27197265625\n","Epoch [3/30] Batch [260/798]:                     loss = 3126.986328125\n","Epoch [3/30] Batch [270/798]:                     loss = 2483.012451171875\n","Epoch [3/30] Batch [280/798]:                     loss = 1912.00390625\n","Epoch [3/30] Batch [290/798]:                     loss = 1832.1827392578125\n","Epoch [3/30] Batch [300/798]:                     loss = 1624.436767578125\n","Epoch [3/30] Batch [310/798]:                     loss = 1504.2298583984375\n","Epoch [3/30] Batch [320/798]:                     loss = 1698.0\n","Epoch [3/30] Batch [330/798]:                     loss = 2553.061767578125\n","Epoch [3/30] Batch [340/798]:                     loss = 6153.708984375\n","Epoch [3/30] Batch [350/798]:                     loss = 7883.39013671875\n","Epoch [3/30] Batch [360/798]:                     loss = 9368.296875\n","Epoch [3/30] Batch [370/798]:                     loss = 10095.2958984375\n","Epoch [3/30] Batch [380/798]:                     loss = 9855.6826171875\n","Epoch [3/30] Batch [390/798]:                     loss = 8709.7109375\n","Epoch [3/30] Batch [400/798]:                     loss = 7182.9443359375\n","Epoch [3/30] Batch [410/798]:                     loss = 4646.2021484375\n","Epoch [3/30] Batch [420/798]:                     loss = 3234.2294921875\n","Epoch [3/30] Batch [430/798]:                     loss = 2547.091552734375\n","Epoch [3/30] Batch [440/798]:                     loss = 1945.312744140625\n","Epoch [3/30] Batch [450/798]:                     loss = 1749.760498046875\n","Epoch [3/30] Batch [460/798]:                     loss = 1668.2454833984375\n","Epoch [3/30] Batch [470/798]:                     loss = 1950.8717041015625\n","Epoch [3/30] Batch [480/798]:                     loss = 4768.99755859375\n","Epoch [3/30] Batch [490/798]:                     loss = 7614.71337890625\n","Epoch [3/30] Batch [500/798]:                     loss = 9470.53125\n","Epoch [3/30] Batch [510/798]:                     loss = 9246.0595703125\n","Epoch [3/30] Batch [520/798]:                     loss = 9973.904296875\n","Epoch [3/30] Batch [530/798]:                     loss = 8920.7119140625\n","Epoch [3/30] Batch [540/798]:                     loss = 7635.57958984375\n","Epoch [3/30] Batch [550/798]:                     loss = 5007.427734375\n","Epoch [3/30] Batch [560/798]:                     loss = 3455.886962890625\n","Epoch [3/30] Batch [570/798]:                     loss = 2372.7607421875\n","Epoch [3/30] Batch [580/798]:                     loss = 2180.050537109375\n","Epoch [3/30] Batch [590/798]:                     loss = 1831.067138671875\n","Epoch [3/30] Batch [600/798]:                     loss = 1613.25732421875\n","Epoch [3/30] Batch [610/798]:                     loss = 1461.5343017578125\n","Epoch [3/30] Batch [620/798]:                     loss = 1562.397705078125\n","Epoch [3/30] Batch [630/798]:                     loss = 2097.3408203125\n","Epoch [3/30] Batch [640/798]:                     loss = 2984.775146484375\n","Epoch [3/30] Batch [650/798]:                     loss = 3590.60986328125\n","Epoch [3/30] Batch [660/798]:                     loss = 3918.822265625\n","Epoch [3/30] Batch [670/798]:                     loss = 4059.15478515625\n","Epoch [3/30] Batch [680/798]:                     loss = 3846.384765625\n","Epoch [3/30] Batch [690/798]:                     loss = 3152.856689453125\n","Epoch [3/30] Batch [700/798]:                     loss = 2625.248779296875\n","Epoch [3/30] Batch [710/798]:                     loss = 1951.6241455078125\n","Epoch [3/30] Batch [720/798]:                     loss = 1895.2406005859375\n","Epoch [3/30] Batch [730/798]:                     loss = 1667.0179443359375\n","Epoch [3/30] Batch [740/798]:                     loss = 1526.6290283203125\n","Epoch [3/30] Batch [750/798]:                     loss = 1549.2894287109375\n","Epoch [3/30] Batch [760/798]:                     loss = 2062.73681640625\n","Epoch [3/30] Batch [770/798]:                     loss = 5109.38525390625\n","Epoch [3/30] Batch [780/798]:                     loss = 5468.14697265625\n","Epoch [3/30] Batch [790/798]:                     loss = 6481.947265625\n","Epoch [3/30]: training loss= 3903.127085, training mse loss= 3903.127085, training bce loss= nan\n","            : validation loss= 3426.159122, validation mse loss= 3426.159122, validation bce loss= nan\n","Epoch [4/30] Batch [10/798]:                     loss = 2271.043701171875\n","Epoch [4/30] Batch [20/798]:                     loss = 1872.954833984375\n","Epoch [4/30] Batch [30/798]:                     loss = 1771.8648681640625\n","Epoch [4/30] Batch [40/798]:                     loss = 1733.705322265625\n","Epoch [4/30] Batch [50/798]:                     loss = 2533.923828125\n","Epoch [4/30] Batch [60/798]:                     loss = 3903.582763671875\n","Epoch [4/30] Batch [70/798]:                     loss = 5935.6181640625\n","Epoch [4/30] Batch [80/798]:                     loss = 6422.2490234375\n","Epoch [4/30] Batch [90/798]:                     loss = 6901.70263671875\n","Epoch [4/30] Batch [100/798]:                     loss = 6155.029296875\n","Epoch [4/30] Batch [110/798]:                     loss = 5256.89794921875\n","Epoch [4/30] Batch [120/798]:                     loss = 3826.404296875\n","Epoch [4/30] Batch [130/798]:                     loss = 3172.47412109375\n","Epoch [4/30] Batch [140/798]:                     loss = 2176.186279296875\n","Epoch [4/30] Batch [150/798]:                     loss = 2016.1142578125\n","Epoch [4/30] Batch [160/798]:                     loss = 1820.821044921875\n","Epoch [4/30] Batch [170/798]:                     loss = 1574.3094482421875\n","Epoch [4/30] Batch [180/798]:                     loss = 1454.5301513671875\n","Epoch [4/30] Batch [190/798]:                     loss = 1662.6524658203125\n","Epoch [4/30] Batch [200/798]:                     loss = 2322.37255859375\n","Epoch [4/30] Batch [210/798]:                     loss = 3398.217529296875\n","Epoch [4/30] Batch [220/798]:                     loss = 4484.78955078125\n","Epoch [4/30] Batch [230/798]:                     loss = 4714.83056640625\n","Epoch [4/30] Batch [240/798]:                     loss = 4695.93798828125\n","Epoch [4/30] Batch [250/798]:                     loss = 4170.2705078125\n","Epoch [4/30] Batch [260/798]:                     loss = 3126.985107421875\n","Epoch [4/30] Batch [270/798]:                     loss = 2483.010986328125\n","Epoch [4/30] Batch [280/798]:                     loss = 1912.0028076171875\n","Epoch [4/30] Batch [290/798]:                     loss = 1832.19775390625\n","Epoch [4/30] Batch [300/798]:                     loss = 1624.439453125\n","Epoch [4/30] Batch [310/798]:                     loss = 1504.22900390625\n","Epoch [4/30] Batch [320/798]:                     loss = 1697.9959716796875\n","Epoch [4/30] Batch [330/798]:                     loss = 2553.055419921875\n","Epoch [4/30] Batch [340/798]:                     loss = 6153.70361328125\n","Epoch [4/30] Batch [350/798]:                     loss = 7883.38330078125\n","Epoch [4/30] Batch [360/798]:                     loss = 9368.2998046875\n","Epoch [4/30] Batch [370/798]:                     loss = 10095.29296875\n","Epoch [4/30] Batch [380/798]:                     loss = 9855.6806640625\n","Epoch [4/30] Batch [390/798]:                     loss = 8709.708984375\n","Epoch [4/30] Batch [400/798]:                     loss = 7182.9453125\n","Epoch [4/30] Batch [410/798]:                     loss = 4646.201171875\n","Epoch [4/30] Batch [420/798]:                     loss = 3234.227783203125\n","Epoch [4/30] Batch [430/798]:                     loss = 2547.08984375\n","Epoch [4/30] Batch [440/798]:                     loss = 1945.30859375\n","Epoch [4/30] Batch [450/798]:                     loss = 1749.757568359375\n","Epoch [4/30] Batch [460/798]:                     loss = 1668.2386474609375\n","Epoch [4/30] Batch [470/798]:                     loss = 1950.8724365234375\n","Epoch [4/30] Batch [480/798]:                     loss = 4768.99853515625\n","Epoch [4/30] Batch [490/798]:                     loss = 7614.71337890625\n","Epoch [4/30] Batch [500/798]:                     loss = 9470.529296875\n","Epoch [4/30] Batch [510/798]:                     loss = 9246.05859375\n","Epoch [4/30] Batch [520/798]:                     loss = 9973.9033203125\n","Epoch [4/30] Batch [530/798]:                     loss = 8920.708984375\n","Epoch [4/30] Batch [540/798]:                     loss = 7635.58056640625\n","Epoch [4/30] Batch [550/798]:                     loss = 5007.42626953125\n","Epoch [4/30] Batch [560/798]:                     loss = 3455.88525390625\n","Epoch [4/30] Batch [570/798]:                     loss = 2372.759033203125\n","Epoch [4/30] Batch [580/798]:                     loss = 2180.044677734375\n","Epoch [4/30] Batch [590/798]:                     loss = 1831.0645751953125\n","Epoch [4/30] Batch [600/798]:                     loss = 1613.254638671875\n","Epoch [4/30] Batch [610/798]:                     loss = 1461.540771484375\n","Epoch [4/30] Batch [620/798]:                     loss = 1562.4012451171875\n","Epoch [4/30] Batch [630/798]:                     loss = 2097.337158203125\n","Epoch [4/30] Batch [640/798]:                     loss = 2984.774658203125\n","Epoch [4/30] Batch [650/798]:                     loss = 3590.607177734375\n","Epoch [4/30] Batch [660/798]:                     loss = 3918.821533203125\n","Epoch [4/30] Batch [670/798]:                     loss = 4059.1552734375\n","Epoch [4/30] Batch [680/798]:                     loss = 3846.384521484375\n","Epoch [4/30] Batch [690/798]:                     loss = 3152.856689453125\n","Epoch [4/30] Batch [700/798]:                     loss = 2625.247802734375\n","Epoch [4/30] Batch [710/798]:                     loss = 1951.6217041015625\n","Epoch [4/30] Batch [720/798]:                     loss = 1895.238037109375\n","Epoch [4/30] Batch [730/798]:                     loss = 1667.0203857421875\n","Epoch [4/30] Batch [740/798]:                     loss = 1526.630859375\n","Epoch [4/30] Batch [750/798]:                     loss = 1549.284423828125\n","Epoch [4/30] Batch [760/798]:                     loss = 2062.728759765625\n","Epoch [4/30] Batch [770/798]:                     loss = 5109.38232421875\n","Epoch [4/30] Batch [780/798]:                     loss = 5468.146484375\n","Epoch [4/30] Batch [790/798]:                     loss = 6481.9482421875\n","Epoch [4/30]: training loss= 3903.126238, training mse loss= 3903.126238, training bce loss= nan\n","            : validation loss= 3426.158684, validation mse loss= 3426.158684, validation bce loss= nan\n","Epoch [5/30] Batch [10/798]:                     loss = 2271.04296875\n","Epoch [5/30] Batch [20/798]:                     loss = 1872.9547119140625\n","Epoch [5/30] Batch [30/798]:                     loss = 1771.86328125\n","Epoch [5/30] Batch [40/798]:                     loss = 1733.707275390625\n","Epoch [5/30] Batch [50/798]:                     loss = 2533.924560546875\n","Epoch [5/30] Batch [60/798]:                     loss = 3903.581298828125\n","Epoch [5/30] Batch [70/798]:                     loss = 5935.61474609375\n","Epoch [5/30] Batch [80/798]:                     loss = 6422.248046875\n","Epoch [5/30] Batch [90/798]:                     loss = 6901.70263671875\n","Epoch [5/30] Batch [100/798]:                     loss = 6155.02783203125\n","Epoch [5/30] Batch [110/798]:                     loss = 5256.8974609375\n","Epoch [5/30] Batch [120/798]:                     loss = 3826.40380859375\n","Epoch [5/30] Batch [130/798]:                     loss = 3172.472900390625\n","Epoch [5/30] Batch [140/798]:                     loss = 2176.18701171875\n","Epoch [5/30] Batch [150/798]:                     loss = 2016.113037109375\n","Epoch [5/30] Batch [160/798]:                     loss = 1820.8189697265625\n","Epoch [5/30] Batch [170/798]:                     loss = 1574.312744140625\n","Epoch [5/30] Batch [180/798]:                     loss = 1454.5303955078125\n","Epoch [5/30] Batch [190/798]:                     loss = 1662.650146484375\n","Epoch [5/30] Batch [200/798]:                     loss = 2322.3720703125\n","Epoch [5/30] Batch [210/798]:                     loss = 3398.21728515625\n","Epoch [5/30] Batch [220/798]:                     loss = 4484.78955078125\n","Epoch [5/30] Batch [230/798]:                     loss = 4714.83056640625\n","Epoch [5/30] Batch [240/798]:                     loss = 4695.93505859375\n","Epoch [5/30] Batch [250/798]:                     loss = 4170.27001953125\n","Epoch [5/30] Batch [260/798]:                     loss = 3126.984619140625\n","Epoch [5/30] Batch [270/798]:                     loss = 2483.010986328125\n","Epoch [5/30] Batch [280/798]:                     loss = 1912.002685546875\n","Epoch [5/30] Batch [290/798]:                     loss = 1832.1878662109375\n","Epoch [5/30] Batch [300/798]:                     loss = 1624.4356689453125\n","Epoch [5/30] Batch [310/798]:                     loss = 1504.2279052734375\n","Epoch [5/30] Batch [320/798]:                     loss = 1697.999755859375\n","Epoch [5/30] Batch [330/798]:                     loss = 2553.056640625\n","Epoch [5/30] Batch [340/798]:                     loss = 6153.7041015625\n","Epoch [5/30] Batch [350/798]:                     loss = 7883.3828125\n","Epoch [5/30] Batch [360/798]:                     loss = 9368.2919921875\n","Epoch [5/30] Batch [370/798]:                     loss = 10095.291015625\n","Epoch [5/30] Batch [380/798]:                     loss = 9855.6806640625\n","Epoch [5/30] Batch [390/798]:                     loss = 8709.708984375\n","Epoch [5/30] Batch [400/798]:                     loss = 7182.9462890625\n","Epoch [5/30] Batch [410/798]:                     loss = 4646.2001953125\n","Epoch [5/30] Batch [420/798]:                     loss = 3234.22607421875\n","Epoch [5/30] Batch [430/798]:                     loss = 2547.08935546875\n","Epoch [5/30] Batch [440/798]:                     loss = 1945.3096923828125\n","Epoch [5/30] Batch [450/798]:                     loss = 1749.7591552734375\n","Epoch [5/30] Batch [460/798]:                     loss = 1668.2420654296875\n","Epoch [5/30] Batch [470/798]:                     loss = 1950.8734130859375\n","Epoch [5/30] Batch [480/798]:                     loss = 4768.98974609375\n","Epoch [5/30] Batch [490/798]:                     loss = 7614.71044921875\n","Epoch [5/30] Batch [500/798]:                     loss = 9470.5302734375\n","Epoch [5/30] Batch [510/798]:                     loss = 9246.0576171875\n","Epoch [5/30] Batch [520/798]:                     loss = 9973.9033203125\n","Epoch [5/30] Batch [530/798]:                     loss = 8920.708984375\n","Epoch [5/30] Batch [540/798]:                     loss = 7635.57958984375\n","Epoch [5/30] Batch [550/798]:                     loss = 5007.4267578125\n","Epoch [5/30] Batch [560/798]:                     loss = 3455.88232421875\n","Epoch [5/30] Batch [570/798]:                     loss = 2372.756591796875\n","Epoch [5/30] Batch [580/798]:                     loss = 2180.042236328125\n","Epoch [5/30] Batch [590/798]:                     loss = 1831.061279296875\n","Epoch [5/30] Batch [600/798]:                     loss = 1613.260009765625\n","Epoch [5/30] Batch [610/798]:                     loss = 1461.5367431640625\n","Epoch [5/30] Batch [620/798]:                     loss = 1562.3983154296875\n","Epoch [5/30] Batch [630/798]:                     loss = 2097.337158203125\n","Epoch [5/30] Batch [640/798]:                     loss = 2984.7744140625\n","Epoch [5/30] Batch [650/798]:                     loss = 3590.60693359375\n","Epoch [5/30] Batch [660/798]:                     loss = 3918.822021484375\n","Epoch [5/30] Batch [670/798]:                     loss = 4059.153564453125\n","Epoch [5/30] Batch [680/798]:                     loss = 3846.385009765625\n","Epoch [5/30] Batch [690/798]:                     loss = 3152.85546875\n","Epoch [5/30] Batch [700/798]:                     loss = 2625.24755859375\n","Epoch [5/30] Batch [710/798]:                     loss = 1951.62255859375\n","Epoch [5/30] Batch [720/798]:                     loss = 1895.2401123046875\n","Epoch [5/30] Batch [730/798]:                     loss = 1667.0196533203125\n","Epoch [5/30] Batch [740/798]:                     loss = 1526.6259765625\n","Epoch [5/30] Batch [750/798]:                     loss = 1549.2855224609375\n","Epoch [5/30] Batch [760/798]:                     loss = 2062.724853515625\n","Epoch [5/30] Batch [770/798]:                     loss = 5109.384765625\n","Epoch [5/30] Batch [780/798]:                     loss = 5468.14453125\n","Epoch [5/30] Batch [790/798]:                     loss = 6481.9453125\n","Epoch [5/30]: training loss= 3903.125602, training mse loss= 3903.125602, training bce loss= nan\n","            : validation loss= 3426.157540, validation mse loss= 3426.157540, validation bce loss= nan\n","Epoch [6/30] Batch [10/798]:                     loss = 2271.042724609375\n","Epoch [6/30] Batch [20/798]:                     loss = 1872.9522705078125\n","Epoch [6/30] Batch [30/798]:                     loss = 1771.863525390625\n","Epoch [6/30] Batch [40/798]:                     loss = 1733.703369140625\n","Epoch [6/30] Batch [50/798]:                     loss = 2533.921630859375\n","Epoch [6/30] Batch [60/798]:                     loss = 3903.579345703125\n","Epoch [6/30] Batch [70/798]:                     loss = 5935.61474609375\n","Epoch [6/30] Batch [80/798]:                     loss = 6422.24609375\n","Epoch [6/30] Batch [90/798]:                     loss = 6901.701171875\n","Epoch [6/30] Batch [100/798]:                     loss = 6155.02685546875\n","Epoch [6/30] Batch [110/798]:                     loss = 5256.8955078125\n","Epoch [6/30] Batch [120/798]:                     loss = 3826.402099609375\n","Epoch [6/30] Batch [130/798]:                     loss = 3172.470947265625\n","Epoch [6/30] Batch [140/798]:                     loss = 2176.186767578125\n","Epoch [6/30] Batch [150/798]:                     loss = 2016.1114501953125\n","Epoch [6/30] Batch [160/798]:                     loss = 1820.8167724609375\n","Epoch [6/30] Batch [170/798]:                     loss = 1574.3074951171875\n","Epoch [6/30] Batch [180/798]:                     loss = 1454.532958984375\n","Epoch [6/30] Batch [190/798]:                     loss = 1662.6468505859375\n","Epoch [6/30] Batch [200/798]:                     loss = 2322.3701171875\n","Epoch [6/30] Batch [210/798]:                     loss = 3398.217041015625\n","Epoch [6/30] Batch [220/798]:                     loss = 4484.78759765625\n","Epoch [6/30] Batch [230/798]:                     loss = 4714.828125\n","Epoch [6/30] Batch [240/798]:                     loss = 4695.93408203125\n","Epoch [6/30] Batch [250/798]:                     loss = 4170.2685546875\n","Epoch [6/30] Batch [260/798]:                     loss = 3126.983154296875\n","Epoch [6/30] Batch [270/798]:                     loss = 2483.010009765625\n","Epoch [6/30] Batch [280/798]:                     loss = 1912.00146484375\n","Epoch [6/30] Batch [290/798]:                     loss = 1832.1873779296875\n","Epoch [6/30] Batch [300/798]:                     loss = 1624.4317626953125\n","Epoch [6/30] Batch [310/798]:                     loss = 1504.2247314453125\n","Epoch [6/30] Batch [320/798]:                     loss = 1697.9942626953125\n","Epoch [6/30] Batch [330/798]:                     loss = 2553.05322265625\n","Epoch [6/30] Batch [340/798]:                     loss = 6153.70751953125\n","Epoch [6/30] Batch [350/798]:                     loss = 7883.3798828125\n","Epoch [6/30] Batch [360/798]:                     loss = 9368.291015625\n","Epoch [6/30] Batch [370/798]:                     loss = 10095.2890625\n","Epoch [6/30] Batch [380/798]:                     loss = 9855.6787109375\n","Epoch [6/30] Batch [390/798]:                     loss = 8709.70703125\n","Epoch [6/30] Batch [400/798]:                     loss = 7182.94189453125\n","Epoch [6/30] Batch [410/798]:                     loss = 4646.19775390625\n","Epoch [6/30] Batch [420/798]:                     loss = 3234.224365234375\n","Epoch [6/30] Batch [430/798]:                     loss = 2547.08984375\n","Epoch [6/30] Batch [440/798]:                     loss = 1945.3084716796875\n","Epoch [6/30] Batch [450/798]:                     loss = 1749.7542724609375\n","Epoch [6/30] Batch [460/798]:                     loss = 1668.23681640625\n","Epoch [6/30] Batch [470/798]:                     loss = 1950.8741455078125\n","Epoch [6/30] Batch [480/798]:                     loss = 4768.9833984375\n","Epoch [6/30] Batch [490/798]:                     loss = 7614.70751953125\n","Epoch [6/30] Batch [500/798]:                     loss = 9470.525390625\n","Epoch [6/30] Batch [510/798]:                     loss = 9246.0556640625\n","Epoch [6/30] Batch [520/798]:                     loss = 9973.9013671875\n","Epoch [6/30] Batch [530/798]:                     loss = 8920.70703125\n","Epoch [6/30] Batch [540/798]:                     loss = 7635.58203125\n","Epoch [6/30] Batch [550/798]:                     loss = 5007.42431640625\n","Epoch [6/30] Batch [560/798]:                     loss = 3455.881103515625\n","Epoch [6/30] Batch [570/798]:                     loss = 2372.75439453125\n","Epoch [6/30] Batch [580/798]:                     loss = 2180.040283203125\n","Epoch [6/30] Batch [590/798]:                     loss = 1831.0540771484375\n","Epoch [6/30] Batch [600/798]:                     loss = 1613.2491455078125\n","Epoch [6/30] Batch [610/798]:                     loss = 1461.5260009765625\n","Epoch [6/30] Batch [620/798]:                     loss = 1562.3944091796875\n","Epoch [6/30] Batch [630/798]:                     loss = 2097.33203125\n","Epoch [6/30] Batch [640/798]:                     loss = 2984.771728515625\n","Epoch [6/30] Batch [650/798]:                     loss = 3590.604248046875\n","Epoch [6/30] Batch [660/798]:                     loss = 3918.819091796875\n","Epoch [6/30] Batch [670/798]:                     loss = 4059.152587890625\n","Epoch [6/30] Batch [680/798]:                     loss = 3846.381591796875\n","Epoch [6/30] Batch [690/798]:                     loss = 3152.85302734375\n","Epoch [6/30] Batch [700/798]:                     loss = 2625.244873046875\n","Epoch [6/30] Batch [710/798]:                     loss = 1951.6192626953125\n","Epoch [6/30] Batch [720/798]:                     loss = 1895.2359619140625\n","Epoch [6/30] Batch [730/798]:                     loss = 1667.0164794921875\n","Epoch [6/30] Batch [740/798]:                     loss = 1526.62744140625\n","Epoch [6/30] Batch [750/798]:                     loss = 1549.2781982421875\n","Epoch [6/30] Batch [760/798]:                     loss = 2062.722412109375\n","Epoch [6/30] Batch [770/798]:                     loss = 5109.392578125\n","Epoch [6/30] Batch [780/798]:                     loss = 5468.15234375\n","Epoch [6/30] Batch [790/798]:                     loss = 6481.9580078125\n","Epoch [6/30]: training loss= 3903.123681, training mse loss= 3903.123681, training bce loss= nan\n","            : validation loss= 3426.166525, validation mse loss= 3426.166525, validation bce loss= nan\n","Epoch [7/30] Batch [10/798]:                     loss = 2271.04248046875\n","Epoch [7/30] Batch [20/798]:                     loss = 1872.951171875\n","Epoch [7/30] Batch [30/798]:                     loss = 1771.860107421875\n","Epoch [7/30] Batch [40/798]:                     loss = 1733.703857421875\n","Epoch [7/30] Batch [50/798]:                     loss = 2533.920166015625\n","Epoch [7/30] Batch [60/798]:                     loss = 3903.57861328125\n","Epoch [7/30] Batch [70/798]:                     loss = 5935.61181640625\n","Epoch [7/30] Batch [80/798]:                     loss = 6422.24609375\n","Epoch [7/30] Batch [90/798]:                     loss = 6901.701171875\n","Epoch [7/30] Batch [100/798]:                     loss = 6155.025390625\n","Epoch [7/30] Batch [110/798]:                     loss = 5256.89404296875\n","Epoch [7/30] Batch [120/798]:                     loss = 3826.40087890625\n","Epoch [7/30] Batch [130/798]:                     loss = 3172.468994140625\n","Epoch [7/30] Batch [140/798]:                     loss = 2176.18408203125\n","Epoch [7/30] Batch [150/798]:                     loss = 2016.109130859375\n","Epoch [7/30] Batch [160/798]:                     loss = 1820.8150634765625\n","Epoch [7/30] Batch [170/798]:                     loss = 1574.3109130859375\n","Epoch [7/30] Batch [180/798]:                     loss = 1454.5372314453125\n","Epoch [7/30] Batch [190/798]:                     loss = 1662.6484375\n","Epoch [7/30] Batch [200/798]:                     loss = 2322.37060546875\n","Epoch [7/30] Batch [210/798]:                     loss = 3398.215576171875\n","Epoch [7/30] Batch [220/798]:                     loss = 4484.78759765625\n","Epoch [7/30] Batch [230/798]:                     loss = 4714.82763671875\n","Epoch [7/30] Batch [240/798]:                     loss = 4695.93212890625\n","Epoch [7/30] Batch [250/798]:                     loss = 4170.267578125\n","Epoch [7/30] Batch [260/798]:                     loss = 3126.98193359375\n","Epoch [7/30] Batch [270/798]:                     loss = 2483.006591796875\n","Epoch [7/30] Batch [280/798]:                     loss = 1911.9989013671875\n","Epoch [7/30] Batch [290/798]:                     loss = 1832.176513671875\n","Epoch [7/30] Batch [300/798]:                     loss = 1624.429443359375\n","Epoch [7/30] Batch [310/798]:                     loss = 1504.238037109375\n","Epoch [7/30] Batch [320/798]:                     loss = 1697.990478515625\n","Epoch [7/30] Batch [330/798]:                     loss = 2553.049560546875\n","Epoch [7/30] Batch [340/798]:                     loss = 6153.7060546875\n","Epoch [7/30] Batch [350/798]:                     loss = 7883.37939453125\n","Epoch [7/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [7/30] Batch [370/798]:                     loss = 10095.28515625\n","Epoch [7/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [7/30] Batch [390/798]:                     loss = 8709.705078125\n","Epoch [7/30] Batch [400/798]:                     loss = 7182.94091796875\n","Epoch [7/30] Batch [410/798]:                     loss = 4646.19775390625\n","Epoch [7/30] Batch [420/798]:                     loss = 3234.221435546875\n","Epoch [7/30] Batch [430/798]:                     loss = 2547.0849609375\n","Epoch [7/30] Batch [440/798]:                     loss = 1945.3052978515625\n","Epoch [7/30] Batch [450/798]:                     loss = 1749.755859375\n","Epoch [7/30] Batch [460/798]:                     loss = 1668.238525390625\n","Epoch [7/30] Batch [470/798]:                     loss = 1950.8729248046875\n","Epoch [7/30] Batch [480/798]:                     loss = 4768.9833984375\n","Epoch [7/30] Batch [490/798]:                     loss = 7614.71923828125\n","Epoch [7/30] Batch [500/798]:                     loss = 9470.525390625\n","Epoch [7/30] Batch [510/798]:                     loss = 9246.0556640625\n","Epoch [7/30] Batch [520/798]:                     loss = 9973.8994140625\n","Epoch [7/30] Batch [530/798]:                     loss = 8920.705078125\n","Epoch [7/30] Batch [540/798]:                     loss = 7635.5869140625\n","Epoch [7/30] Batch [550/798]:                     loss = 5007.42138671875\n","Epoch [7/30] Batch [560/798]:                     loss = 3455.880615234375\n","Epoch [7/30] Batch [570/798]:                     loss = 2372.7529296875\n","Epoch [7/30] Batch [580/798]:                     loss = 2180.039306640625\n","Epoch [7/30] Batch [590/798]:                     loss = 1831.053955078125\n","Epoch [7/30] Batch [600/798]:                     loss = 1613.2501220703125\n","Epoch [7/30] Batch [610/798]:                     loss = 1461.525146484375\n","Epoch [7/30] Batch [620/798]:                     loss = 1562.38916015625\n","Epoch [7/30] Batch [630/798]:                     loss = 2097.3310546875\n","Epoch [7/30] Batch [640/798]:                     loss = 2984.773193359375\n","Epoch [7/30] Batch [650/798]:                     loss = 3590.604736328125\n","Epoch [7/30] Batch [660/798]:                     loss = 3918.81884765625\n","Epoch [7/30] Batch [670/798]:                     loss = 4059.15185546875\n","Epoch [7/30] Batch [680/798]:                     loss = 3846.381591796875\n","Epoch [7/30] Batch [690/798]:                     loss = 3152.8544921875\n","Epoch [7/30] Batch [700/798]:                     loss = 2625.246337890625\n","Epoch [7/30] Batch [710/798]:                     loss = 1951.6202392578125\n","Epoch [7/30] Batch [720/798]:                     loss = 1895.2342529296875\n","Epoch [7/30] Batch [730/798]:                     loss = 1667.0140380859375\n","Epoch [7/30] Batch [740/798]:                     loss = 1526.6243896484375\n","Epoch [7/30] Batch [750/798]:                     loss = 1549.27783203125\n","Epoch [7/30] Batch [760/798]:                     loss = 2062.727783203125\n","Epoch [7/30] Batch [770/798]:                     loss = 5109.376953125\n","Epoch [7/30] Batch [780/798]:                     loss = 5468.15283203125\n","Epoch [7/30] Batch [790/798]:                     loss = 6481.951171875\n","Epoch [7/30]: training loss= 3903.122384, training mse loss= 3903.122384, training bce loss= nan\n","            : validation loss= 3426.170346, validation mse loss= 3426.170346, validation bce loss= nan\n","Epoch [8/30] Batch [10/798]:                     loss = 2271.03857421875\n","Epoch [8/30] Batch [20/798]:                     loss = 1872.9534912109375\n","Epoch [8/30] Batch [30/798]:                     loss = 1771.8570556640625\n","Epoch [8/30] Batch [40/798]:                     loss = 1733.698974609375\n","Epoch [8/30] Batch [50/798]:                     loss = 2533.91748046875\n","Epoch [8/30] Batch [60/798]:                     loss = 3903.57958984375\n","Epoch [8/30] Batch [70/798]:                     loss = 5935.62841796875\n","Epoch [8/30] Batch [80/798]:                     loss = 6422.2451171875\n","Epoch [8/30] Batch [90/798]:                     loss = 6901.70263671875\n","Epoch [8/30] Batch [100/798]:                     loss = 6155.01806640625\n","Epoch [8/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [8/30] Batch [120/798]:                     loss = 3826.39697265625\n","Epoch [8/30] Batch [130/798]:                     loss = 3172.46630859375\n","Epoch [8/30] Batch [140/798]:                     loss = 2176.1748046875\n","Epoch [8/30] Batch [150/798]:                     loss = 2016.095703125\n","Epoch [8/30] Batch [160/798]:                     loss = 1820.81005859375\n","Epoch [8/30] Batch [170/798]:                     loss = 1574.3006591796875\n","Epoch [8/30] Batch [180/798]:                     loss = 1454.520751953125\n","Epoch [8/30] Batch [190/798]:                     loss = 1662.642822265625\n","Epoch [8/30] Batch [200/798]:                     loss = 2322.367431640625\n","Epoch [8/30] Batch [210/798]:                     loss = 3398.20703125\n","Epoch [8/30] Batch [220/798]:                     loss = 4484.78173828125\n","Epoch [8/30] Batch [230/798]:                     loss = 4714.8203125\n","Epoch [8/30] Batch [240/798]:                     loss = 4695.927734375\n","Epoch [8/30] Batch [250/798]:                     loss = 4170.25634765625\n","Epoch [8/30] Batch [260/798]:                     loss = 3126.972900390625\n","Epoch [8/30] Batch [270/798]:                     loss = 2482.9970703125\n","Epoch [8/30] Batch [280/798]:                     loss = 1911.994873046875\n","Epoch [8/30] Batch [290/798]:                     loss = 1832.1636962890625\n","Epoch [8/30] Batch [300/798]:                     loss = 1624.4227294921875\n","Epoch [8/30] Batch [310/798]:                     loss = 1504.2149658203125\n","Epoch [8/30] Batch [320/798]:                     loss = 1697.9796142578125\n","Epoch [8/30] Batch [330/798]:                     loss = 2553.047607421875\n","Epoch [8/30] Batch [340/798]:                     loss = 6153.69482421875\n","Epoch [8/30] Batch [350/798]:                     loss = 7883.37841796875\n","Epoch [8/30] Batch [360/798]:                     loss = 9368.2890625\n","Epoch [8/30] Batch [370/798]:                     loss = 10095.28515625\n","Epoch [8/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [8/30] Batch [390/798]:                     loss = 8709.7021484375\n","Epoch [8/30] Batch [400/798]:                     loss = 7182.935546875\n","Epoch [8/30] Batch [410/798]:                     loss = 4646.1904296875\n","Epoch [8/30] Batch [420/798]:                     loss = 3234.211669921875\n","Epoch [8/30] Batch [430/798]:                     loss = 2547.08349609375\n","Epoch [8/30] Batch [440/798]:                     loss = 1945.295166015625\n","Epoch [8/30] Batch [450/798]:                     loss = 1749.7420654296875\n","Epoch [8/30] Batch [460/798]:                     loss = 1668.2244873046875\n","Epoch [8/30] Batch [470/798]:                     loss = 1950.85888671875\n","Epoch [8/30] Batch [480/798]:                     loss = 4768.98193359375\n","Epoch [8/30] Batch [490/798]:                     loss = 7614.70849609375\n","Epoch [8/30] Batch [500/798]:                     loss = 9470.5263671875\n","Epoch [8/30] Batch [510/798]:                     loss = 9246.0537109375\n","Epoch [8/30] Batch [520/798]:                     loss = 9973.8984375\n","Epoch [8/30] Batch [530/798]:                     loss = 8920.703125\n","Epoch [8/30] Batch [540/798]:                     loss = 7635.5849609375\n","Epoch [8/30] Batch [550/798]:                     loss = 5007.416015625\n","Epoch [8/30] Batch [560/798]:                     loss = 3455.874267578125\n","Epoch [8/30] Batch [570/798]:                     loss = 2372.736572265625\n","Epoch [8/30] Batch [580/798]:                     loss = 2180.037353515625\n","Epoch [8/30] Batch [590/798]:                     loss = 1831.02685546875\n","Epoch [8/30] Batch [600/798]:                     loss = 1613.234619140625\n","Epoch [8/30] Batch [610/798]:                     loss = 1461.4996337890625\n","Epoch [8/30] Batch [620/798]:                     loss = 1562.370849609375\n","Epoch [8/30] Batch [630/798]:                     loss = 2097.332275390625\n","Epoch [8/30] Batch [640/798]:                     loss = 2984.763671875\n","Epoch [8/30] Batch [650/798]:                     loss = 3590.5947265625\n","Epoch [8/30] Batch [660/798]:                     loss = 3918.80859375\n","Epoch [8/30] Batch [670/798]:                     loss = 4059.140869140625\n","Epoch [8/30] Batch [680/798]:                     loss = 3846.37451171875\n","Epoch [8/30] Batch [690/798]:                     loss = 3152.84326171875\n","Epoch [8/30] Batch [700/798]:                     loss = 2625.228759765625\n","Epoch [8/30] Batch [710/798]:                     loss = 1951.5985107421875\n","Epoch [8/30] Batch [720/798]:                     loss = 1895.21875\n","Epoch [8/30] Batch [730/798]:                     loss = 1666.9881591796875\n","Epoch [8/30] Batch [740/798]:                     loss = 1526.6103515625\n","Epoch [8/30] Batch [750/798]:                     loss = 1549.256103515625\n","Epoch [8/30] Batch [760/798]:                     loss = 2062.709228515625\n","Epoch [8/30] Batch [770/798]:                     loss = 5109.37890625\n","Epoch [8/30] Batch [780/798]:                     loss = 5468.150390625\n","Epoch [8/30] Batch [790/798]:                     loss = 6481.9453125\n","Epoch [8/30]: training loss= 3903.114367, training mse loss= 3903.114367, training bce loss= nan\n","            : validation loss= 3426.159578, validation mse loss= 3426.159578, validation bce loss= nan\n","Epoch [9/30] Batch [10/798]:                     loss = 2271.02734375\n","Epoch [9/30] Batch [20/798]:                     loss = 1872.923583984375\n","Epoch [9/30] Batch [30/798]:                     loss = 1771.8255615234375\n","Epoch [9/30] Batch [40/798]:                     loss = 1733.683349609375\n","Epoch [9/30] Batch [50/798]:                     loss = 2533.912109375\n","Epoch [9/30] Batch [60/798]:                     loss = 3903.582763671875\n","Epoch [9/30] Batch [70/798]:                     loss = 5935.61962890625\n","Epoch [9/30] Batch [80/798]:                     loss = 6422.23876953125\n","Epoch [9/30] Batch [90/798]:                     loss = 6901.70361328125\n","Epoch [9/30] Batch [100/798]:                     loss = 6155.0166015625\n","Epoch [9/30] Batch [110/798]:                     loss = 5256.884765625\n","Epoch [9/30] Batch [120/798]:                     loss = 3826.38916015625\n","Epoch [9/30] Batch [130/798]:                     loss = 3172.462646484375\n","Epoch [9/30] Batch [140/798]:                     loss = 2176.16357421875\n","Epoch [9/30] Batch [150/798]:                     loss = 2016.070068359375\n","Epoch [9/30] Batch [160/798]:                     loss = 1820.7861328125\n","Epoch [9/30] Batch [170/798]:                     loss = 1574.2701416015625\n","Epoch [9/30] Batch [180/798]:                     loss = 1454.5025634765625\n","Epoch [9/30] Batch [190/798]:                     loss = 1662.6416015625\n","Epoch [9/30] Batch [200/798]:                     loss = 2322.367431640625\n","Epoch [9/30] Batch [210/798]:                     loss = 3398.2080078125\n","Epoch [9/30] Batch [220/798]:                     loss = 4484.78515625\n","Epoch [9/30] Batch [230/798]:                     loss = 4714.81884765625\n","Epoch [9/30] Batch [240/798]:                     loss = 4695.92724609375\n","Epoch [9/30] Batch [250/798]:                     loss = 4170.25244140625\n","Epoch [9/30] Batch [260/798]:                     loss = 3126.975830078125\n","Epoch [9/30] Batch [270/798]:                     loss = 2482.997314453125\n","Epoch [9/30] Batch [280/798]:                     loss = 1911.9857177734375\n","Epoch [9/30] Batch [290/798]:                     loss = 1832.1513671875\n","Epoch [9/30] Batch [300/798]:                     loss = 1624.3939208984375\n","Epoch [9/30] Batch [310/798]:                     loss = 1504.190185546875\n","Epoch [9/30] Batch [320/798]:                     loss = 1697.9754638671875\n","Epoch [9/30] Batch [330/798]:                     loss = 2553.03955078125\n","Epoch [9/30] Batch [340/798]:                     loss = 6153.7080078125\n","Epoch [9/30] Batch [350/798]:                     loss = 7883.3916015625\n","Epoch [9/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [9/30] Batch [370/798]:                     loss = 10095.2822265625\n","Epoch [9/30] Batch [380/798]:                     loss = 9855.6787109375\n","Epoch [9/30] Batch [390/798]:                     loss = 8709.701171875\n","Epoch [9/30] Batch [400/798]:                     loss = 7182.93701171875\n","Epoch [9/30] Batch [410/798]:                     loss = 4646.1865234375\n","Epoch [9/30] Batch [420/798]:                     loss = 3234.208740234375\n","Epoch [9/30] Batch [430/798]:                     loss = 2547.073974609375\n","Epoch [9/30] Batch [440/798]:                     loss = 1945.2705078125\n","Epoch [9/30] Batch [450/798]:                     loss = 1749.715576171875\n","Epoch [9/30] Batch [460/798]:                     loss = 1668.198974609375\n","Epoch [9/30] Batch [470/798]:                     loss = 1950.85546875\n","Epoch [9/30] Batch [480/798]:                     loss = 4768.9853515625\n","Epoch [9/30] Batch [490/798]:                     loss = 7614.70703125\n","Epoch [9/30] Batch [500/798]:                     loss = 9470.5263671875\n","Epoch [9/30] Batch [510/798]:                     loss = 9246.05078125\n","Epoch [9/30] Batch [520/798]:                     loss = 9973.9013671875\n","Epoch [9/30] Batch [530/798]:                     loss = 8920.701171875\n","Epoch [9/30] Batch [540/798]:                     loss = 7635.58642578125\n","Epoch [9/30] Batch [550/798]:                     loss = 5007.41748046875\n","Epoch [9/30] Batch [560/798]:                     loss = 3455.870361328125\n","Epoch [9/30] Batch [570/798]:                     loss = 2372.730224609375\n","Epoch [9/30] Batch [580/798]:                     loss = 2180.00927734375\n","Epoch [9/30] Batch [590/798]:                     loss = 1831.0101318359375\n","Epoch [9/30] Batch [600/798]:                     loss = 1613.2174072265625\n","Epoch [9/30] Batch [610/798]:                     loss = 1461.490234375\n","Epoch [9/30] Batch [620/798]:                     loss = 1562.3614501953125\n","Epoch [9/30] Batch [630/798]:                     loss = 2097.328369140625\n","Epoch [9/30] Batch [640/798]:                     loss = 2984.7587890625\n","Epoch [9/30] Batch [650/798]:                     loss = 3590.59619140625\n","Epoch [9/30] Batch [660/798]:                     loss = 3918.806396484375\n","Epoch [9/30] Batch [670/798]:                     loss = 4059.139404296875\n","Epoch [9/30] Batch [680/798]:                     loss = 3846.370849609375\n","Epoch [9/30] Batch [690/798]:                     loss = 3152.841064453125\n","Epoch [9/30] Batch [700/798]:                     loss = 2625.22265625\n","Epoch [9/30] Batch [710/798]:                     loss = 1951.5943603515625\n","Epoch [9/30] Batch [720/798]:                     loss = 1895.206298828125\n","Epoch [9/30] Batch [730/798]:                     loss = 1666.971923828125\n","Epoch [9/30] Batch [740/798]:                     loss = 1526.5916748046875\n","Epoch [9/30] Batch [750/798]:                     loss = 1549.2435302734375\n","Epoch [9/30] Batch [760/798]:                     loss = 2062.7099609375\n","Epoch [9/30] Batch [770/798]:                     loss = 5109.3798828125\n","Epoch [9/30] Batch [780/798]:                     loss = 5468.1396484375\n","Epoch [9/30] Batch [790/798]:                     loss = 6481.9443359375\n","Epoch [9/30]: training loss= 3903.107294, training mse loss= 3903.107294, training bce loss= nan\n","            : validation loss= 3426.146965, validation mse loss= 3426.146965, validation bce loss= nan\n","Epoch [10/30] Batch [10/798]:                     loss = 2271.024169921875\n","Epoch [10/30] Batch [20/798]:                     loss = 1872.917724609375\n","Epoch [10/30] Batch [30/798]:                     loss = 1771.8271484375\n","Epoch [10/30] Batch [40/798]:                     loss = 1733.6865234375\n","Epoch [10/30] Batch [50/798]:                     loss = 2533.91259765625\n","Epoch [10/30] Batch [60/798]:                     loss = 3903.581298828125\n","Epoch [10/30] Batch [70/798]:                     loss = 5935.61279296875\n","Epoch [10/30] Batch [80/798]:                     loss = 6422.24658203125\n","Epoch [10/30] Batch [90/798]:                     loss = 6901.708984375\n","Epoch [10/30] Batch [100/798]:                     loss = 6155.02001953125\n","Epoch [10/30] Batch [110/798]:                     loss = 5256.8837890625\n","Epoch [10/30] Batch [120/798]:                     loss = 3826.390869140625\n","Epoch [10/30] Batch [130/798]:                     loss = 3172.461181640625\n","Epoch [10/30] Batch [140/798]:                     loss = 2176.165771484375\n","Epoch [10/30] Batch [150/798]:                     loss = 2016.06884765625\n","Epoch [10/30] Batch [160/798]:                     loss = 1820.791015625\n","Epoch [10/30] Batch [170/798]:                     loss = 1574.2703857421875\n","Epoch [10/30] Batch [180/798]:                     loss = 1454.5018310546875\n","Epoch [10/30] Batch [190/798]:                     loss = 1662.6407470703125\n","Epoch [10/30] Batch [200/798]:                     loss = 2322.3671875\n","Epoch [10/30] Batch [210/798]:                     loss = 3398.20556640625\n","Epoch [10/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [10/30] Batch [230/798]:                     loss = 4714.81689453125\n","Epoch [10/30] Batch [240/798]:                     loss = 4695.9248046875\n","Epoch [10/30] Batch [250/798]:                     loss = 4170.2470703125\n","Epoch [10/30] Batch [260/798]:                     loss = 3126.971435546875\n","Epoch [10/30] Batch [270/798]:                     loss = 2482.9892578125\n","Epoch [10/30] Batch [280/798]:                     loss = 1911.97216796875\n","Epoch [10/30] Batch [290/798]:                     loss = 1832.134033203125\n","Epoch [10/30] Batch [300/798]:                     loss = 1624.376708984375\n","Epoch [10/30] Batch [310/798]:                     loss = 1504.172119140625\n","Epoch [10/30] Batch [320/798]:                     loss = 1697.9588623046875\n","Epoch [10/30] Batch [330/798]:                     loss = 2553.044677734375\n","Epoch [10/30] Batch [340/798]:                     loss = 6153.69580078125\n","Epoch [10/30] Batch [350/798]:                     loss = 7883.375\n","Epoch [10/30] Batch [360/798]:                     loss = 9368.291015625\n","Epoch [10/30] Batch [370/798]:                     loss = 10095.2841796875\n","Epoch [10/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [10/30] Batch [390/798]:                     loss = 8709.7001953125\n","Epoch [10/30] Batch [400/798]:                     loss = 7182.93603515625\n","Epoch [10/30] Batch [410/798]:                     loss = 4646.1865234375\n","Epoch [10/30] Batch [420/798]:                     loss = 3234.21142578125\n","Epoch [10/30] Batch [430/798]:                     loss = 2547.072265625\n","Epoch [10/30] Batch [440/798]:                     loss = 1945.2711181640625\n","Epoch [10/30] Batch [450/798]:                     loss = 1749.7100830078125\n","Epoch [10/30] Batch [460/798]:                     loss = 1668.1961669921875\n","Epoch [10/30] Batch [470/798]:                     loss = 1950.84912109375\n","Epoch [10/30] Batch [480/798]:                     loss = 4768.98291015625\n","Epoch [10/30] Batch [490/798]:                     loss = 7614.70458984375\n","Epoch [10/30] Batch [500/798]:                     loss = 9470.5322265625\n","Epoch [10/30] Batch [510/798]:                     loss = 9246.046875\n","Epoch [10/30] Batch [520/798]:                     loss = 9973.8974609375\n","Epoch [10/30] Batch [530/798]:                     loss = 8920.69921875\n","Epoch [10/30] Batch [540/798]:                     loss = 7635.58056640625\n","Epoch [10/30] Batch [550/798]:                     loss = 5007.40869140625\n","Epoch [10/30] Batch [560/798]:                     loss = 3455.861572265625\n","Epoch [10/30] Batch [570/798]:                     loss = 2372.7236328125\n","Epoch [10/30] Batch [580/798]:                     loss = 2180.001953125\n","Epoch [10/30] Batch [590/798]:                     loss = 1831.007568359375\n","Epoch [10/30] Batch [600/798]:                     loss = 1613.2132568359375\n","Epoch [10/30] Batch [610/798]:                     loss = 1461.4791259765625\n","Epoch [10/30] Batch [620/798]:                     loss = 1562.3565673828125\n","Epoch [10/30] Batch [630/798]:                     loss = 2097.333740234375\n","Epoch [10/30] Batch [640/798]:                     loss = 2984.7548828125\n","Epoch [10/30] Batch [650/798]:                     loss = 3590.593017578125\n","Epoch [10/30] Batch [660/798]:                     loss = 3918.806640625\n","Epoch [10/30] Batch [670/798]:                     loss = 4059.137939453125\n","Epoch [10/30] Batch [680/798]:                     loss = 3846.365234375\n","Epoch [10/30] Batch [690/798]:                     loss = 3152.842529296875\n","Epoch [10/30] Batch [700/798]:                     loss = 2625.218505859375\n","Epoch [10/30] Batch [710/798]:                     loss = 1951.5904541015625\n","Epoch [10/30] Batch [720/798]:                     loss = 1895.201416015625\n","Epoch [10/30] Batch [730/798]:                     loss = 1666.9654541015625\n","Epoch [10/30] Batch [740/798]:                     loss = 1526.582275390625\n","Epoch [10/30] Batch [750/798]:                     loss = 1549.2320556640625\n","Epoch [10/30] Batch [760/798]:                     loss = 2062.709228515625\n","Epoch [10/30] Batch [770/798]:                     loss = 5109.380859375\n","Epoch [10/30] Batch [780/798]:                     loss = 5468.1357421875\n","Epoch [10/30] Batch [790/798]:                     loss = 6481.943359375\n","Epoch [10/30]: training loss= 3903.104148, training mse loss= 3903.104148, training bce loss= nan\n","             : validation loss= 3426.146272, validation mse loss= 3426.146272, validation bce loss= nan\n","Epoch [11/30] Batch [10/798]:                     loss = 2271.020263671875\n","Epoch [11/30] Batch [20/798]:                     loss = 1872.9105224609375\n","Epoch [11/30] Batch [30/798]:                     loss = 1771.8177490234375\n","Epoch [11/30] Batch [40/798]:                     loss = 1733.6807861328125\n","Epoch [11/30] Batch [50/798]:                     loss = 2533.910888671875\n","Epoch [11/30] Batch [60/798]:                     loss = 3903.580810546875\n","Epoch [11/30] Batch [70/798]:                     loss = 5935.61181640625\n","Epoch [11/30] Batch [80/798]:                     loss = 6422.24072265625\n","Epoch [11/30] Batch [90/798]:                     loss = 6901.70556640625\n","Epoch [11/30] Batch [100/798]:                     loss = 6155.0166015625\n","Epoch [11/30] Batch [110/798]:                     loss = 5256.88427734375\n","Epoch [11/30] Batch [120/798]:                     loss = 3826.388671875\n","Epoch [11/30] Batch [130/798]:                     loss = 3172.462158203125\n","Epoch [11/30] Batch [140/798]:                     loss = 2176.161376953125\n","Epoch [11/30] Batch [150/798]:                     loss = 2016.06787109375\n","Epoch [11/30] Batch [160/798]:                     loss = 1820.7860107421875\n","Epoch [11/30] Batch [170/798]:                     loss = 1574.2686767578125\n","Epoch [11/30] Batch [180/798]:                     loss = 1454.5013427734375\n","Epoch [11/30] Batch [190/798]:                     loss = 1662.6380615234375\n","Epoch [11/30] Batch [200/798]:                     loss = 2322.366455078125\n","Epoch [11/30] Batch [210/798]:                     loss = 3398.20654296875\n","Epoch [11/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [11/30] Batch [230/798]:                     loss = 4714.81787109375\n","Epoch [11/30] Batch [240/798]:                     loss = 4695.92578125\n","Epoch [11/30] Batch [250/798]:                     loss = 4170.25146484375\n","Epoch [11/30] Batch [260/798]:                     loss = 3126.972900390625\n","Epoch [11/30] Batch [270/798]:                     loss = 2482.994384765625\n","Epoch [11/30] Batch [280/798]:                     loss = 1911.9852294921875\n","Epoch [11/30] Batch [290/798]:                     loss = 1832.150146484375\n","Epoch [11/30] Batch [300/798]:                     loss = 1624.3912353515625\n","Epoch [11/30] Batch [310/798]:                     loss = 1504.1885986328125\n","Epoch [11/30] Batch [320/798]:                     loss = 1697.9683837890625\n","Epoch [11/30] Batch [330/798]:                     loss = 2553.0390625\n","Epoch [11/30] Batch [340/798]:                     loss = 6153.69580078125\n","Epoch [11/30] Batch [350/798]:                     loss = 7883.37353515625\n","Epoch [11/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [11/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [11/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [11/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [11/30] Batch [400/798]:                     loss = 7182.93603515625\n","Epoch [11/30] Batch [410/798]:                     loss = 4646.18505859375\n","Epoch [11/30] Batch [420/798]:                     loss = 3234.208251953125\n","Epoch [11/30] Batch [430/798]:                     loss = 2547.072265625\n","Epoch [11/30] Batch [440/798]:                     loss = 1945.2701416015625\n","Epoch [11/30] Batch [450/798]:                     loss = 1749.714599609375\n","Epoch [11/30] Batch [460/798]:                     loss = 1668.1990966796875\n","Epoch [11/30] Batch [470/798]:                     loss = 1950.8524169921875\n","Epoch [11/30] Batch [480/798]:                     loss = 4768.9853515625\n","Epoch [11/30] Batch [490/798]:                     loss = 7614.70361328125\n","Epoch [11/30] Batch [500/798]:                     loss = 9470.5234375\n","Epoch [11/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [11/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [11/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [11/30] Batch [540/798]:                     loss = 7635.5810546875\n","Epoch [11/30] Batch [550/798]:                     loss = 5007.40966796875\n","Epoch [11/30] Batch [560/798]:                     loss = 3455.8662109375\n","Epoch [11/30] Batch [570/798]:                     loss = 2372.72607421875\n","Epoch [11/30] Batch [580/798]:                     loss = 2180.00439453125\n","Epoch [11/30] Batch [590/798]:                     loss = 1831.01025390625\n","Epoch [11/30] Batch [600/798]:                     loss = 1613.2186279296875\n","Epoch [11/30] Batch [610/798]:                     loss = 1461.48779296875\n","Epoch [11/30] Batch [620/798]:                     loss = 1562.3565673828125\n","Epoch [11/30] Batch [630/798]:                     loss = 2097.325927734375\n","Epoch [11/30] Batch [640/798]:                     loss = 2984.754638671875\n","Epoch [11/30] Batch [650/798]:                     loss = 3590.59228515625\n","Epoch [11/30] Batch [660/798]:                     loss = 3918.80517578125\n","Epoch [11/30] Batch [670/798]:                     loss = 4059.138916015625\n","Epoch [11/30] Batch [680/798]:                     loss = 3846.36962890625\n","Epoch [11/30] Batch [690/798]:                     loss = 3152.843017578125\n","Epoch [11/30] Batch [700/798]:                     loss = 2625.220703125\n","Epoch [11/30] Batch [710/798]:                     loss = 1951.5906982421875\n","Epoch [11/30] Batch [720/798]:                     loss = 1895.203369140625\n","Epoch [11/30] Batch [730/798]:                     loss = 1666.9671630859375\n","Epoch [11/30] Batch [740/798]:                     loss = 1526.593505859375\n","Epoch [11/30] Batch [750/798]:                     loss = 1549.238525390625\n","Epoch [11/30] Batch [760/798]:                     loss = 2062.70556640625\n","Epoch [11/30] Batch [770/798]:                     loss = 5109.37890625\n","Epoch [11/30] Batch [780/798]:                     loss = 5468.1357421875\n","Epoch [11/30] Batch [790/798]:                     loss = 6481.9462890625\n","Epoch [11/30]: training loss= 3903.104359, training mse loss= 3903.104359, training bce loss= nan\n","             : validation loss= 3426.146071, validation mse loss= 3426.146071, validation bce loss= nan\n","Epoch [12/30] Batch [10/798]:                     loss = 2271.02197265625\n","Epoch [12/30] Batch [20/798]:                     loss = 1872.9146728515625\n","Epoch [12/30] Batch [30/798]:                     loss = 1771.8214111328125\n","Epoch [12/30] Batch [40/798]:                     loss = 1733.681640625\n","Epoch [12/30] Batch [50/798]:                     loss = 2533.908447265625\n","Epoch [12/30] Batch [60/798]:                     loss = 3903.581787109375\n","Epoch [12/30] Batch [70/798]:                     loss = 5935.6123046875\n","Epoch [12/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [12/30] Batch [90/798]:                     loss = 6901.70361328125\n","Epoch [12/30] Batch [100/798]:                     loss = 6155.0166015625\n","Epoch [12/30] Batch [110/798]:                     loss = 5256.88427734375\n","Epoch [12/30] Batch [120/798]:                     loss = 3826.39013671875\n","Epoch [12/30] Batch [130/798]:                     loss = 3172.46142578125\n","Epoch [12/30] Batch [140/798]:                     loss = 2176.160888671875\n","Epoch [12/30] Batch [150/798]:                     loss = 2016.0684814453125\n","Epoch [12/30] Batch [160/798]:                     loss = 1820.799072265625\n","Epoch [12/30] Batch [170/798]:                     loss = 1574.2686767578125\n","Epoch [12/30] Batch [180/798]:                     loss = 1454.501220703125\n","Epoch [12/30] Batch [190/798]:                     loss = 1662.638916015625\n","Epoch [12/30] Batch [200/798]:                     loss = 2322.366455078125\n","Epoch [12/30] Batch [210/798]:                     loss = 3398.206298828125\n","Epoch [12/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [12/30] Batch [230/798]:                     loss = 4714.81787109375\n","Epoch [12/30] Batch [240/798]:                     loss = 4695.9267578125\n","Epoch [12/30] Batch [250/798]:                     loss = 4170.251953125\n","Epoch [12/30] Batch [260/798]:                     loss = 3126.97265625\n","Epoch [12/30] Batch [270/798]:                     loss = 2482.994384765625\n","Epoch [12/30] Batch [280/798]:                     loss = 1911.984130859375\n","Epoch [12/30] Batch [290/798]:                     loss = 1832.15283203125\n","Epoch [12/30] Batch [300/798]:                     loss = 1624.39208984375\n","Epoch [12/30] Batch [310/798]:                     loss = 1504.1884765625\n","Epoch [12/30] Batch [320/798]:                     loss = 1697.9688720703125\n","Epoch [12/30] Batch [330/798]:                     loss = 2553.0361328125\n","Epoch [12/30] Batch [340/798]:                     loss = 6153.6962890625\n","Epoch [12/30] Batch [350/798]:                     loss = 7883.37353515625\n","Epoch [12/30] Batch [360/798]:                     loss = 9368.2880859375\n","Epoch [12/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [12/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [12/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [12/30] Batch [400/798]:                     loss = 7182.93603515625\n","Epoch [12/30] Batch [410/798]:                     loss = 4646.1845703125\n","Epoch [12/30] Batch [420/798]:                     loss = 3234.209228515625\n","Epoch [12/30] Batch [430/798]:                     loss = 2547.072509765625\n","Epoch [12/30] Batch [440/798]:                     loss = 1945.2698974609375\n","Epoch [12/30] Batch [450/798]:                     loss = 1749.71435546875\n","Epoch [12/30] Batch [460/798]:                     loss = 1668.199462890625\n","Epoch [12/30] Batch [470/798]:                     loss = 1950.8521728515625\n","Epoch [12/30] Batch [480/798]:                     loss = 4768.98193359375\n","Epoch [12/30] Batch [490/798]:                     loss = 7614.70654296875\n","Epoch [12/30] Batch [500/798]:                     loss = 9470.525390625\n","Epoch [12/30] Batch [510/798]:                     loss = 9246.044921875\n","Epoch [12/30] Batch [520/798]:                     loss = 9973.9013671875\n","Epoch [12/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [12/30] Batch [540/798]:                     loss = 7635.5849609375\n","Epoch [12/30] Batch [550/798]:                     loss = 5007.41357421875\n","Epoch [12/30] Batch [560/798]:                     loss = 3455.867431640625\n","Epoch [12/30] Batch [570/798]:                     loss = 2372.72705078125\n","Epoch [12/30] Batch [580/798]:                     loss = 2180.00390625\n","Epoch [12/30] Batch [590/798]:                     loss = 1831.0101318359375\n","Epoch [12/30] Batch [600/798]:                     loss = 1613.2177734375\n","Epoch [12/30] Batch [610/798]:                     loss = 1461.488037109375\n","Epoch [12/30] Batch [620/798]:                     loss = 1562.361083984375\n","Epoch [12/30] Batch [630/798]:                     loss = 2097.3310546875\n","Epoch [12/30] Batch [640/798]:                     loss = 2984.7548828125\n","Epoch [12/30] Batch [650/798]:                     loss = 3590.59912109375\n","Epoch [12/30] Batch [660/798]:                     loss = 3918.80859375\n","Epoch [12/30] Batch [670/798]:                     loss = 4059.14013671875\n","Epoch [12/30] Batch [680/798]:                     loss = 3846.37255859375\n","Epoch [12/30] Batch [690/798]:                     loss = 3152.843505859375\n","Epoch [12/30] Batch [700/798]:                     loss = 2625.2216796875\n","Epoch [12/30] Batch [710/798]:                     loss = 1951.59521484375\n","Epoch [12/30] Batch [720/798]:                     loss = 1895.2056884765625\n","Epoch [12/30] Batch [730/798]:                     loss = 1666.9705810546875\n","Epoch [12/30] Batch [740/798]:                     loss = 1526.5899658203125\n","Epoch [12/30] Batch [750/798]:                     loss = 1549.2442626953125\n","Epoch [12/30] Batch [760/798]:                     loss = 2062.70751953125\n","Epoch [12/30] Batch [770/798]:                     loss = 5109.37744140625\n","Epoch [12/30] Batch [780/798]:                     loss = 5468.138671875\n","Epoch [12/30] Batch [790/798]:                     loss = 6481.947265625\n","Epoch [12/30]: training loss= 3903.105032, training mse loss= 3903.105032, training bce loss= nan\n","             : validation loss= 3426.161657, validation mse loss= 3426.161657, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [13/30] Batch [10/798]:                     loss = 2271.02685546875\n","Epoch [13/30] Batch [20/798]:                     loss = 1872.920654296875\n","Epoch [13/30] Batch [30/798]:                     loss = 1771.82373046875\n","Epoch [13/30] Batch [40/798]:                     loss = 1733.6824951171875\n","Epoch [13/30] Batch [50/798]:                     loss = 2533.908935546875\n","Epoch [13/30] Batch [60/798]:                     loss = 3903.579833984375\n","Epoch [13/30] Batch [70/798]:                     loss = 5935.61181640625\n","Epoch [13/30] Batch [80/798]:                     loss = 6422.23876953125\n","Epoch [13/30] Batch [90/798]:                     loss = 6901.703125\n","Epoch [13/30] Batch [100/798]:                     loss = 6155.01611328125\n","Epoch [13/30] Batch [110/798]:                     loss = 5256.88427734375\n","Epoch [13/30] Batch [120/798]:                     loss = 3826.387939453125\n","Epoch [13/30] Batch [130/798]:                     loss = 3172.46142578125\n","Epoch [13/30] Batch [140/798]:                     loss = 2176.160400390625\n","Epoch [13/30] Batch [150/798]:                     loss = 2016.0675048828125\n","Epoch [13/30] Batch [160/798]:                     loss = 1820.7862548828125\n","Epoch [13/30] Batch [170/798]:                     loss = 1574.267578125\n","Epoch [13/30] Batch [180/798]:                     loss = 1454.5008544921875\n","Epoch [13/30] Batch [190/798]:                     loss = 1662.6383056640625\n","Epoch [13/30] Batch [200/798]:                     loss = 2322.366943359375\n","Epoch [13/30] Batch [210/798]:                     loss = 3398.205322265625\n","Epoch [13/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [13/30] Batch [230/798]:                     loss = 4714.81787109375\n","Epoch [13/30] Batch [240/798]:                     loss = 4695.9267578125\n","Epoch [13/30] Batch [250/798]:                     loss = 4170.25146484375\n","Epoch [13/30] Batch [260/798]:                     loss = 3126.972900390625\n","Epoch [13/30] Batch [270/798]:                     loss = 2482.99365234375\n","Epoch [13/30] Batch [280/798]:                     loss = 1911.9840087890625\n","Epoch [13/30] Batch [290/798]:                     loss = 1832.149169921875\n","Epoch [13/30] Batch [300/798]:                     loss = 1624.391845703125\n","Epoch [13/30] Batch [310/798]:                     loss = 1504.1881103515625\n","Epoch [13/30] Batch [320/798]:                     loss = 1697.968994140625\n","Epoch [13/30] Batch [330/798]:                     loss = 2553.03662109375\n","Epoch [13/30] Batch [340/798]:                     loss = 6153.69482421875\n","Epoch [13/30] Batch [350/798]:                     loss = 7883.3720703125\n","Epoch [13/30] Batch [360/798]:                     loss = 9368.28515625\n","Epoch [13/30] Batch [370/798]:                     loss = 10095.279296875\n","Epoch [13/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [13/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [13/30] Batch [400/798]:                     loss = 7182.93408203125\n","Epoch [13/30] Batch [410/798]:                     loss = 4646.18408203125\n","Epoch [13/30] Batch [420/798]:                     loss = 3234.2080078125\n","Epoch [13/30] Batch [430/798]:                     loss = 2547.072021484375\n","Epoch [13/30] Batch [440/798]:                     loss = 1945.2703857421875\n","Epoch [13/30] Batch [450/798]:                     loss = 1749.7138671875\n","Epoch [13/30] Batch [460/798]:                     loss = 1668.2003173828125\n","Epoch [13/30] Batch [470/798]:                     loss = 1950.8531494140625\n","Epoch [13/30] Batch [480/798]:                     loss = 4768.98193359375\n","Epoch [13/30] Batch [490/798]:                     loss = 7614.70556640625\n","Epoch [13/30] Batch [500/798]:                     loss = 9470.525390625\n","Epoch [13/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [13/30] Batch [520/798]:                     loss = 9973.89453125\n","Epoch [13/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [13/30] Batch [540/798]:                     loss = 7635.58056640625\n","Epoch [13/30] Batch [550/798]:                     loss = 5007.40869140625\n","Epoch [13/30] Batch [560/798]:                     loss = 3455.86474609375\n","Epoch [13/30] Batch [570/798]:                     loss = 2372.725830078125\n","Epoch [13/30] Batch [580/798]:                     loss = 2180.00341796875\n","Epoch [13/30] Batch [590/798]:                     loss = 1831.0101318359375\n","Epoch [13/30] Batch [600/798]:                     loss = 1613.2178955078125\n","Epoch [13/30] Batch [610/798]:                     loss = 1461.48779296875\n","Epoch [13/30] Batch [620/798]:                     loss = 1562.3603515625\n","Epoch [13/30] Batch [630/798]:                     loss = 2097.33056640625\n","Epoch [13/30] Batch [640/798]:                     loss = 2984.7568359375\n","Epoch [13/30] Batch [650/798]:                     loss = 3590.59326171875\n","Epoch [13/30] Batch [660/798]:                     loss = 3918.8095703125\n","Epoch [13/30] Batch [670/798]:                     loss = 4059.140380859375\n","Epoch [13/30] Batch [680/798]:                     loss = 3846.376220703125\n","Epoch [13/30] Batch [690/798]:                     loss = 3152.841796875\n","Epoch [13/30] Batch [700/798]:                     loss = 2625.222412109375\n","Epoch [13/30] Batch [710/798]:                     loss = 1951.5943603515625\n","Epoch [13/30] Batch [720/798]:                     loss = 1895.2056884765625\n","Epoch [13/30] Batch [730/798]:                     loss = 1666.97021484375\n","Epoch [13/30] Batch [740/798]:                     loss = 1526.5899658203125\n","Epoch [13/30] Batch [750/798]:                     loss = 1549.2435302734375\n","Epoch [13/30] Batch [760/798]:                     loss = 2062.707763671875\n","Epoch [13/30] Batch [770/798]:                     loss = 5109.3759765625\n","Epoch [13/30] Batch [780/798]:                     loss = 5468.134765625\n","Epoch [13/30] Batch [790/798]:                     loss = 6481.94140625\n","Epoch [13/30]: training loss= 3903.104636, training mse loss= 3903.104636, training bce loss= nan\n","             : validation loss= 3426.152928, validation mse loss= 3426.152928, validation bce loss= nan\n","Epoch [14/30] Batch [10/798]:                     loss = 2271.025146484375\n","Epoch [14/30] Batch [20/798]:                     loss = 1872.921630859375\n","Epoch [14/30] Batch [30/798]:                     loss = 1771.8233642578125\n","Epoch [14/30] Batch [40/798]:                     loss = 1733.682373046875\n","Epoch [14/30] Batch [50/798]:                     loss = 2533.90869140625\n","Epoch [14/30] Batch [60/798]:                     loss = 3903.579345703125\n","Epoch [14/30] Batch [70/798]:                     loss = 5935.61083984375\n","Epoch [14/30] Batch [80/798]:                     loss = 6422.2392578125\n","Epoch [14/30] Batch [90/798]:                     loss = 6901.70361328125\n","Epoch [14/30] Batch [100/798]:                     loss = 6155.015625\n","Epoch [14/30] Batch [110/798]:                     loss = 5256.8837890625\n","Epoch [14/30] Batch [120/798]:                     loss = 3826.387451171875\n","Epoch [14/30] Batch [130/798]:                     loss = 3172.4609375\n","Epoch [14/30] Batch [140/798]:                     loss = 2176.159912109375\n","Epoch [14/30] Batch [150/798]:                     loss = 2016.0672607421875\n","Epoch [14/30] Batch [160/798]:                     loss = 1820.78564453125\n","Epoch [14/30] Batch [170/798]:                     loss = 1574.26806640625\n","Epoch [14/30] Batch [180/798]:                     loss = 1454.500732421875\n","Epoch [14/30] Batch [190/798]:                     loss = 1662.63818359375\n","Epoch [14/30] Batch [200/798]:                     loss = 2322.365966796875\n","Epoch [14/30] Batch [210/798]:                     loss = 3398.205810546875\n","Epoch [14/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [14/30] Batch [230/798]:                     loss = 4714.81689453125\n","Epoch [14/30] Batch [240/798]:                     loss = 4695.923828125\n","Epoch [14/30] Batch [250/798]:                     loss = 4170.24560546875\n","Epoch [14/30] Batch [260/798]:                     loss = 3126.9697265625\n","Epoch [14/30] Batch [270/798]:                     loss = 2482.98779296875\n","Epoch [14/30] Batch [280/798]:                     loss = 1911.975830078125\n","Epoch [14/30] Batch [290/798]:                     loss = 1832.1368408203125\n","Epoch [14/30] Batch [300/798]:                     loss = 1624.3779296875\n","Epoch [14/30] Batch [310/798]:                     loss = 1504.1766357421875\n","Epoch [14/30] Batch [320/798]:                     loss = 1697.962158203125\n","Epoch [14/30] Batch [330/798]:                     loss = 2553.040771484375\n","Epoch [14/30] Batch [340/798]:                     loss = 6153.69384765625\n","Epoch [14/30] Batch [350/798]:                     loss = 7883.3720703125\n","Epoch [14/30] Batch [360/798]:                     loss = 9368.2861328125\n","Epoch [14/30] Batch [370/798]:                     loss = 10095.28125\n","Epoch [14/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [14/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [14/30] Batch [400/798]:                     loss = 7182.93505859375\n","Epoch [14/30] Batch [410/798]:                     loss = 4646.18408203125\n","Epoch [14/30] Batch [420/798]:                     loss = 3234.20849609375\n","Epoch [14/30] Batch [430/798]:                     loss = 2547.0703125\n","Epoch [14/30] Batch [440/798]:                     loss = 1945.2646484375\n","Epoch [14/30] Batch [450/798]:                     loss = 1749.7025146484375\n","Epoch [14/30] Batch [460/798]:                     loss = 1668.1881103515625\n","Epoch [14/30] Batch [470/798]:                     loss = 1950.8497314453125\n","Epoch [14/30] Batch [480/798]:                     loss = 4768.982421875\n","Epoch [14/30] Batch [490/798]:                     loss = 7614.70458984375\n","Epoch [14/30] Batch [500/798]:                     loss = 9470.521484375\n","Epoch [14/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [14/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [14/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [14/30] Batch [540/798]:                     loss = 7635.578125\n","Epoch [14/30] Batch [550/798]:                     loss = 5007.40673828125\n","Epoch [14/30] Batch [560/798]:                     loss = 3455.861083984375\n","Epoch [14/30] Batch [570/798]:                     loss = 2372.723388671875\n","Epoch [14/30] Batch [580/798]:                     loss = 2180.0009765625\n","Epoch [14/30] Batch [590/798]:                     loss = 1831.0068359375\n","Epoch [14/30] Batch [600/798]:                     loss = 1613.2139892578125\n","Epoch [14/30] Batch [610/798]:                     loss = 1461.478271484375\n","Epoch [14/30] Batch [620/798]:                     loss = 1562.3565673828125\n","Epoch [14/30] Batch [630/798]:                     loss = 2097.33349609375\n","Epoch [14/30] Batch [640/798]:                     loss = 2984.75439453125\n","Epoch [14/30] Batch [650/798]:                     loss = 3590.59375\n","Epoch [14/30] Batch [660/798]:                     loss = 3918.806884765625\n","Epoch [14/30] Batch [670/798]:                     loss = 4059.138916015625\n","Epoch [14/30] Batch [680/798]:                     loss = 3846.365234375\n","Epoch [14/30] Batch [690/798]:                     loss = 3152.842041015625\n","Epoch [14/30] Batch [700/798]:                     loss = 2625.218017578125\n","Epoch [14/30] Batch [710/798]:                     loss = 1951.5892333984375\n","Epoch [14/30] Batch [720/798]:                     loss = 1895.2003173828125\n","Epoch [14/30] Batch [730/798]:                     loss = 1666.96533203125\n","Epoch [14/30] Batch [740/798]:                     loss = 1526.58203125\n","Epoch [14/30] Batch [750/798]:                     loss = 1549.2332763671875\n","Epoch [14/30] Batch [760/798]:                     loss = 2062.707763671875\n","Epoch [14/30] Batch [770/798]:                     loss = 5109.376953125\n","Epoch [14/30] Batch [780/798]:                     loss = 5468.138671875\n","Epoch [14/30] Batch [790/798]:                     loss = 6481.94384765625\n","Epoch [14/30]: training loss= 3903.102341, training mse loss= 3903.102341, training bce loss= nan\n","             : validation loss= 3426.160927, validation mse loss= 3426.160927, validation bce loss= nan\n","Epoch [15/30] Batch [10/798]:                     loss = 2271.0205078125\n","Epoch [15/30] Batch [20/798]:                     loss = 1872.9107666015625\n","Epoch [15/30] Batch [30/798]:                     loss = 1771.81591796875\n","Epoch [15/30] Batch [40/798]:                     loss = 1733.6810302734375\n","Epoch [15/30] Batch [50/798]:                     loss = 2533.907958984375\n","Epoch [15/30] Batch [60/798]:                     loss = 3903.582763671875\n","Epoch [15/30] Batch [70/798]:                     loss = 5935.61669921875\n","Epoch [15/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [15/30] Batch [90/798]:                     loss = 6901.7041015625\n","Epoch [15/30] Batch [100/798]:                     loss = 6155.0166015625\n","Epoch [15/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [15/30] Batch [120/798]:                     loss = 3826.38671875\n","Epoch [15/30] Batch [130/798]:                     loss = 3172.4658203125\n","Epoch [15/30] Batch [140/798]:                     loss = 2176.160888671875\n","Epoch [15/30] Batch [150/798]:                     loss = 2016.062255859375\n","Epoch [15/30] Batch [160/798]:                     loss = 1820.781005859375\n","Epoch [15/30] Batch [170/798]:                     loss = 1574.2587890625\n","Epoch [15/30] Batch [180/798]:                     loss = 1454.4888916015625\n","Epoch [15/30] Batch [190/798]:                     loss = 1662.6346435546875\n","Epoch [15/30] Batch [200/798]:                     loss = 2322.366943359375\n","Epoch [15/30] Batch [210/798]:                     loss = 3398.203369140625\n","Epoch [15/30] Batch [220/798]:                     loss = 4484.78515625\n","Epoch [15/30] Batch [230/798]:                     loss = 4714.814453125\n","Epoch [15/30] Batch [240/798]:                     loss = 4695.9228515625\n","Epoch [15/30] Batch [250/798]:                     loss = 4170.24560546875\n","Epoch [15/30] Batch [260/798]:                     loss = 3126.968994140625\n","Epoch [15/30] Batch [270/798]:                     loss = 2482.98828125\n","Epoch [15/30] Batch [280/798]:                     loss = 1911.9755859375\n","Epoch [15/30] Batch [290/798]:                     loss = 1832.1363525390625\n","Epoch [15/30] Batch [300/798]:                     loss = 1624.3785400390625\n","Epoch [15/30] Batch [310/798]:                     loss = 1504.176513671875\n","Epoch [15/30] Batch [320/798]:                     loss = 1697.9615478515625\n","Epoch [15/30] Batch [330/798]:                     loss = 2553.04052734375\n","Epoch [15/30] Batch [340/798]:                     loss = 6153.69384765625\n","Epoch [15/30] Batch [350/798]:                     loss = 7883.37451171875\n","Epoch [15/30] Batch [360/798]:                     loss = 9368.2890625\n","Epoch [15/30] Batch [370/798]:                     loss = 10095.283203125\n","Epoch [15/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [15/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [15/30] Batch [400/798]:                     loss = 7182.93310546875\n","Epoch [15/30] Batch [410/798]:                     loss = 4646.18359375\n","Epoch [15/30] Batch [420/798]:                     loss = 3234.207763671875\n","Epoch [15/30] Batch [430/798]:                     loss = 2547.0703125\n","Epoch [15/30] Batch [440/798]:                     loss = 1945.2646484375\n","Epoch [15/30] Batch [450/798]:                     loss = 1749.702392578125\n","Epoch [15/30] Batch [460/798]:                     loss = 1668.1878662109375\n","Epoch [15/30] Batch [470/798]:                     loss = 1950.8492431640625\n","Epoch [15/30] Batch [480/798]:                     loss = 4768.98291015625\n","Epoch [15/30] Batch [490/798]:                     loss = 7614.70263671875\n","Epoch [15/30] Batch [500/798]:                     loss = 9470.5224609375\n","Epoch [15/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [15/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [15/30] Batch [530/798]:                     loss = 8920.6962890625\n","Epoch [15/30] Batch [540/798]:                     loss = 7635.578125\n","Epoch [15/30] Batch [550/798]:                     loss = 5007.4072265625\n","Epoch [15/30] Batch [560/798]:                     loss = 3455.860107421875\n","Epoch [15/30] Batch [570/798]:                     loss = 2372.723876953125\n","Epoch [15/30] Batch [580/798]:                     loss = 2180.0009765625\n","Epoch [15/30] Batch [590/798]:                     loss = 1831.0068359375\n","Epoch [15/30] Batch [600/798]:                     loss = 1613.2130126953125\n","Epoch [15/30] Batch [610/798]:                     loss = 1461.4786376953125\n","Epoch [15/30] Batch [620/798]:                     loss = 1562.3563232421875\n","Epoch [15/30] Batch [630/798]:                     loss = 2097.3330078125\n","Epoch [15/30] Batch [640/798]:                     loss = 2984.75537109375\n","Epoch [15/30] Batch [650/798]:                     loss = 3590.59228515625\n","Epoch [15/30] Batch [660/798]:                     loss = 3918.807373046875\n","Epoch [15/30] Batch [670/798]:                     loss = 4059.137451171875\n","Epoch [15/30] Batch [680/798]:                     loss = 3846.364990234375\n","Epoch [15/30] Batch [690/798]:                     loss = 3152.841552734375\n","Epoch [15/30] Batch [700/798]:                     loss = 2625.21826171875\n","Epoch [15/30] Batch [710/798]:                     loss = 1951.58984375\n","Epoch [15/30] Batch [720/798]:                     loss = 1895.1998291015625\n","Epoch [15/30] Batch [730/798]:                     loss = 1666.965087890625\n","Epoch [15/30] Batch [740/798]:                     loss = 1526.5810546875\n","Epoch [15/30] Batch [750/798]:                     loss = 1549.228271484375\n","Epoch [15/30] Batch [760/798]:                     loss = 2062.707763671875\n","Epoch [15/30] Batch [770/798]:                     loss = 5109.3828125\n","Epoch [15/30] Batch [780/798]:                     loss = 5468.13916015625\n","Epoch [15/30] Batch [790/798]:                     loss = 6481.94580078125\n","Epoch [15/30]: training loss= 3903.101857, training mse loss= 3903.101857, training bce loss= nan\n","             : validation loss= 3426.151056, validation mse loss= 3426.151056, validation bce loss= nan\n","Epoch [16/30] Batch [10/798]:                     loss = 2271.02099609375\n","Epoch [16/30] Batch [20/798]:                     loss = 1872.9107666015625\n","Epoch [16/30] Batch [30/798]:                     loss = 1771.8145751953125\n","Epoch [16/30] Batch [40/798]:                     loss = 1733.6766357421875\n","Epoch [16/30] Batch [50/798]:                     loss = 2533.906982421875\n","Epoch [16/30] Batch [60/798]:                     loss = 3903.578369140625\n","Epoch [16/30] Batch [70/798]:                     loss = 5935.61181640625\n","Epoch [16/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [16/30] Batch [90/798]:                     loss = 6901.70703125\n","Epoch [16/30] Batch [100/798]:                     loss = 6155.0166015625\n","Epoch [16/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [16/30] Batch [120/798]:                     loss = 3826.38623046875\n","Epoch [16/30] Batch [130/798]:                     loss = 3172.46630859375\n","Epoch [16/30] Batch [140/798]:                     loss = 2176.16064453125\n","Epoch [16/30] Batch [150/798]:                     loss = 2016.0625\n","Epoch [16/30] Batch [160/798]:                     loss = 1820.7802734375\n","Epoch [16/30] Batch [170/798]:                     loss = 1574.2578125\n","Epoch [16/30] Batch [180/798]:                     loss = 1454.489013671875\n","Epoch [16/30] Batch [190/798]:                     loss = 1662.6341552734375\n","Epoch [16/30] Batch [200/798]:                     loss = 2322.366943359375\n","Epoch [16/30] Batch [210/798]:                     loss = 3398.202880859375\n","Epoch [16/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [16/30] Batch [230/798]:                     loss = 4714.81494140625\n","Epoch [16/30] Batch [240/798]:                     loss = 4695.92333984375\n","Epoch [16/30] Batch [250/798]:                     loss = 4170.24560546875\n","Epoch [16/30] Batch [260/798]:                     loss = 3126.969482421875\n","Epoch [16/30] Batch [270/798]:                     loss = 2482.98779296875\n","Epoch [16/30] Batch [280/798]:                     loss = 1911.9757080078125\n","Epoch [16/30] Batch [290/798]:                     loss = 1832.1343994140625\n","Epoch [16/30] Batch [300/798]:                     loss = 1624.374755859375\n","Epoch [16/30] Batch [310/798]:                     loss = 1504.171630859375\n","Epoch [16/30] Batch [320/798]:                     loss = 1697.9591064453125\n","Epoch [16/30] Batch [330/798]:                     loss = 2553.041259765625\n","Epoch [16/30] Batch [340/798]:                     loss = 6153.69384765625\n","Epoch [16/30] Batch [350/798]:                     loss = 7883.373046875\n","Epoch [16/30] Batch [360/798]:                     loss = 9368.291015625\n","Epoch [16/30] Batch [370/798]:                     loss = 10095.283203125\n","Epoch [16/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [16/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [16/30] Batch [400/798]:                     loss = 7182.93408203125\n","Epoch [16/30] Batch [410/798]:                     loss = 4646.18359375\n","Epoch [16/30] Batch [420/798]:                     loss = 3234.214111328125\n","Epoch [16/30] Batch [430/798]:                     loss = 2547.06787109375\n","Epoch [16/30] Batch [440/798]:                     loss = 1945.2650146484375\n","Epoch [16/30] Batch [450/798]:                     loss = 1749.6981201171875\n","Epoch [16/30] Batch [460/798]:                     loss = 1668.1844482421875\n","Epoch [16/30] Batch [470/798]:                     loss = 1950.8458251953125\n","Epoch [16/30] Batch [480/798]:                     loss = 4768.9814453125\n","Epoch [16/30] Batch [490/798]:                     loss = 7614.70166015625\n","Epoch [16/30] Batch [500/798]:                     loss = 9470.5263671875\n","Epoch [16/30] Batch [510/798]:                     loss = 9246.044921875\n","Epoch [16/30] Batch [520/798]:                     loss = 9973.89453125\n","Epoch [16/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [16/30] Batch [540/798]:                     loss = 7635.576171875\n","Epoch [16/30] Batch [550/798]:                     loss = 5007.4072265625\n","Epoch [16/30] Batch [560/798]:                     loss = 3455.861572265625\n","Epoch [16/30] Batch [570/798]:                     loss = 2372.724365234375\n","Epoch [16/30] Batch [580/798]:                     loss = 2180.01953125\n","Epoch [16/30] Batch [590/798]:                     loss = 1831.0230712890625\n","Epoch [16/30] Batch [600/798]:                     loss = 1613.228759765625\n","Epoch [16/30] Batch [610/798]:                     loss = 1461.48486328125\n","Epoch [16/30] Batch [620/798]:                     loss = 1562.363525390625\n","Epoch [16/30] Batch [630/798]:                     loss = 2097.33349609375\n","Epoch [16/30] Batch [640/798]:                     loss = 2984.756591796875\n","Epoch [16/30] Batch [650/798]:                     loss = 3590.5908203125\n","Epoch [16/30] Batch [660/798]:                     loss = 3918.80810546875\n","Epoch [16/30] Batch [670/798]:                     loss = 4059.137939453125\n","Epoch [16/30] Batch [680/798]:                     loss = 3846.3671875\n","Epoch [16/30] Batch [690/798]:                     loss = 3152.84130859375\n","Epoch [16/30] Batch [700/798]:                     loss = 2625.223876953125\n","Epoch [16/30] Batch [710/798]:                     loss = 1951.594482421875\n","Epoch [16/30] Batch [720/798]:                     loss = 1895.212890625\n","Epoch [16/30] Batch [730/798]:                     loss = 1666.98095703125\n","Epoch [16/30] Batch [740/798]:                     loss = 1526.596435546875\n","Epoch [16/30] Batch [750/798]:                     loss = 1549.244384765625\n","Epoch [16/30] Batch [760/798]:                     loss = 2062.706298828125\n","Epoch [16/30] Batch [770/798]:                     loss = 5109.37353515625\n","Epoch [16/30] Batch [780/798]:                     loss = 5468.13671875\n","Epoch [16/30] Batch [790/798]:                     loss = 6481.94091796875\n","Epoch [16/30]: training loss= 3903.103020, training mse loss= 3903.103020, training bce loss= nan\n","             : validation loss= 3426.150602, validation mse loss= 3426.150602, validation bce loss= nan\n","Epoch [17/30] Batch [10/798]:                     loss = 2271.021484375\n","Epoch [17/30] Batch [20/798]:                     loss = 1872.9228515625\n","Epoch [17/30] Batch [30/798]:                     loss = 1771.8258056640625\n","Epoch [17/30] Batch [40/798]:                     loss = 1733.68310546875\n","Epoch [17/30] Batch [50/798]:                     loss = 2533.909912109375\n","Epoch [17/30] Batch [60/798]:                     loss = 3903.579345703125\n","Epoch [17/30] Batch [70/798]:                     loss = 5935.60986328125\n","Epoch [17/30] Batch [80/798]:                     loss = 6422.24072265625\n","Epoch [17/30] Batch [90/798]:                     loss = 6901.70556640625\n","Epoch [17/30] Batch [100/798]:                     loss = 6155.01708984375\n","Epoch [17/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [17/30] Batch [120/798]:                     loss = 3826.38916015625\n","Epoch [17/30] Batch [130/798]:                     loss = 3172.462158203125\n","Epoch [17/30] Batch [140/798]:                     loss = 2176.162109375\n","Epoch [17/30] Batch [150/798]:                     loss = 2016.085205078125\n","Epoch [17/30] Batch [160/798]:                     loss = 1820.79931640625\n","Epoch [17/30] Batch [170/798]:                     loss = 1574.2821044921875\n","Epoch [17/30] Batch [180/798]:                     loss = 1454.5081787109375\n","Epoch [17/30] Batch [190/798]:                     loss = 1662.6405029296875\n","Epoch [17/30] Batch [200/798]:                     loss = 2322.365478515625\n","Epoch [17/30] Batch [210/798]:                     loss = 3398.205078125\n","Epoch [17/30] Batch [220/798]:                     loss = 4484.78369140625\n","Epoch [17/30] Batch [230/798]:                     loss = 4714.81689453125\n","Epoch [17/30] Batch [240/798]:                     loss = 4695.92626953125\n","Epoch [17/30] Batch [250/798]:                     loss = 4170.25341796875\n","Epoch [17/30] Batch [260/798]:                     loss = 3126.97265625\n","Epoch [17/30] Batch [270/798]:                     loss = 2483.00048828125\n","Epoch [17/30] Batch [280/798]:                     loss = 1911.988037109375\n","Epoch [17/30] Batch [290/798]:                     loss = 1832.1529541015625\n","Epoch [17/30] Batch [300/798]:                     loss = 1624.40869140625\n","Epoch [17/30] Batch [310/798]:                     loss = 1504.20068359375\n","Epoch [17/30] Batch [320/798]:                     loss = 1697.9736328125\n","Epoch [17/30] Batch [330/798]:                     loss = 2553.033447265625\n","Epoch [17/30] Batch [340/798]:                     loss = 6153.68994140625\n","Epoch [17/30] Batch [350/798]:                     loss = 7883.3701171875\n","Epoch [17/30] Batch [360/798]:                     loss = 9368.2880859375\n","Epoch [17/30] Batch [370/798]:                     loss = 10095.2822265625\n","Epoch [17/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [17/30] Batch [390/798]:                     loss = 8709.6982421875\n","Epoch [17/30] Batch [400/798]:                     loss = 7182.93310546875\n","Epoch [17/30] Batch [410/798]:                     loss = 4646.18408203125\n","Epoch [17/30] Batch [420/798]:                     loss = 3234.207763671875\n","Epoch [17/30] Batch [430/798]:                     loss = 2547.076904296875\n","Epoch [17/30] Batch [440/798]:                     loss = 1945.2852783203125\n","Epoch [17/30] Batch [450/798]:                     loss = 1749.7301025390625\n","Epoch [17/30] Batch [460/798]:                     loss = 1668.21337890625\n","Epoch [17/30] Batch [470/798]:                     loss = 1950.8519287109375\n","Epoch [17/30] Batch [480/798]:                     loss = 4768.97412109375\n","Epoch [17/30] Batch [490/798]:                     loss = 7614.7021484375\n","Epoch [17/30] Batch [500/798]:                     loss = 9470.5224609375\n","Epoch [17/30] Batch [510/798]:                     loss = 9246.044921875\n","Epoch [17/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [17/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [17/30] Batch [540/798]:                     loss = 7635.5732421875\n","Epoch [17/30] Batch [550/798]:                     loss = 5007.4072265625\n","Epoch [17/30] Batch [560/798]:                     loss = 3455.86376953125\n","Epoch [17/30] Batch [570/798]:                     loss = 2372.726318359375\n","Epoch [17/30] Batch [580/798]:                     loss = 2180.01953125\n","Epoch [17/30] Batch [590/798]:                     loss = 1831.022216796875\n","Epoch [17/30] Batch [600/798]:                     loss = 1613.2252197265625\n","Epoch [17/30] Batch [610/798]:                     loss = 1461.4825439453125\n","Epoch [17/30] Batch [620/798]:                     loss = 1562.3533935546875\n","Epoch [17/30] Batch [630/798]:                     loss = 2097.32666015625\n","Epoch [17/30] Batch [640/798]:                     loss = 2984.754638671875\n","Epoch [17/30] Batch [650/798]:                     loss = 3590.590576171875\n","Epoch [17/30] Batch [660/798]:                     loss = 3918.80615234375\n","Epoch [17/30] Batch [670/798]:                     loss = 4059.137939453125\n","Epoch [17/30] Batch [680/798]:                     loss = 3846.368896484375\n","Epoch [17/30] Batch [690/798]:                     loss = 3152.8388671875\n","Epoch [17/30] Batch [700/798]:                     loss = 2625.221923828125\n","Epoch [17/30] Batch [710/798]:                     loss = 1951.5892333984375\n","Epoch [17/30] Batch [720/798]:                     loss = 1895.21044921875\n","Epoch [17/30] Batch [730/798]:                     loss = 1666.9779052734375\n","Epoch [17/30] Batch [740/798]:                     loss = 1526.6005859375\n","Epoch [17/30] Batch [750/798]:                     loss = 1549.2408447265625\n","Epoch [17/30] Batch [760/798]:                     loss = 2062.703857421875\n","Epoch [17/30] Batch [770/798]:                     loss = 5109.373046875\n","Epoch [17/30] Batch [780/798]:                     loss = 5468.13818359375\n","Epoch [17/30] Batch [790/798]:                     loss = 6481.943359375\n","Epoch [17/30]: training loss= 3903.106387, training mse loss= 3903.106387, training bce loss= nan\n","             : validation loss= 3426.149642, validation mse loss= 3426.149642, validation bce loss= nan\n","Epoch [18/30] Batch [10/798]:                     loss = 2271.018310546875\n","Epoch [18/30] Batch [20/798]:                     loss = 1872.917724609375\n","Epoch [18/30] Batch [30/798]:                     loss = 1771.823486328125\n","Epoch [18/30] Batch [40/798]:                     loss = 1733.6822509765625\n","Epoch [18/30] Batch [50/798]:                     loss = 2533.909423828125\n","Epoch [18/30] Batch [60/798]:                     loss = 3903.578857421875\n","Epoch [18/30] Batch [70/798]:                     loss = 5935.61083984375\n","Epoch [18/30] Batch [80/798]:                     loss = 6422.24072265625\n","Epoch [18/30] Batch [90/798]:                     loss = 6901.7041015625\n","Epoch [18/30] Batch [100/798]:                     loss = 6155.01708984375\n","Epoch [18/30] Batch [110/798]:                     loss = 5256.8857421875\n","Epoch [18/30] Batch [120/798]:                     loss = 3826.390380859375\n","Epoch [18/30] Batch [130/798]:                     loss = 3172.462158203125\n","Epoch [18/30] Batch [140/798]:                     loss = 2176.161865234375\n","Epoch [18/30] Batch [150/798]:                     loss = 2016.08447265625\n","Epoch [18/30] Batch [160/798]:                     loss = 1820.7957763671875\n","Epoch [18/30] Batch [170/798]:                     loss = 1574.271484375\n","Epoch [18/30] Batch [180/798]:                     loss = 1454.4873046875\n","Epoch [18/30] Batch [190/798]:                     loss = 1662.6357421875\n","Epoch [18/30] Batch [200/798]:                     loss = 2322.3720703125\n","Epoch [18/30] Batch [210/798]:                     loss = 3398.205810546875\n","Epoch [18/30] Batch [220/798]:                     loss = 4484.78369140625\n","Epoch [18/30] Batch [230/798]:                     loss = 4714.81494140625\n","Epoch [18/30] Batch [240/798]:                     loss = 4695.9228515625\n","Epoch [18/30] Batch [250/798]:                     loss = 4170.24560546875\n","Epoch [18/30] Batch [260/798]:                     loss = 3126.9697265625\n","Epoch [18/30] Batch [270/798]:                     loss = 2482.987548828125\n","Epoch [18/30] Batch [280/798]:                     loss = 1911.97119140625\n","Epoch [18/30] Batch [290/798]:                     loss = 1832.1322021484375\n","Epoch [18/30] Batch [300/798]:                     loss = 1624.37451171875\n","Epoch [18/30] Batch [310/798]:                     loss = 1504.1708984375\n","Epoch [18/30] Batch [320/798]:                     loss = 1697.9583740234375\n","Epoch [18/30] Batch [330/798]:                     loss = 2553.041015625\n","Epoch [18/30] Batch [340/798]:                     loss = 6153.69384765625\n","Epoch [18/30] Batch [350/798]:                     loss = 7883.37255859375\n","Epoch [18/30] Batch [360/798]:                     loss = 9368.2900390625\n","Epoch [18/30] Batch [370/798]:                     loss = 10095.283203125\n","Epoch [18/30] Batch [380/798]:                     loss = 9855.6767578125\n","Epoch [18/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [18/30] Batch [400/798]:                     loss = 7182.93212890625\n","Epoch [18/30] Batch [410/798]:                     loss = 4646.1826171875\n","Epoch [18/30] Batch [420/798]:                     loss = 3234.2119140625\n","Epoch [18/30] Batch [430/798]:                     loss = 2547.068115234375\n","Epoch [18/30] Batch [440/798]:                     loss = 1945.2645263671875\n","Epoch [18/30] Batch [450/798]:                     loss = 1749.6983642578125\n","Epoch [18/30] Batch [460/798]:                     loss = 1668.184326171875\n","Epoch [18/30] Batch [470/798]:                     loss = 1950.8460693359375\n","Epoch [18/30] Batch [480/798]:                     loss = 4768.9794921875\n","Epoch [18/30] Batch [490/798]:                     loss = 7614.70263671875\n","Epoch [18/30] Batch [500/798]:                     loss = 9470.525390625\n","Epoch [18/30] Batch [510/798]:                     loss = 9246.0458984375\n","Epoch [18/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [18/30] Batch [530/798]:                     loss = 8920.69921875\n","Epoch [18/30] Batch [540/798]:                     loss = 7635.5751953125\n","Epoch [18/30] Batch [550/798]:                     loss = 5007.40673828125\n","Epoch [18/30] Batch [560/798]:                     loss = 3455.860107421875\n","Epoch [18/30] Batch [570/798]:                     loss = 2372.720947265625\n","Epoch [18/30] Batch [580/798]:                     loss = 2179.993408203125\n","Epoch [18/30] Batch [590/798]:                     loss = 1831.00341796875\n","Epoch [18/30] Batch [600/798]:                     loss = 1613.203369140625\n","Epoch [18/30] Batch [610/798]:                     loss = 1461.4744873046875\n","Epoch [18/30] Batch [620/798]:                     loss = 1562.3524169921875\n","Epoch [18/30] Batch [630/798]:                     loss = 2097.326904296875\n","Epoch [18/30] Batch [640/798]:                     loss = 2984.75244140625\n","Epoch [18/30] Batch [650/798]:                     loss = 3590.5927734375\n","Epoch [18/30] Batch [660/798]:                     loss = 3918.8046875\n","Epoch [18/30] Batch [670/798]:                     loss = 4059.1376953125\n","Epoch [18/30] Batch [680/798]:                     loss = 3846.365234375\n","Epoch [18/30] Batch [690/798]:                     loss = 3152.839599609375\n","Epoch [18/30] Batch [700/798]:                     loss = 2625.21630859375\n","Epoch [18/30] Batch [710/798]:                     loss = 1951.5849609375\n","Epoch [18/30] Batch [720/798]:                     loss = 1895.1971435546875\n","Epoch [18/30] Batch [730/798]:                     loss = 1666.9617919921875\n","Epoch [18/30] Batch [740/798]:                     loss = 1526.5860595703125\n","Epoch [18/30] Batch [750/798]:                     loss = 1549.22802734375\n","Epoch [18/30] Batch [760/798]:                     loss = 2062.70458984375\n","Epoch [18/30] Batch [770/798]:                     loss = 5109.37646484375\n","Epoch [18/30] Batch [780/798]:                     loss = 5468.1357421875\n","Epoch [18/30] Batch [790/798]:                     loss = 6481.93994140625\n","Epoch [18/30]: training loss= 3903.101422, training mse loss= 3903.101422, training bce loss= nan\n","             : validation loss= 3426.157810, validation mse loss= 3426.157810, validation bce loss= nan\n","Epoch [19/30] Batch [10/798]:                     loss = 2271.0185546875\n","Epoch [19/30] Batch [20/798]:                     loss = 1872.9063720703125\n","Epoch [19/30] Batch [30/798]:                     loss = 1771.8126220703125\n","Epoch [19/30] Batch [40/798]:                     loss = 1733.6788330078125\n","Epoch [19/30] Batch [50/798]:                     loss = 2533.908935546875\n","Epoch [19/30] Batch [60/798]:                     loss = 3903.579833984375\n","Epoch [19/30] Batch [70/798]:                     loss = 5935.61083984375\n","Epoch [19/30] Batch [80/798]:                     loss = 6422.23876953125\n","Epoch [19/30] Batch [90/798]:                     loss = 6901.70361328125\n","Epoch [19/30] Batch [100/798]:                     loss = 6155.01513671875\n","Epoch [19/30] Batch [110/798]:                     loss = 5256.8828125\n","Epoch [19/30] Batch [120/798]:                     loss = 3826.38720703125\n","Epoch [19/30] Batch [130/798]:                     loss = 3172.461181640625\n","Epoch [19/30] Batch [140/798]:                     loss = 2176.158447265625\n","Epoch [19/30] Batch [150/798]:                     loss = 2016.0625\n","Epoch [19/30] Batch [160/798]:                     loss = 1820.7847900390625\n","Epoch [19/30] Batch [170/798]:                     loss = 1574.2591552734375\n","Epoch [19/30] Batch [180/798]:                     loss = 1454.486572265625\n","Epoch [19/30] Batch [190/798]:                     loss = 1662.634521484375\n","Epoch [19/30] Batch [200/798]:                     loss = 2322.365966796875\n","Epoch [19/30] Batch [210/798]:                     loss = 3398.203369140625\n","Epoch [19/30] Batch [220/798]:                     loss = 4484.7841796875\n","Epoch [19/30] Batch [230/798]:                     loss = 4714.81396484375\n","Epoch [19/30] Batch [240/798]:                     loss = 4695.92431640625\n","Epoch [19/30] Batch [250/798]:                     loss = 4170.24560546875\n","Epoch [19/30] Batch [260/798]:                     loss = 3126.969482421875\n","Epoch [19/30] Batch [270/798]:                     loss = 2482.9873046875\n","Epoch [19/30] Batch [280/798]:                     loss = 1911.9752197265625\n","Epoch [19/30] Batch [290/798]:                     loss = 1832.1317138671875\n","Epoch [19/30] Batch [300/798]:                     loss = 1624.3743896484375\n","Epoch [19/30] Batch [310/798]:                     loss = 1504.1715087890625\n","Epoch [19/30] Batch [320/798]:                     loss = 1697.9586181640625\n","Epoch [19/30] Batch [330/798]:                     loss = 2553.04052734375\n","Epoch [19/30] Batch [340/798]:                     loss = 6153.69384765625\n","Epoch [19/30] Batch [350/798]:                     loss = 7883.37060546875\n","Epoch [19/30] Batch [360/798]:                     loss = 9368.2861328125\n","Epoch [19/30] Batch [370/798]:                     loss = 10095.28125\n","Epoch [19/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [19/30] Batch [390/798]:                     loss = 8709.6982421875\n","Epoch [19/30] Batch [400/798]:                     loss = 7182.93212890625\n","Epoch [19/30] Batch [410/798]:                     loss = 4646.1826171875\n","Epoch [19/30] Batch [420/798]:                     loss = 3234.213623046875\n","Epoch [19/30] Batch [430/798]:                     loss = 2547.06787109375\n","Epoch [19/30] Batch [440/798]:                     loss = 1945.264892578125\n","Epoch [19/30] Batch [450/798]:                     loss = 1749.6982421875\n","Epoch [19/30] Batch [460/798]:                     loss = 1668.1844482421875\n","Epoch [19/30] Batch [470/798]:                     loss = 1950.84521484375\n","Epoch [19/30] Batch [480/798]:                     loss = 4768.97802734375\n","Epoch [19/30] Batch [490/798]:                     loss = 7614.70166015625\n","Epoch [19/30] Batch [500/798]:                     loss = 9470.5224609375\n","Epoch [19/30] Batch [510/798]:                     loss = 9246.0439453125\n","Epoch [19/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [19/30] Batch [530/798]:                     loss = 8920.6982421875\n","Epoch [19/30] Batch [540/798]:                     loss = 7635.57470703125\n","Epoch [19/30] Batch [550/798]:                     loss = 5007.40771484375\n","Epoch [19/30] Batch [560/798]:                     loss = 3455.862060546875\n","Epoch [19/30] Batch [570/798]:                     loss = 2372.718994140625\n","Epoch [19/30] Batch [580/798]:                     loss = 2179.9931640625\n","Epoch [19/30] Batch [590/798]:                     loss = 1831.00146484375\n","Epoch [19/30] Batch [600/798]:                     loss = 1613.202392578125\n","Epoch [19/30] Batch [610/798]:                     loss = 1461.4739990234375\n","Epoch [19/30] Batch [620/798]:                     loss = 1562.3387451171875\n","Epoch [19/30] Batch [630/798]:                     loss = 2097.3203125\n","Epoch [19/30] Batch [640/798]:                     loss = 2984.75244140625\n","Epoch [19/30] Batch [650/798]:                     loss = 3590.58984375\n","Epoch [19/30] Batch [660/798]:                     loss = 3918.8017578125\n","Epoch [19/30] Batch [670/798]:                     loss = 4059.13671875\n","Epoch [19/30] Batch [680/798]:                     loss = 3846.3662109375\n","Epoch [19/30] Batch [690/798]:                     loss = 3152.839111328125\n","Epoch [19/30] Batch [700/798]:                     loss = 2625.220703125\n","Epoch [19/30] Batch [710/798]:                     loss = 1951.583740234375\n","Epoch [19/30] Batch [720/798]:                     loss = 1895.19677734375\n","Epoch [19/30] Batch [730/798]:                     loss = 1666.9605712890625\n","Epoch [19/30] Batch [740/798]:                     loss = 1526.5828857421875\n","Epoch [19/30] Batch [750/798]:                     loss = 1549.222900390625\n","Epoch [19/30] Batch [760/798]:                     loss = 2062.701171875\n","Epoch [19/30] Batch [770/798]:                     loss = 5109.37939453125\n","Epoch [19/30] Batch [780/798]:                     loss = 5468.140625\n","Epoch [19/30] Batch [790/798]:                     loss = 6481.94189453125\n","Epoch [19/30]: training loss= 3903.099740, training mse loss= 3903.099740, training bce loss= nan\n","             : validation loss= 3426.150831, validation mse loss= 3426.150831, validation bce loss= nan\n","Epoch [20/30] Batch [10/798]:                     loss = 2271.016357421875\n","Epoch [20/30] Batch [20/798]:                     loss = 1872.9039306640625\n","Epoch [20/30] Batch [30/798]:                     loss = 1771.812744140625\n","Epoch [20/30] Batch [40/798]:                     loss = 1733.6705322265625\n","Epoch [20/30] Batch [50/798]:                     loss = 2533.90673828125\n","Epoch [20/30] Batch [60/798]:                     loss = 3903.58154296875\n","Epoch [20/30] Batch [70/798]:                     loss = 5935.63330078125\n","Epoch [20/30] Batch [80/798]:                     loss = 6422.24462890625\n","Epoch [20/30] Batch [90/798]:                     loss = 6901.705078125\n","Epoch [20/30] Batch [100/798]:                     loss = 6155.01611328125\n","Epoch [20/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [20/30] Batch [120/798]:                     loss = 3826.390380859375\n","Epoch [20/30] Batch [130/798]:                     loss = 3172.4619140625\n","Epoch [20/30] Batch [140/798]:                     loss = 2176.160888671875\n","Epoch [20/30] Batch [150/798]:                     loss = 2016.0615234375\n","Epoch [20/30] Batch [160/798]:                     loss = 1820.7838134765625\n","Epoch [20/30] Batch [170/798]:                     loss = 1574.2576904296875\n","Epoch [20/30] Batch [180/798]:                     loss = 1454.4803466796875\n","Epoch [20/30] Batch [190/798]:                     loss = 1662.6259765625\n","Epoch [20/30] Batch [200/798]:                     loss = 2322.35888671875\n","Epoch [20/30] Batch [210/798]:                     loss = 3398.199462890625\n","Epoch [20/30] Batch [220/798]:                     loss = 4484.78271484375\n","Epoch [20/30] Batch [230/798]:                     loss = 4714.81591796875\n","Epoch [20/30] Batch [240/798]:                     loss = 4695.9267578125\n","Epoch [20/30] Batch [250/798]:                     loss = 4170.24853515625\n","Epoch [20/30] Batch [260/798]:                     loss = 3126.97119140625\n","Epoch [20/30] Batch [270/798]:                     loss = 2482.986328125\n","Epoch [20/30] Batch [280/798]:                     loss = 1911.9661865234375\n","Epoch [20/30] Batch [290/798]:                     loss = 1832.1318359375\n","Epoch [20/30] Batch [300/798]:                     loss = 1624.3740234375\n","Epoch [20/30] Batch [310/798]:                     loss = 1504.17138671875\n","Epoch [20/30] Batch [320/798]:                     loss = 1697.961181640625\n","Epoch [20/30] Batch [330/798]:                     loss = 2553.0498046875\n","Epoch [20/30] Batch [340/798]:                     loss = 6153.69775390625\n","Epoch [20/30] Batch [350/798]:                     loss = 7883.3681640625\n","Epoch [20/30] Batch [360/798]:                     loss = 9368.2880859375\n","Epoch [20/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [20/30] Batch [380/798]:                     loss = 9855.6787109375\n","Epoch [20/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [20/30] Batch [400/798]:                     loss = 7182.93310546875\n","Epoch [20/30] Batch [410/798]:                     loss = 4646.18359375\n","Epoch [20/30] Batch [420/798]:                     loss = 3234.210693359375\n","Epoch [20/30] Batch [430/798]:                     loss = 2547.0654296875\n","Epoch [20/30] Batch [440/798]:                     loss = 1945.265869140625\n","Epoch [20/30] Batch [450/798]:                     loss = 1749.68359375\n","Epoch [20/30] Batch [460/798]:                     loss = 1668.16552734375\n","Epoch [20/30] Batch [470/798]:                     loss = 1950.8524169921875\n","Epoch [20/30] Batch [480/798]:                     loss = 4769.00146484375\n","Epoch [20/30] Batch [490/798]:                     loss = 7614.70166015625\n","Epoch [20/30] Batch [500/798]:                     loss = 9470.521484375\n","Epoch [20/30] Batch [510/798]:                     loss = 9246.04296875\n","Epoch [20/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [20/30] Batch [530/798]:                     loss = 8920.69921875\n","Epoch [20/30] Batch [540/798]:                     loss = 7635.57470703125\n","Epoch [20/30] Batch [550/798]:                     loss = 5007.41064453125\n","Epoch [20/30] Batch [560/798]:                     loss = 3455.86376953125\n","Epoch [20/30] Batch [570/798]:                     loss = 2372.718994140625\n","Epoch [20/30] Batch [580/798]:                     loss = 2179.987060546875\n","Epoch [20/30] Batch [590/798]:                     loss = 1830.979248046875\n","Epoch [20/30] Batch [600/798]:                     loss = 1613.1971435546875\n","Epoch [20/30] Batch [610/798]:                     loss = 1461.465576171875\n","Epoch [20/30] Batch [620/798]:                     loss = 1562.3505859375\n","Epoch [20/30] Batch [630/798]:                     loss = 2097.3291015625\n","Epoch [20/30] Batch [640/798]:                     loss = 2984.767822265625\n","Epoch [20/30] Batch [650/798]:                     loss = 3590.599365234375\n","Epoch [20/30] Batch [660/798]:                     loss = 3918.810302734375\n","Epoch [20/30] Batch [670/798]:                     loss = 4059.136474609375\n","Epoch [20/30] Batch [680/798]:                     loss = 3846.363525390625\n","Epoch [20/30] Batch [690/798]:                     loss = 3152.841064453125\n","Epoch [20/30] Batch [700/798]:                     loss = 2625.22021484375\n","Epoch [20/30] Batch [710/798]:                     loss = 1951.584228515625\n","Epoch [20/30] Batch [720/798]:                     loss = 1895.19775390625\n","Epoch [20/30] Batch [730/798]:                     loss = 1666.9617919921875\n","Epoch [20/30] Batch [740/798]:                     loss = 1526.576171875\n","Epoch [20/30] Batch [750/798]:                     loss = 1549.2249755859375\n","Epoch [20/30] Batch [760/798]:                     loss = 2062.714599609375\n","Epoch [20/30] Batch [770/798]:                     loss = 5109.373046875\n","Epoch [20/30] Batch [780/798]:                     loss = 5468.1357421875\n","Epoch [20/30] Batch [790/798]:                     loss = 6481.9384765625\n","Epoch [20/30]: training loss= 3903.099473, training mse loss= 3903.099473, training bce loss= nan\n","             : validation loss= 3426.171304, validation mse loss= 3426.171304, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [21/30] Batch [10/798]:                     loss = 2270.992919921875\n","Epoch [21/30] Batch [20/798]:                     loss = 1872.892822265625\n","Epoch [21/30] Batch [30/798]:                     loss = 1771.8101806640625\n","Epoch [21/30] Batch [40/798]:                     loss = 1733.6744384765625\n","Epoch [21/30] Batch [50/798]:                     loss = 2533.906982421875\n","Epoch [21/30] Batch [60/798]:                     loss = 3903.574951171875\n","Epoch [21/30] Batch [70/798]:                     loss = 5935.60498046875\n","Epoch [21/30] Batch [80/798]:                     loss = 6422.23779296875\n","Epoch [21/30] Batch [90/798]:                     loss = 6901.6982421875\n","Epoch [21/30] Batch [100/798]:                     loss = 6155.015625\n","Epoch [21/30] Batch [110/798]:                     loss = 5256.88525390625\n","Epoch [21/30] Batch [120/798]:                     loss = 3826.390380859375\n","Epoch [21/30] Batch [130/798]:                     loss = 3172.4619140625\n","Epoch [21/30] Batch [140/798]:                     loss = 2176.155517578125\n","Epoch [21/30] Batch [150/798]:                     loss = 2016.0361328125\n","Epoch [21/30] Batch [160/798]:                     loss = 1820.76171875\n","Epoch [21/30] Batch [170/798]:                     loss = 1574.2376708984375\n","Epoch [21/30] Batch [180/798]:                     loss = 1454.48291015625\n","Epoch [21/30] Batch [190/798]:                     loss = 1662.6292724609375\n","Epoch [21/30] Batch [200/798]:                     loss = 2322.368896484375\n","Epoch [21/30] Batch [210/798]:                     loss = 3398.199951171875\n","Epoch [21/30] Batch [220/798]:                     loss = 4484.78125\n","Epoch [21/30] Batch [230/798]:                     loss = 4714.81396484375\n","Epoch [21/30] Batch [240/798]:                     loss = 4695.92724609375\n","Epoch [21/30] Batch [250/798]:                     loss = 4170.24755859375\n","Epoch [21/30] Batch [260/798]:                     loss = 3126.97119140625\n","Epoch [21/30] Batch [270/798]:                     loss = 2482.98583984375\n","Epoch [21/30] Batch [280/798]:                     loss = 1911.9669189453125\n","Epoch [21/30] Batch [290/798]:                     loss = 1832.1082763671875\n","Epoch [21/30] Batch [300/798]:                     loss = 1624.351806640625\n","Epoch [21/30] Batch [310/798]:                     loss = 1504.152587890625\n","Epoch [21/30] Batch [320/798]:                     loss = 1697.9627685546875\n","Epoch [21/30] Batch [330/798]:                     loss = 2553.064697265625\n","Epoch [21/30] Batch [340/798]:                     loss = 6153.69091796875\n","Epoch [21/30] Batch [350/798]:                     loss = 7883.37109375\n","Epoch [21/30] Batch [360/798]:                     loss = 9368.28515625\n","Epoch [21/30] Batch [370/798]:                     loss = 10095.283203125\n","Epoch [21/30] Batch [380/798]:                     loss = 9855.673828125\n","Epoch [21/30] Batch [390/798]:                     loss = 8709.6982421875\n","Epoch [21/30] Batch [400/798]:                     loss = 7182.93115234375\n","Epoch [21/30] Batch [410/798]:                     loss = 4646.18359375\n","Epoch [21/30] Batch [420/798]:                     loss = 3234.2119140625\n","Epoch [21/30] Batch [430/798]:                     loss = 2547.060302734375\n","Epoch [21/30] Batch [440/798]:                     loss = 1945.2457275390625\n","Epoch [21/30] Batch [450/798]:                     loss = 1749.6744384765625\n","Epoch [21/30] Batch [460/798]:                     loss = 1668.164794921875\n","Epoch [21/30] Batch [470/798]:                     loss = 1950.8499755859375\n","Epoch [21/30] Batch [480/798]:                     loss = 4768.9736328125\n","Epoch [21/30] Batch [490/798]:                     loss = 7614.70068359375\n","Epoch [21/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [21/30] Batch [510/798]:                     loss = 9246.04296875\n","Epoch [21/30] Batch [520/798]:                     loss = 9973.8955078125\n","Epoch [21/30] Batch [530/798]:                     loss = 8920.69921875\n","Epoch [21/30] Batch [540/798]:                     loss = 7635.57421875\n","Epoch [21/30] Batch [550/798]:                     loss = 5007.41015625\n","Epoch [21/30] Batch [560/798]:                     loss = 3455.861572265625\n","Epoch [21/30] Batch [570/798]:                     loss = 2372.719482421875\n","Epoch [21/30] Batch [580/798]:                     loss = 2179.97412109375\n","Epoch [21/30] Batch [590/798]:                     loss = 1830.9774169921875\n","Epoch [21/30] Batch [600/798]:                     loss = 1613.19580078125\n","Epoch [21/30] Batch [610/798]:                     loss = 1461.458251953125\n","Epoch [21/30] Batch [620/798]:                     loss = 1562.350341796875\n","Epoch [21/30] Batch [630/798]:                     loss = 2097.32861328125\n","Epoch [21/30] Batch [640/798]:                     loss = 2984.766845703125\n","Epoch [21/30] Batch [650/798]:                     loss = 3590.601318359375\n","Epoch [21/30] Batch [660/798]:                     loss = 3918.802734375\n","Epoch [21/30] Batch [670/798]:                     loss = 4059.13525390625\n","Epoch [21/30] Batch [680/798]:                     loss = 3846.36279296875\n","Epoch [21/30] Batch [690/798]:                     loss = 3152.841064453125\n","Epoch [21/30] Batch [700/798]:                     loss = 2625.21826171875\n","Epoch [21/30] Batch [710/798]:                     loss = 1951.5830078125\n","Epoch [21/30] Batch [720/798]:                     loss = 1895.197998046875\n","Epoch [21/30] Batch [730/798]:                     loss = 1666.954345703125\n","Epoch [21/30] Batch [740/798]:                     loss = 1526.574462890625\n","Epoch [21/30] Batch [750/798]:                     loss = 1549.2225341796875\n","Epoch [21/30] Batch [760/798]:                     loss = 2062.703125\n","Epoch [21/30] Batch [770/798]:                     loss = 5109.36962890625\n","Epoch [21/30] Batch [780/798]:                     loss = 5468.1416015625\n","Epoch [21/30] Batch [790/798]:                     loss = 6481.93701171875\n","Epoch [21/30]: training loss= 3903.095858, training mse loss= 3903.095858, training bce loss= nan\n","             : validation loss= 3426.148988, validation mse loss= 3426.148988, validation bce loss= nan\n","Epoch [22/30] Batch [10/798]:                     loss = 2271.0068359375\n","Epoch [22/30] Batch [20/798]:                     loss = 1872.889404296875\n","Epoch [22/30] Batch [30/798]:                     loss = 1771.8056640625\n","Epoch [22/30] Batch [40/798]:                     loss = 1733.670654296875\n","Epoch [22/30] Batch [50/798]:                     loss = 2533.906005859375\n","Epoch [22/30] Batch [60/798]:                     loss = 3903.574462890625\n","Epoch [22/30] Batch [70/798]:                     loss = 5935.60986328125\n","Epoch [22/30] Batch [80/798]:                     loss = 6422.24169921875\n","Epoch [22/30] Batch [90/798]:                     loss = 6901.69970703125\n","Epoch [22/30] Batch [100/798]:                     loss = 6155.0146484375\n","Epoch [22/30] Batch [110/798]:                     loss = 5256.88427734375\n","Epoch [22/30] Batch [120/798]:                     loss = 3826.391845703125\n","Epoch [22/30] Batch [130/798]:                     loss = 3172.4609375\n","Epoch [22/30] Batch [140/798]:                     loss = 2176.15625\n","Epoch [22/30] Batch [150/798]:                     loss = 2016.0352783203125\n","Epoch [22/30] Batch [160/798]:                     loss = 1820.7681884765625\n","Epoch [22/30] Batch [170/798]:                     loss = 1574.23046875\n","Epoch [22/30] Batch [180/798]:                     loss = 1454.4798583984375\n","Epoch [22/30] Batch [190/798]:                     loss = 1662.6259765625\n","Epoch [22/30] Batch [200/798]:                     loss = 2322.362060546875\n","Epoch [22/30] Batch [210/798]:                     loss = 3398.196044921875\n","Epoch [22/30] Batch [220/798]:                     loss = 4484.77880859375\n","Epoch [22/30] Batch [230/798]:                     loss = 4714.81494140625\n","Epoch [22/30] Batch [240/798]:                     loss = 4695.92578125\n","Epoch [22/30] Batch [250/798]:                     loss = 4170.24658203125\n","Epoch [22/30] Batch [260/798]:                     loss = 3126.970947265625\n","Epoch [22/30] Batch [270/798]:                     loss = 2482.986328125\n","Epoch [22/30] Batch [280/798]:                     loss = 1911.966064453125\n","Epoch [22/30] Batch [290/798]:                     loss = 1832.1109619140625\n","Epoch [22/30] Batch [300/798]:                     loss = 1624.3507080078125\n","Epoch [22/30] Batch [310/798]:                     loss = 1504.1485595703125\n","Epoch [22/30] Batch [320/798]:                     loss = 1697.9674072265625\n","Epoch [22/30] Batch [330/798]:                     loss = 2553.0498046875\n","Epoch [22/30] Batch [340/798]:                     loss = 6153.6904296875\n","Epoch [22/30] Batch [350/798]:                     loss = 7883.37109375\n","Epoch [22/30] Batch [360/798]:                     loss = 9368.2841796875\n","Epoch [22/30] Batch [370/798]:                     loss = 10095.2822265625\n","Epoch [22/30] Batch [380/798]:                     loss = 9855.673828125\n","Epoch [22/30] Batch [390/798]:                     loss = 8709.6982421875\n","Epoch [22/30] Batch [400/798]:                     loss = 7182.9306640625\n","Epoch [22/30] Batch [410/798]:                     loss = 4646.18359375\n","Epoch [22/30] Batch [420/798]:                     loss = 3234.213134765625\n","Epoch [22/30] Batch [430/798]:                     loss = 2547.060302734375\n","Epoch [22/30] Batch [440/798]:                     loss = 1945.250732421875\n","Epoch [22/30] Batch [450/798]:                     loss = 1749.6773681640625\n","Epoch [22/30] Batch [460/798]:                     loss = 1668.1658935546875\n","Epoch [22/30] Batch [470/798]:                     loss = 1950.8426513671875\n","Epoch [22/30] Batch [480/798]:                     loss = 4768.97216796875\n","Epoch [22/30] Batch [490/798]:                     loss = 7614.701171875\n","Epoch [22/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [22/30] Batch [510/798]:                     loss = 9246.0439453125\n","Epoch [22/30] Batch [520/798]:                     loss = 9973.89453125\n","Epoch [22/30] Batch [530/798]:                     loss = 8920.69921875\n","Epoch [22/30] Batch [540/798]:                     loss = 7635.576171875\n","Epoch [22/30] Batch [550/798]:                     loss = 5007.40966796875\n","Epoch [22/30] Batch [560/798]:                     loss = 3455.863525390625\n","Epoch [22/30] Batch [570/798]:                     loss = 2372.714599609375\n","Epoch [22/30] Batch [580/798]:                     loss = 2179.9755859375\n","Epoch [22/30] Batch [590/798]:                     loss = 1830.980224609375\n","Epoch [22/30] Batch [600/798]:                     loss = 1613.1973876953125\n","Epoch [22/30] Batch [610/798]:                     loss = 1461.45947265625\n","Epoch [22/30] Batch [620/798]:                     loss = 1562.35107421875\n","Epoch [22/30] Batch [630/798]:                     loss = 2097.32666015625\n","Epoch [22/30] Batch [640/798]:                     loss = 2984.757568359375\n","Epoch [22/30] Batch [650/798]:                     loss = 3590.5927734375\n","Epoch [22/30] Batch [660/798]:                     loss = 3918.803955078125\n","Epoch [22/30] Batch [670/798]:                     loss = 4059.13525390625\n","Epoch [22/30] Batch [680/798]:                     loss = 3846.363037109375\n","Epoch [22/30] Batch [690/798]:                     loss = 3152.840576171875\n","Epoch [22/30] Batch [700/798]:                     loss = 2625.22021484375\n","Epoch [22/30] Batch [710/798]:                     loss = 1951.5816650390625\n","Epoch [22/30] Batch [720/798]:                     loss = 1895.197021484375\n","Epoch [22/30] Batch [730/798]:                     loss = 1666.952392578125\n","Epoch [22/30] Batch [740/798]:                     loss = 1526.572998046875\n","Epoch [22/30] Batch [750/798]:                     loss = 1549.225341796875\n","Epoch [22/30] Batch [760/798]:                     loss = 2062.70556640625\n","Epoch [22/30] Batch [770/798]:                     loss = 5109.3701171875\n","Epoch [22/30] Batch [780/798]:                     loss = 5468.13330078125\n","Epoch [22/30] Batch [790/798]:                     loss = 6481.9375\n","Epoch [22/30]: training loss= 3903.094985, training mse loss= 3903.094985, training bce loss= nan\n","             : validation loss= 3426.148496, validation mse loss= 3426.148496, validation bce loss= nan\n","Epoch [23/30] Batch [10/798]:                     loss = 2270.986083984375\n","Epoch [23/30] Batch [20/798]:                     loss = 1872.889404296875\n","Epoch [23/30] Batch [30/798]:                     loss = 1771.807373046875\n","Epoch [23/30] Batch [40/798]:                     loss = 1733.6708984375\n","Epoch [23/30] Batch [50/798]:                     loss = 2533.90478515625\n","Epoch [23/30] Batch [60/798]:                     loss = 3903.574462890625\n","Epoch [23/30] Batch [70/798]:                     loss = 5935.61962890625\n","Epoch [23/30] Batch [80/798]:                     loss = 6422.240234375\n","Epoch [23/30] Batch [90/798]:                     loss = 6901.69921875\n","Epoch [23/30] Batch [100/798]:                     loss = 6155.015625\n","Epoch [23/30] Batch [110/798]:                     loss = 5256.88427734375\n","Epoch [23/30] Batch [120/798]:                     loss = 3826.391357421875\n","Epoch [23/30] Batch [130/798]:                     loss = 3172.46044921875\n","Epoch [23/30] Batch [140/798]:                     loss = 2176.154541015625\n","Epoch [23/30] Batch [150/798]:                     loss = 2016.0347900390625\n","Epoch [23/30] Batch [160/798]:                     loss = 1820.7591552734375\n","Epoch [23/30] Batch [170/798]:                     loss = 1574.2330322265625\n","Epoch [23/30] Batch [180/798]:                     loss = 1454.4822998046875\n","Epoch [23/30] Batch [190/798]:                     loss = 1662.6259765625\n","Epoch [23/30] Batch [200/798]:                     loss = 2322.363037109375\n","Epoch [23/30] Batch [210/798]:                     loss = 3398.195556640625\n","Epoch [23/30] Batch [220/798]:                     loss = 4484.779296875\n","Epoch [23/30] Batch [230/798]:                     loss = 4714.81494140625\n","Epoch [23/30] Batch [240/798]:                     loss = 4695.9267578125\n","Epoch [23/30] Batch [250/798]:                     loss = 4170.2470703125\n","Epoch [23/30] Batch [260/798]:                     loss = 3126.970947265625\n","Epoch [23/30] Batch [270/798]:                     loss = 2482.986572265625\n","Epoch [23/30] Batch [280/798]:                     loss = 1911.9661865234375\n","Epoch [23/30] Batch [290/798]:                     loss = 1832.1094970703125\n","Epoch [23/30] Batch [300/798]:                     loss = 1624.351318359375\n","Epoch [23/30] Batch [310/798]:                     loss = 1504.149169921875\n","Epoch [23/30] Batch [320/798]:                     loss = 1697.9649658203125\n","Epoch [23/30] Batch [330/798]:                     loss = 2553.0498046875\n","Epoch [23/30] Batch [340/798]:                     loss = 6153.689453125\n","Epoch [23/30] Batch [350/798]:                     loss = 7883.37060546875\n","Epoch [23/30] Batch [360/798]:                     loss = 9368.2861328125\n","Epoch [23/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [23/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [23/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [23/30] Batch [400/798]:                     loss = 7182.93115234375\n","Epoch [23/30] Batch [410/798]:                     loss = 4646.1826171875\n","Epoch [23/30] Batch [420/798]:                     loss = 3234.208984375\n","Epoch [23/30] Batch [430/798]:                     loss = 2547.06103515625\n","Epoch [23/30] Batch [440/798]:                     loss = 1945.2486572265625\n","Epoch [23/30] Batch [450/798]:                     loss = 1749.6744384765625\n","Epoch [23/30] Batch [460/798]:                     loss = 1668.163330078125\n","Epoch [23/30] Batch [470/798]:                     loss = 1950.8416748046875\n","Epoch [23/30] Batch [480/798]:                     loss = 4768.97119140625\n","Epoch [23/30] Batch [490/798]:                     loss = 7614.69970703125\n","Epoch [23/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [23/30] Batch [510/798]:                     loss = 9246.041015625\n","Epoch [23/30] Batch [520/798]:                     loss = 9973.89453125\n","Epoch [23/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [23/30] Batch [540/798]:                     loss = 7635.57666015625\n","Epoch [23/30] Batch [550/798]:                     loss = 5007.4091796875\n","Epoch [23/30] Batch [560/798]:                     loss = 3455.861572265625\n","Epoch [23/30] Batch [570/798]:                     loss = 2372.71484375\n","Epoch [23/30] Batch [580/798]:                     loss = 2179.97314453125\n","Epoch [23/30] Batch [590/798]:                     loss = 1830.976318359375\n","Epoch [23/30] Batch [600/798]:                     loss = 1613.1820068359375\n","Epoch [23/30] Batch [610/798]:                     loss = 1461.4569091796875\n","Epoch [23/30] Batch [620/798]:                     loss = 1562.3497314453125\n","Epoch [23/30] Batch [630/798]:                     loss = 2097.3193359375\n","Epoch [23/30] Batch [640/798]:                     loss = 2984.7490234375\n","Epoch [23/30] Batch [650/798]:                     loss = 3590.591796875\n","Epoch [23/30] Batch [660/798]:                     loss = 3918.802734375\n","Epoch [23/30] Batch [670/798]:                     loss = 4059.136474609375\n","Epoch [23/30] Batch [680/798]:                     loss = 3846.3623046875\n","Epoch [23/30] Batch [690/798]:                     loss = 3152.839599609375\n","Epoch [23/30] Batch [700/798]:                     loss = 2625.218505859375\n","Epoch [23/30] Batch [710/798]:                     loss = 1951.5819091796875\n","Epoch [23/30] Batch [720/798]:                     loss = 1895.198486328125\n","Epoch [23/30] Batch [730/798]:                     loss = 1666.948974609375\n","Epoch [23/30] Batch [740/798]:                     loss = 1526.5692138671875\n","Epoch [23/30] Batch [750/798]:                     loss = 1549.2222900390625\n","Epoch [23/30] Batch [760/798]:                     loss = 2062.701171875\n","Epoch [23/30] Batch [770/798]:                     loss = 5109.3701171875\n","Epoch [23/30] Batch [780/798]:                     loss = 5468.1337890625\n","Epoch [23/30] Batch [790/798]:                     loss = 6481.9375\n","Epoch [23/30]: training loss= 3903.093824, training mse loss= 3903.093824, training bce loss= nan\n","             : validation loss= 3426.147734, validation mse loss= 3426.147734, validation bce loss= nan\n","Epoch [24/30] Batch [10/798]:                     loss = 2270.97998046875\n","Epoch [24/30] Batch [20/798]:                     loss = 1872.8917236328125\n","Epoch [24/30] Batch [30/798]:                     loss = 1771.8033447265625\n","Epoch [24/30] Batch [40/798]:                     loss = 1733.669189453125\n","Epoch [24/30] Batch [50/798]:                     loss = 2533.9052734375\n","Epoch [24/30] Batch [60/798]:                     loss = 3903.572509765625\n","Epoch [24/30] Batch [70/798]:                     loss = 5935.6083984375\n","Epoch [24/30] Batch [80/798]:                     loss = 6422.240234375\n","Epoch [24/30] Batch [90/798]:                     loss = 6901.6982421875\n","Epoch [24/30] Batch [100/798]:                     loss = 6155.013671875\n","Epoch [24/30] Batch [110/798]:                     loss = 5256.88232421875\n","Epoch [24/30] Batch [120/798]:                     loss = 3826.390869140625\n","Epoch [24/30] Batch [130/798]:                     loss = 3172.45947265625\n","Epoch [24/30] Batch [140/798]:                     loss = 2176.15625\n","Epoch [24/30] Batch [150/798]:                     loss = 2016.037841796875\n","Epoch [24/30] Batch [160/798]:                     loss = 1820.7921142578125\n","Epoch [24/30] Batch [170/798]:                     loss = 1574.231201171875\n","Epoch [24/30] Batch [180/798]:                     loss = 1454.4805908203125\n","Epoch [24/30] Batch [190/798]:                     loss = 1662.6253662109375\n","Epoch [24/30] Batch [200/798]:                     loss = 2322.36328125\n","Epoch [24/30] Batch [210/798]:                     loss = 3398.197021484375\n","Epoch [24/30] Batch [220/798]:                     loss = 4484.77880859375\n","Epoch [24/30] Batch [230/798]:                     loss = 4714.81298828125\n","Epoch [24/30] Batch [240/798]:                     loss = 4695.92578125\n","Epoch [24/30] Batch [250/798]:                     loss = 4170.24755859375\n","Epoch [24/30] Batch [260/798]:                     loss = 3126.970703125\n","Epoch [24/30] Batch [270/798]:                     loss = 2482.985107421875\n","Epoch [24/30] Batch [280/798]:                     loss = 1911.9661865234375\n","Epoch [24/30] Batch [290/798]:                     loss = 1832.1092529296875\n","Epoch [24/30] Batch [300/798]:                     loss = 1624.3519287109375\n","Epoch [24/30] Batch [310/798]:                     loss = 1504.152099609375\n","Epoch [24/30] Batch [320/798]:                     loss = 1697.9639892578125\n","Epoch [24/30] Batch [330/798]:                     loss = 2553.04833984375\n","Epoch [24/30] Batch [340/798]:                     loss = 6153.689453125\n","Epoch [24/30] Batch [350/798]:                     loss = 7883.369140625\n","Epoch [24/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [24/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [24/30] Batch [380/798]:                     loss = 9855.6728515625\n","Epoch [24/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [24/30] Batch [400/798]:                     loss = 7182.93115234375\n","Epoch [24/30] Batch [410/798]:                     loss = 4646.18310546875\n","Epoch [24/30] Batch [420/798]:                     loss = 3234.213623046875\n","Epoch [24/30] Batch [430/798]:                     loss = 2547.0595703125\n","Epoch [24/30] Batch [440/798]:                     loss = 1945.24755859375\n","Epoch [24/30] Batch [450/798]:                     loss = 1749.6734619140625\n","Epoch [24/30] Batch [460/798]:                     loss = 1668.1605224609375\n","Epoch [24/30] Batch [470/798]:                     loss = 1950.8426513671875\n","Epoch [24/30] Batch [480/798]:                     loss = 4768.97216796875\n","Epoch [24/30] Batch [490/798]:                     loss = 7614.70068359375\n","Epoch [24/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [24/30] Batch [510/798]:                     loss = 9246.04296875\n","Epoch [24/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [24/30] Batch [530/798]:                     loss = 8920.6962890625\n","Epoch [24/30] Batch [540/798]:                     loss = 7635.572265625\n","Epoch [24/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [24/30] Batch [560/798]:                     loss = 3455.8603515625\n","Epoch [24/30] Batch [570/798]:                     loss = 2372.714111328125\n","Epoch [24/30] Batch [580/798]:                     loss = 2179.97216796875\n","Epoch [24/30] Batch [590/798]:                     loss = 1830.979736328125\n","Epoch [24/30] Batch [600/798]:                     loss = 1613.1827392578125\n","Epoch [24/30] Batch [610/798]:                     loss = 1461.454833984375\n","Epoch [24/30] Batch [620/798]:                     loss = 1562.3387451171875\n","Epoch [24/30] Batch [630/798]:                     loss = 2097.3173828125\n","Epoch [24/30] Batch [640/798]:                     loss = 2984.748779296875\n","Epoch [24/30] Batch [650/798]:                     loss = 3590.58984375\n","Epoch [24/30] Batch [660/798]:                     loss = 3918.802978515625\n","Epoch [24/30] Batch [670/798]:                     loss = 4059.1357421875\n","Epoch [24/30] Batch [680/798]:                     loss = 3846.36279296875\n","Epoch [24/30] Batch [690/798]:                     loss = 3152.840087890625\n","Epoch [24/30] Batch [700/798]:                     loss = 2625.220703125\n","Epoch [24/30] Batch [710/798]:                     loss = 1951.5814208984375\n","Epoch [24/30] Batch [720/798]:                     loss = 1895.1971435546875\n","Epoch [24/30] Batch [730/798]:                     loss = 1666.9471435546875\n","Epoch [24/30] Batch [740/798]:                     loss = 1526.565185546875\n","Epoch [24/30] Batch [750/798]:                     loss = 1549.2225341796875\n","Epoch [24/30] Batch [760/798]:                     loss = 2062.69921875\n","Epoch [24/30] Batch [770/798]:                     loss = 5109.36962890625\n","Epoch [24/30] Batch [780/798]:                     loss = 5468.13134765625\n","Epoch [24/30] Batch [790/798]:                     loss = 6481.9365234375\n","Epoch [24/30]: training loss= 3903.093162, training mse loss= 3903.093162, training bce loss= nan\n","             : validation loss= 3426.149026, validation mse loss= 3426.149026, validation bce loss= nan\n","Epoch [25/30] Batch [10/798]:                     loss = 2270.97802734375\n","Epoch [25/30] Batch [20/798]:                     loss = 1872.89208984375\n","Epoch [25/30] Batch [30/798]:                     loss = 1771.802978515625\n","Epoch [25/30] Batch [40/798]:                     loss = 1733.66748046875\n","Epoch [25/30] Batch [50/798]:                     loss = 2533.9033203125\n","Epoch [25/30] Batch [60/798]:                     loss = 3903.5712890625\n","Epoch [25/30] Batch [70/798]:                     loss = 5935.60595703125\n","Epoch [25/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [25/30] Batch [90/798]:                     loss = 6901.69873046875\n","Epoch [25/30] Batch [100/798]:                     loss = 6155.01513671875\n","Epoch [25/30] Batch [110/798]:                     loss = 5256.8818359375\n","Epoch [25/30] Batch [120/798]:                     loss = 3826.388916015625\n","Epoch [25/30] Batch [130/798]:                     loss = 3172.45947265625\n","Epoch [25/30] Batch [140/798]:                     loss = 2176.156982421875\n","Epoch [25/30] Batch [150/798]:                     loss = 2016.03369140625\n","Epoch [25/30] Batch [160/798]:                     loss = 1820.75830078125\n","Epoch [25/30] Batch [170/798]:                     loss = 1574.228515625\n","Epoch [25/30] Batch [180/798]:                     loss = 1454.482421875\n","Epoch [25/30] Batch [190/798]:                     loss = 1662.6258544921875\n","Epoch [25/30] Batch [200/798]:                     loss = 2322.364013671875\n","Epoch [25/30] Batch [210/798]:                     loss = 3398.197021484375\n","Epoch [25/30] Batch [220/798]:                     loss = 4484.78173828125\n","Epoch [25/30] Batch [230/798]:                     loss = 4714.81396484375\n","Epoch [25/30] Batch [240/798]:                     loss = 4695.9248046875\n","Epoch [25/30] Batch [250/798]:                     loss = 4170.24658203125\n","Epoch [25/30] Batch [260/798]:                     loss = 3126.970947265625\n","Epoch [25/30] Batch [270/798]:                     loss = 2482.985107421875\n","Epoch [25/30] Batch [280/798]:                     loss = 1911.9666748046875\n","Epoch [25/30] Batch [290/798]:                     loss = 1832.1090087890625\n","Epoch [25/30] Batch [300/798]:                     loss = 1624.3516845703125\n","Epoch [25/30] Batch [310/798]:                     loss = 1504.1549072265625\n","Epoch [25/30] Batch [320/798]:                     loss = 1697.9632568359375\n","Epoch [25/30] Batch [330/798]:                     loss = 2553.04638671875\n","Epoch [25/30] Batch [340/798]:                     loss = 6153.689453125\n","Epoch [25/30] Batch [350/798]:                     loss = 7883.36962890625\n","Epoch [25/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [25/30] Batch [370/798]:                     loss = 10095.2822265625\n","Epoch [25/30] Batch [380/798]:                     loss = 9855.673828125\n","Epoch [25/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [25/30] Batch [400/798]:                     loss = 7182.93115234375\n","Epoch [25/30] Batch [410/798]:                     loss = 4646.18212890625\n","Epoch [25/30] Batch [420/798]:                     loss = 3234.212158203125\n","Epoch [25/30] Batch [430/798]:                     loss = 2547.058837890625\n","Epoch [25/30] Batch [440/798]:                     loss = 1945.2452392578125\n","Epoch [25/30] Batch [450/798]:                     loss = 1749.673828125\n","Epoch [25/30] Batch [460/798]:                     loss = 1668.1583251953125\n","Epoch [25/30] Batch [470/798]:                     loss = 1950.8424072265625\n","Epoch [25/30] Batch [480/798]:                     loss = 4768.97119140625\n","Epoch [25/30] Batch [490/798]:                     loss = 7614.69970703125\n","Epoch [25/30] Batch [500/798]:                     loss = 9470.51953125\n","Epoch [25/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [25/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [25/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [25/30] Batch [540/798]:                     loss = 7635.57568359375\n","Epoch [25/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [25/30] Batch [560/798]:                     loss = 3455.860107421875\n","Epoch [25/30] Batch [570/798]:                     loss = 2372.713134765625\n","Epoch [25/30] Batch [580/798]:                     loss = 2179.972412109375\n","Epoch [25/30] Batch [590/798]:                     loss = 1830.97265625\n","Epoch [25/30] Batch [600/798]:                     loss = 1613.1773681640625\n","Epoch [25/30] Batch [610/798]:                     loss = 1461.45751953125\n","Epoch [25/30] Batch [620/798]:                     loss = 1562.3358154296875\n","Epoch [25/30] Batch [630/798]:                     loss = 2097.318359375\n","Epoch [25/30] Batch [640/798]:                     loss = 2984.7490234375\n","Epoch [25/30] Batch [650/798]:                     loss = 3590.59033203125\n","Epoch [25/30] Batch [660/798]:                     loss = 3918.802734375\n","Epoch [25/30] Batch [670/798]:                     loss = 4059.1357421875\n","Epoch [25/30] Batch [680/798]:                     loss = 3846.362060546875\n","Epoch [25/30] Batch [690/798]:                     loss = 3152.83935546875\n","Epoch [25/30] Batch [700/798]:                     loss = 2625.2197265625\n","Epoch [25/30] Batch [710/798]:                     loss = 1951.5809326171875\n","Epoch [25/30] Batch [720/798]:                     loss = 1895.1942138671875\n","Epoch [25/30] Batch [730/798]:                     loss = 1666.9453125\n","Epoch [25/30] Batch [740/798]:                     loss = 1526.5635986328125\n","Epoch [25/30] Batch [750/798]:                     loss = 1549.22119140625\n","Epoch [25/30] Batch [760/798]:                     loss = 2062.69921875\n","Epoch [25/30] Batch [770/798]:                     loss = 5109.369140625\n","Epoch [25/30] Batch [780/798]:                     loss = 5468.1328125\n","Epoch [25/30] Batch [790/798]:                     loss = 6481.9365234375\n","Epoch [25/30]: training loss= 3903.092686, training mse loss= 3903.092686, training bce loss= nan\n","             : validation loss= 3426.142726, validation mse loss= 3426.142726, validation bce loss= nan\n","Epoch [26/30] Batch [10/798]:                     loss = 2270.9814453125\n","Epoch [26/30] Batch [20/798]:                     loss = 1872.8892822265625\n","Epoch [26/30] Batch [30/798]:                     loss = 1771.797607421875\n","Epoch [26/30] Batch [40/798]:                     loss = 1733.6676025390625\n","Epoch [26/30] Batch [50/798]:                     loss = 2533.90234375\n","Epoch [26/30] Batch [60/798]:                     loss = 3903.571044921875\n","Epoch [26/30] Batch [70/798]:                     loss = 5935.61474609375\n","Epoch [26/30] Batch [80/798]:                     loss = 6422.2392578125\n","Epoch [26/30] Batch [90/798]:                     loss = 6901.6982421875\n","Epoch [26/30] Batch [100/798]:                     loss = 6155.01416015625\n","Epoch [26/30] Batch [110/798]:                     loss = 5256.88232421875\n","Epoch [26/30] Batch [120/798]:                     loss = 3826.389892578125\n","Epoch [26/30] Batch [130/798]:                     loss = 3172.4599609375\n","Epoch [26/30] Batch [140/798]:                     loss = 2176.154052734375\n","Epoch [26/30] Batch [150/798]:                     loss = 2016.03125\n","Epoch [26/30] Batch [160/798]:                     loss = 1820.7593994140625\n","Epoch [26/30] Batch [170/798]:                     loss = 1574.227294921875\n","Epoch [26/30] Batch [180/798]:                     loss = 1454.48291015625\n","Epoch [26/30] Batch [190/798]:                     loss = 1662.6251220703125\n","Epoch [26/30] Batch [200/798]:                     loss = 2322.36328125\n","Epoch [26/30] Batch [210/798]:                     loss = 3398.196533203125\n","Epoch [26/30] Batch [220/798]:                     loss = 4484.78076171875\n","Epoch [26/30] Batch [230/798]:                     loss = 4714.81298828125\n","Epoch [26/30] Batch [240/798]:                     loss = 4695.9248046875\n","Epoch [26/30] Batch [250/798]:                     loss = 4170.24658203125\n","Epoch [26/30] Batch [260/798]:                     loss = 3126.97119140625\n","Epoch [26/30] Batch [270/798]:                     loss = 2482.986328125\n","Epoch [26/30] Batch [280/798]:                     loss = 1911.9669189453125\n","Epoch [26/30] Batch [290/798]:                     loss = 1832.1075439453125\n","Epoch [26/30] Batch [300/798]:                     loss = 1624.350341796875\n","Epoch [26/30] Batch [310/798]:                     loss = 1504.151123046875\n","Epoch [26/30] Batch [320/798]:                     loss = 1697.9632568359375\n","Epoch [26/30] Batch [330/798]:                     loss = 2553.03955078125\n","Epoch [26/30] Batch [340/798]:                     loss = 6153.68994140625\n","Epoch [26/30] Batch [350/798]:                     loss = 7883.37060546875\n","Epoch [26/30] Batch [360/798]:                     loss = 9368.287109375\n","Epoch [26/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [26/30] Batch [380/798]:                     loss = 9855.6748046875\n","Epoch [26/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [26/30] Batch [400/798]:                     loss = 7182.931640625\n","Epoch [26/30] Batch [410/798]:                     loss = 4646.18212890625\n","Epoch [26/30] Batch [420/798]:                     loss = 3234.211181640625\n","Epoch [26/30] Batch [430/798]:                     loss = 2547.056884765625\n","Epoch [26/30] Batch [440/798]:                     loss = 1945.244384765625\n","Epoch [26/30] Batch [450/798]:                     loss = 1749.6717529296875\n","Epoch [26/30] Batch [460/798]:                     loss = 1668.157470703125\n","Epoch [26/30] Batch [470/798]:                     loss = 1950.840576171875\n","Epoch [26/30] Batch [480/798]:                     loss = 4768.970703125\n","Epoch [26/30] Batch [490/798]:                     loss = 7614.7001953125\n","Epoch [26/30] Batch [500/798]:                     loss = 9470.5224609375\n","Epoch [26/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [26/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [26/30] Batch [530/798]:                     loss = 8920.6962890625\n","Epoch [26/30] Batch [540/798]:                     loss = 7635.5712890625\n","Epoch [26/30] Batch [550/798]:                     loss = 5007.4052734375\n","Epoch [26/30] Batch [560/798]:                     loss = 3455.860107421875\n","Epoch [26/30] Batch [570/798]:                     loss = 2372.714111328125\n","Epoch [26/30] Batch [580/798]:                     loss = 2179.970947265625\n","Epoch [26/30] Batch [590/798]:                     loss = 1830.96923828125\n","Epoch [26/30] Batch [600/798]:                     loss = 1613.1773681640625\n","Epoch [26/30] Batch [610/798]:                     loss = 1461.456787109375\n","Epoch [26/30] Batch [620/798]:                     loss = 1562.339599609375\n","Epoch [26/30] Batch [630/798]:                     loss = 2097.31787109375\n","Epoch [26/30] Batch [640/798]:                     loss = 2984.749267578125\n","Epoch [26/30] Batch [650/798]:                     loss = 3590.58935546875\n","Epoch [26/30] Batch [660/798]:                     loss = 3918.802734375\n","Epoch [26/30] Batch [670/798]:                     loss = 4059.134521484375\n","Epoch [26/30] Batch [680/798]:                     loss = 3846.36279296875\n","Epoch [26/30] Batch [690/798]:                     loss = 3152.840576171875\n","Epoch [26/30] Batch [700/798]:                     loss = 2625.22021484375\n","Epoch [26/30] Batch [710/798]:                     loss = 1951.581298828125\n","Epoch [26/30] Batch [720/798]:                     loss = 1895.1866455078125\n","Epoch [26/30] Batch [730/798]:                     loss = 1666.947021484375\n","Epoch [26/30] Batch [740/798]:                     loss = 1526.5526123046875\n","Epoch [26/30] Batch [750/798]:                     loss = 1549.2171630859375\n","Epoch [26/30] Batch [760/798]:                     loss = 2062.700927734375\n","Epoch [26/30] Batch [770/798]:                     loss = 5109.369140625\n","Epoch [26/30] Batch [780/798]:                     loss = 5468.1318359375\n","Epoch [26/30] Batch [790/798]:                     loss = 6481.9365234375\n","Epoch [26/30]: training loss= 3903.092200, training mse loss= 3903.092200, training bce loss= nan\n","             : validation loss= 3426.148026, validation mse loss= 3426.148026, validation bce loss= nan\n","Epoch [27/30] Batch [10/798]:                     loss = 2270.98828125\n","Epoch [27/30] Batch [20/798]:                     loss = 1872.889404296875\n","Epoch [27/30] Batch [30/798]:                     loss = 1771.7938232421875\n","Epoch [27/30] Batch [40/798]:                     loss = 1733.6656494140625\n","Epoch [27/30] Batch [50/798]:                     loss = 2533.9013671875\n","Epoch [27/30] Batch [60/798]:                     loss = 3903.571044921875\n","Epoch [27/30] Batch [70/798]:                     loss = 5935.60595703125\n","Epoch [27/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [27/30] Batch [90/798]:                     loss = 6901.69677734375\n","Epoch [27/30] Batch [100/798]:                     loss = 6155.0146484375\n","Epoch [27/30] Batch [110/798]:                     loss = 5256.88134765625\n","Epoch [27/30] Batch [120/798]:                     loss = 3826.39013671875\n","Epoch [27/30] Batch [130/798]:                     loss = 3172.460693359375\n","Epoch [27/30] Batch [140/798]:                     loss = 2176.153564453125\n","Epoch [27/30] Batch [150/798]:                     loss = 2016.0299072265625\n","Epoch [27/30] Batch [160/798]:                     loss = 1820.7581787109375\n","Epoch [27/30] Batch [170/798]:                     loss = 1574.228759765625\n","Epoch [27/30] Batch [180/798]:                     loss = 1454.4810791015625\n","Epoch [27/30] Batch [190/798]:                     loss = 1662.6246337890625\n","Epoch [27/30] Batch [200/798]:                     loss = 2322.36083984375\n","Epoch [27/30] Batch [210/798]:                     loss = 3398.196533203125\n","Epoch [27/30] Batch [220/798]:                     loss = 4484.78076171875\n","Epoch [27/30] Batch [230/798]:                     loss = 4714.8134765625\n","Epoch [27/30] Batch [240/798]:                     loss = 4695.92529296875\n","Epoch [27/30] Batch [250/798]:                     loss = 4170.24853515625\n","Epoch [27/30] Batch [260/798]:                     loss = 3126.970703125\n","Epoch [27/30] Batch [270/798]:                     loss = 2482.98583984375\n","Epoch [27/30] Batch [280/798]:                     loss = 1911.9659423828125\n","Epoch [27/30] Batch [290/798]:                     loss = 1832.1070556640625\n","Epoch [27/30] Batch [300/798]:                     loss = 1624.3486328125\n","Epoch [27/30] Batch [310/798]:                     loss = 1504.14697265625\n","Epoch [27/30] Batch [320/798]:                     loss = 1697.9615478515625\n","Epoch [27/30] Batch [330/798]:                     loss = 2553.04443359375\n","Epoch [27/30] Batch [340/798]:                     loss = 6153.68994140625\n","Epoch [27/30] Batch [350/798]:                     loss = 7883.3701171875\n","Epoch [27/30] Batch [360/798]:                     loss = 9368.28515625\n","Epoch [27/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [27/30] Batch [380/798]:                     loss = 9855.6728515625\n","Epoch [27/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [27/30] Batch [400/798]:                     loss = 7182.9306640625\n","Epoch [27/30] Batch [410/798]:                     loss = 4646.18212890625\n","Epoch [27/30] Batch [420/798]:                     loss = 3234.210693359375\n","Epoch [27/30] Batch [430/798]:                     loss = 2547.0576171875\n","Epoch [27/30] Batch [440/798]:                     loss = 1945.241943359375\n","Epoch [27/30] Batch [450/798]:                     loss = 1749.669189453125\n","Epoch [27/30] Batch [460/798]:                     loss = 1668.1571044921875\n","Epoch [27/30] Batch [470/798]:                     loss = 1950.8408203125\n","Epoch [27/30] Batch [480/798]:                     loss = 4768.970703125\n","Epoch [27/30] Batch [490/798]:                     loss = 7614.69921875\n","Epoch [27/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [27/30] Batch [510/798]:                     loss = 9246.041015625\n","Epoch [27/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [27/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [27/30] Batch [540/798]:                     loss = 7635.5732421875\n","Epoch [27/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [27/30] Batch [560/798]:                     loss = 3455.860595703125\n","Epoch [27/30] Batch [570/798]:                     loss = 2372.714111328125\n","Epoch [27/30] Batch [580/798]:                     loss = 2179.9697265625\n","Epoch [27/30] Batch [590/798]:                     loss = 1830.97119140625\n","Epoch [27/30] Batch [600/798]:                     loss = 1613.177001953125\n","Epoch [27/30] Batch [610/798]:                     loss = 1461.455078125\n","Epoch [27/30] Batch [620/798]:                     loss = 1562.3360595703125\n","Epoch [27/30] Batch [630/798]:                     loss = 2097.31640625\n","Epoch [27/30] Batch [640/798]:                     loss = 2984.7490234375\n","Epoch [27/30] Batch [650/798]:                     loss = 3590.589599609375\n","Epoch [27/30] Batch [660/798]:                     loss = 3918.802001953125\n","Epoch [27/30] Batch [670/798]:                     loss = 4059.134521484375\n","Epoch [27/30] Batch [680/798]:                     loss = 3846.36181640625\n","Epoch [27/30] Batch [690/798]:                     loss = 3152.840087890625\n","Epoch [27/30] Batch [700/798]:                     loss = 2625.21923828125\n","Epoch [27/30] Batch [710/798]:                     loss = 1951.580810546875\n","Epoch [27/30] Batch [720/798]:                     loss = 1895.1861572265625\n","Epoch [27/30] Batch [730/798]:                     loss = 1666.947265625\n","Epoch [27/30] Batch [740/798]:                     loss = 1526.5460205078125\n","Epoch [27/30] Batch [750/798]:                     loss = 1549.2161865234375\n","Epoch [27/30] Batch [760/798]:                     loss = 2062.698974609375\n","Epoch [27/30] Batch [770/798]:                     loss = 5109.369140625\n","Epoch [27/30] Batch [780/798]:                     loss = 5468.1318359375\n","Epoch [27/30] Batch [790/798]:                     loss = 6481.9375\n","Epoch [27/30]: training loss= 3903.091656, training mse loss= 3903.091656, training bce loss= nan\n","             : validation loss= 3426.146048, validation mse loss= 3426.146048, validation bce loss= nan\n","Epoch [28/30] Batch [10/798]:                     loss = 2270.9833984375\n","Epoch [28/30] Batch [20/798]:                     loss = 1872.885986328125\n","Epoch [28/30] Batch [30/798]:                     loss = 1771.7869873046875\n","Epoch [28/30] Batch [40/798]:                     loss = 1733.66357421875\n","Epoch [28/30] Batch [50/798]:                     loss = 2533.9013671875\n","Epoch [28/30] Batch [60/798]:                     loss = 3903.570556640625\n","Epoch [28/30] Batch [70/798]:                     loss = 5935.60693359375\n","Epoch [28/30] Batch [80/798]:                     loss = 6422.23974609375\n","Epoch [28/30] Batch [90/798]:                     loss = 6901.69775390625\n","Epoch [28/30] Batch [100/798]:                     loss = 6155.01416015625\n","Epoch [28/30] Batch [110/798]:                     loss = 5256.8818359375\n","Epoch [28/30] Batch [120/798]:                     loss = 3826.389892578125\n","Epoch [28/30] Batch [130/798]:                     loss = 3172.458740234375\n","Epoch [28/30] Batch [140/798]:                     loss = 2176.1533203125\n","Epoch [28/30] Batch [150/798]:                     loss = 2016.03125\n","Epoch [28/30] Batch [160/798]:                     loss = 1820.7591552734375\n","Epoch [28/30] Batch [170/798]:                     loss = 1574.2298583984375\n","Epoch [28/30] Batch [180/798]:                     loss = 1454.4779052734375\n","Epoch [28/30] Batch [190/798]:                     loss = 1662.6243896484375\n","Epoch [28/30] Batch [200/798]:                     loss = 2322.3603515625\n","Epoch [28/30] Batch [210/798]:                     loss = 3398.19482421875\n","Epoch [28/30] Batch [220/798]:                     loss = 4484.77978515625\n","Epoch [28/30] Batch [230/798]:                     loss = 4714.814453125\n","Epoch [28/30] Batch [240/798]:                     loss = 4695.9248046875\n","Epoch [28/30] Batch [250/798]:                     loss = 4170.24658203125\n","Epoch [28/30] Batch [260/798]:                     loss = 3126.9716796875\n","Epoch [28/30] Batch [270/798]:                     loss = 2482.9853515625\n","Epoch [28/30] Batch [280/798]:                     loss = 1911.9666748046875\n","Epoch [28/30] Batch [290/798]:                     loss = 1832.1065673828125\n","Epoch [28/30] Batch [300/798]:                     loss = 1624.348876953125\n","Epoch [28/30] Batch [310/798]:                     loss = 1504.152587890625\n","Epoch [28/30] Batch [320/798]:                     loss = 1697.9630126953125\n","Epoch [28/30] Batch [330/798]:                     loss = 2553.03564453125\n","Epoch [28/30] Batch [340/798]:                     loss = 6153.68994140625\n","Epoch [28/30] Batch [350/798]:                     loss = 7883.36865234375\n","Epoch [28/30] Batch [360/798]:                     loss = 9368.28515625\n","Epoch [28/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [28/30] Batch [380/798]:                     loss = 9855.673828125\n","Epoch [28/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [28/30] Batch [400/798]:                     loss = 7182.93212890625\n","Epoch [28/30] Batch [410/798]:                     loss = 4646.181640625\n","Epoch [28/30] Batch [420/798]:                     loss = 3234.21044921875\n","Epoch [28/30] Batch [430/798]:                     loss = 2547.057861328125\n","Epoch [28/30] Batch [440/798]:                     loss = 1945.241943359375\n","Epoch [28/30] Batch [450/798]:                     loss = 1749.66845703125\n","Epoch [28/30] Batch [460/798]:                     loss = 1668.1561279296875\n","Epoch [28/30] Batch [470/798]:                     loss = 1950.840576171875\n","Epoch [28/30] Batch [480/798]:                     loss = 4768.97119140625\n","Epoch [28/30] Batch [490/798]:                     loss = 7614.69970703125\n","Epoch [28/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [28/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [28/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [28/30] Batch [530/798]:                     loss = 8920.6962890625\n","Epoch [28/30] Batch [540/798]:                     loss = 7635.572265625\n","Epoch [28/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [28/30] Batch [560/798]:                     loss = 3455.859619140625\n","Epoch [28/30] Batch [570/798]:                     loss = 2372.713134765625\n","Epoch [28/30] Batch [580/798]:                     loss = 2179.968505859375\n","Epoch [28/30] Batch [590/798]:                     loss = 1830.9703369140625\n","Epoch [28/30] Batch [600/798]:                     loss = 1613.1768798828125\n","Epoch [28/30] Batch [610/798]:                     loss = 1461.4599609375\n","Epoch [28/30] Batch [620/798]:                     loss = 1562.3363037109375\n","Epoch [28/30] Batch [630/798]:                     loss = 2097.317138671875\n","Epoch [28/30] Batch [640/798]:                     loss = 2984.7490234375\n","Epoch [28/30] Batch [650/798]:                     loss = 3590.58984375\n","Epoch [28/30] Batch [660/798]:                     loss = 3918.802978515625\n","Epoch [28/30] Batch [670/798]:                     loss = 4059.136474609375\n","Epoch [28/30] Batch [680/798]:                     loss = 3846.36181640625\n","Epoch [28/30] Batch [690/798]:                     loss = 3152.839111328125\n","Epoch [28/30] Batch [700/798]:                     loss = 2625.2177734375\n","Epoch [28/30] Batch [710/798]:                     loss = 1951.5794677734375\n","Epoch [28/30] Batch [720/798]:                     loss = 1895.1846923828125\n","Epoch [28/30] Batch [730/798]:                     loss = 1666.943359375\n","Epoch [28/30] Batch [740/798]:                     loss = 1526.541015625\n","Epoch [28/30] Batch [750/798]:                     loss = 1549.212646484375\n","Epoch [28/30] Batch [760/798]:                     loss = 2062.69677734375\n","Epoch [28/30] Batch [770/798]:                     loss = 5109.369140625\n","Epoch [28/30] Batch [780/798]:                     loss = 5468.1328125\n","Epoch [28/30] Batch [790/798]:                     loss = 6481.9375\n","Epoch [28/30]: training loss= 3903.091045, training mse loss= 3903.091045, training bce loss= nan\n","             : validation loss= 3426.149129, validation mse loss= 3426.149129, validation bce loss= nan\n","Epoch [29/30] Batch [10/798]:                     loss = 2270.979736328125\n","Epoch [29/30] Batch [20/798]:                     loss = 1872.888671875\n","Epoch [29/30] Batch [30/798]:                     loss = 1771.7867431640625\n","Epoch [29/30] Batch [40/798]:                     loss = 1733.664794921875\n","Epoch [29/30] Batch [50/798]:                     loss = 2533.901611328125\n","Epoch [29/30] Batch [60/798]:                     loss = 3903.571044921875\n","Epoch [29/30] Batch [70/798]:                     loss = 5935.60693359375\n","Epoch [29/30] Batch [80/798]:                     loss = 6422.23876953125\n","Epoch [29/30] Batch [90/798]:                     loss = 6901.69775390625\n","Epoch [29/30] Batch [100/798]:                     loss = 6155.0146484375\n","Epoch [29/30] Batch [110/798]:                     loss = 5256.88134765625\n","Epoch [29/30] Batch [120/798]:                     loss = 3826.388427734375\n","Epoch [29/30] Batch [130/798]:                     loss = 3172.4599609375\n","Epoch [29/30] Batch [140/798]:                     loss = 2176.153564453125\n","Epoch [29/30] Batch [150/798]:                     loss = 2016.032470703125\n","Epoch [29/30] Batch [160/798]:                     loss = 1820.7596435546875\n","Epoch [29/30] Batch [170/798]:                     loss = 1574.227783203125\n","Epoch [29/30] Batch [180/798]:                     loss = 1454.479736328125\n","Epoch [29/30] Batch [190/798]:                     loss = 1662.6240234375\n","Epoch [29/30] Batch [200/798]:                     loss = 2322.360107421875\n","Epoch [29/30] Batch [210/798]:                     loss = 3398.1962890625\n","Epoch [29/30] Batch [220/798]:                     loss = 4484.77978515625\n","Epoch [29/30] Batch [230/798]:                     loss = 4714.8134765625\n","Epoch [29/30] Batch [240/798]:                     loss = 4695.92578125\n","Epoch [29/30] Batch [250/798]:                     loss = 4170.24755859375\n","Epoch [29/30] Batch [260/798]:                     loss = 3126.970947265625\n","Epoch [29/30] Batch [270/798]:                     loss = 2482.98583984375\n","Epoch [29/30] Batch [280/798]:                     loss = 1911.96630859375\n","Epoch [29/30] Batch [290/798]:                     loss = 1832.1080322265625\n","Epoch [29/30] Batch [300/798]:                     loss = 1624.3487548828125\n","Epoch [29/30] Batch [310/798]:                     loss = 1504.146484375\n","Epoch [29/30] Batch [320/798]:                     loss = 1697.9605712890625\n","Epoch [29/30] Batch [330/798]:                     loss = 2553.03369140625\n","Epoch [29/30] Batch [340/798]:                     loss = 6153.68896484375\n","Epoch [29/30] Batch [350/798]:                     loss = 7883.36865234375\n","Epoch [29/30] Batch [360/798]:                     loss = 9368.2841796875\n","Epoch [29/30] Batch [370/798]:                     loss = 10095.2802734375\n","Epoch [29/30] Batch [380/798]:                     loss = 9855.6728515625\n","Epoch [29/30] Batch [390/798]:                     loss = 8709.69921875\n","Epoch [29/30] Batch [400/798]:                     loss = 7182.93115234375\n","Epoch [29/30] Batch [410/798]:                     loss = 4646.181640625\n","Epoch [29/30] Batch [420/798]:                     loss = 3234.208251953125\n","Epoch [29/30] Batch [430/798]:                     loss = 2547.055908203125\n","Epoch [29/30] Batch [440/798]:                     loss = 1945.2420654296875\n","Epoch [29/30] Batch [450/798]:                     loss = 1749.668212890625\n","Epoch [29/30] Batch [460/798]:                     loss = 1668.154052734375\n","Epoch [29/30] Batch [470/798]:                     loss = 1950.841796875\n","Epoch [29/30] Batch [480/798]:                     loss = 4768.97119140625\n","Epoch [29/30] Batch [490/798]:                     loss = 7614.69873046875\n","Epoch [29/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [29/30] Batch [510/798]:                     loss = 9246.041015625\n","Epoch [29/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [29/30] Batch [530/798]:                     loss = 8920.697265625\n","Epoch [29/30] Batch [540/798]:                     loss = 7635.5732421875\n","Epoch [29/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [29/30] Batch [560/798]:                     loss = 3455.859619140625\n","Epoch [29/30] Batch [570/798]:                     loss = 2372.712646484375\n","Epoch [29/30] Batch [580/798]:                     loss = 2179.9658203125\n","Epoch [29/30] Batch [590/798]:                     loss = 1830.9654541015625\n","Epoch [29/30] Batch [600/798]:                     loss = 1613.176513671875\n","Epoch [29/30] Batch [610/798]:                     loss = 1461.457763671875\n","Epoch [29/30] Batch [620/798]:                     loss = 1562.334716796875\n","Epoch [29/30] Batch [630/798]:                     loss = 2097.316650390625\n","Epoch [29/30] Batch [640/798]:                     loss = 2984.74951171875\n","Epoch [29/30] Batch [650/798]:                     loss = 3590.58935546875\n","Epoch [29/30] Batch [660/798]:                     loss = 3918.8017578125\n","Epoch [29/30] Batch [670/798]:                     loss = 4059.134521484375\n","Epoch [29/30] Batch [680/798]:                     loss = 3846.36279296875\n","Epoch [29/30] Batch [690/798]:                     loss = 3152.839111328125\n","Epoch [29/30] Batch [700/798]:                     loss = 2625.21923828125\n","Epoch [29/30] Batch [710/798]:                     loss = 1951.581298828125\n","Epoch [29/30] Batch [720/798]:                     loss = 1895.1856689453125\n","Epoch [29/30] Batch [730/798]:                     loss = 1666.94287109375\n","Epoch [29/30] Batch [740/798]:                     loss = 1526.54443359375\n","Epoch [29/30] Batch [750/798]:                     loss = 1549.208984375\n","Epoch [29/30] Batch [760/798]:                     loss = 2062.69873046875\n","Epoch [29/30] Batch [770/798]:                     loss = 5109.369140625\n","Epoch [29/30] Batch [780/798]:                     loss = 5468.1318359375\n","Epoch [29/30] Batch [790/798]:                     loss = 6481.9365234375\n","Epoch [29/30]: training loss= 3903.090823, training mse loss= 3903.090823, training bce loss= nan\n","             : validation loss= 3426.140720, validation mse loss= 3426.140720, validation bce loss= nan\n","Epoch [30/30] Batch [10/798]:                     loss = 2270.9775390625\n","Epoch [30/30] Batch [20/798]:                     loss = 1872.885986328125\n","Epoch [30/30] Batch [30/798]:                     loss = 1771.7894287109375\n","Epoch [30/30] Batch [40/798]:                     loss = 1733.660888671875\n","Epoch [30/30] Batch [50/798]:                     loss = 2533.900390625\n","Epoch [30/30] Batch [60/798]:                     loss = 3903.570068359375\n","Epoch [30/30] Batch [70/798]:                     loss = 5935.60498046875\n","Epoch [30/30] Batch [80/798]:                     loss = 6422.23876953125\n","Epoch [30/30] Batch [90/798]:                     loss = 6901.69775390625\n","Epoch [30/30] Batch [100/798]:                     loss = 6155.0146484375\n","Epoch [30/30] Batch [110/798]:                     loss = 5256.88134765625\n","Epoch [30/30] Batch [120/798]:                     loss = 3826.389404296875\n","Epoch [30/30] Batch [130/798]:                     loss = 3172.459228515625\n","Epoch [30/30] Batch [140/798]:                     loss = 2176.153076171875\n","Epoch [30/30] Batch [150/798]:                     loss = 2016.03564453125\n","Epoch [30/30] Batch [160/798]:                     loss = 1820.7579345703125\n","Epoch [30/30] Batch [170/798]:                     loss = 1574.2249755859375\n","Epoch [30/30] Batch [180/798]:                     loss = 1454.480224609375\n","Epoch [30/30] Batch [190/798]:                     loss = 1662.624267578125\n","Epoch [30/30] Batch [200/798]:                     loss = 2322.3603515625\n","Epoch [30/30] Batch [210/798]:                     loss = 3398.194580078125\n","Epoch [30/30] Batch [220/798]:                     loss = 4484.77880859375\n","Epoch [30/30] Batch [230/798]:                     loss = 4714.81298828125\n","Epoch [30/30] Batch [240/798]:                     loss = 4695.92431640625\n","Epoch [30/30] Batch [250/798]:                     loss = 4170.24462890625\n","Epoch [30/30] Batch [260/798]:                     loss = 3126.97021484375\n","Epoch [30/30] Batch [270/798]:                     loss = 2482.986328125\n","Epoch [30/30] Batch [280/798]:                     loss = 1911.9664306640625\n","Epoch [30/30] Batch [290/798]:                     loss = 1832.112060546875\n","Epoch [30/30] Batch [300/798]:                     loss = 1624.3475341796875\n","Epoch [30/30] Batch [310/798]:                     loss = 1504.1448974609375\n","Epoch [30/30] Batch [320/798]:                     loss = 1697.9595947265625\n","Epoch [30/30] Batch [330/798]:                     loss = 2553.03564453125\n","Epoch [30/30] Batch [340/798]:                     loss = 6153.689453125\n","Epoch [30/30] Batch [350/798]:                     loss = 7883.3701171875\n","Epoch [30/30] Batch [360/798]:                     loss = 9368.2841796875\n","Epoch [30/30] Batch [370/798]:                     loss = 10095.279296875\n","Epoch [30/30] Batch [380/798]:                     loss = 9855.67578125\n","Epoch [30/30] Batch [390/798]:                     loss = 8709.697265625\n","Epoch [30/30] Batch [400/798]:                     loss = 7182.9306640625\n","Epoch [30/30] Batch [410/798]:                     loss = 4646.18115234375\n","Epoch [30/30] Batch [420/798]:                     loss = 3234.207763671875\n","Epoch [30/30] Batch [430/798]:                     loss = 2547.058837890625\n","Epoch [30/30] Batch [440/798]:                     loss = 1945.2423095703125\n","Epoch [30/30] Batch [450/798]:                     loss = 1749.6663818359375\n","Epoch [30/30] Batch [460/798]:                     loss = 1668.1484375\n","Epoch [30/30] Batch [470/798]:                     loss = 1950.8409423828125\n","Epoch [30/30] Batch [480/798]:                     loss = 4768.97216796875\n","Epoch [30/30] Batch [490/798]:                     loss = 7614.69873046875\n","Epoch [30/30] Batch [500/798]:                     loss = 9470.5205078125\n","Epoch [30/30] Batch [510/798]:                     loss = 9246.0419921875\n","Epoch [30/30] Batch [520/798]:                     loss = 9973.8935546875\n","Epoch [30/30] Batch [530/798]:                     loss = 8920.6962890625\n","Epoch [30/30] Batch [540/798]:                     loss = 7635.5732421875\n","Epoch [30/30] Batch [550/798]:                     loss = 5007.40576171875\n","Epoch [30/30] Batch [560/798]:                     loss = 3455.859619140625\n","Epoch [30/30] Batch [570/798]:                     loss = 2372.712646484375\n","Epoch [30/30] Batch [580/798]:                     loss = 2179.967041015625\n","Epoch [30/30] Batch [590/798]:                     loss = 1830.9669189453125\n","Epoch [30/30] Batch [600/798]:                     loss = 1613.1756591796875\n","Epoch [30/30] Batch [610/798]:                     loss = 1461.45703125\n","Epoch [30/30] Batch [620/798]:                     loss = 1562.3358154296875\n","Epoch [30/30] Batch [630/798]:                     loss = 2097.31640625\n","Epoch [30/30] Batch [640/798]:                     loss = 2984.74853515625\n","Epoch [30/30] Batch [650/798]:                     loss = 3590.589111328125\n","Epoch [30/30] Batch [660/798]:                     loss = 3918.8017578125\n","Epoch [30/30] Batch [670/798]:                     loss = 4059.134765625\n","Epoch [30/30] Batch [680/798]:                     loss = 3846.36181640625\n","Epoch [30/30] Batch [690/798]:                     loss = 3152.83984375\n","Epoch [30/30] Batch [700/798]:                     loss = 2625.21923828125\n","Epoch [30/30] Batch [710/798]:                     loss = 1951.57958984375\n","Epoch [30/30] Batch [720/798]:                     loss = 1895.1839599609375\n","Epoch [30/30] Batch [730/798]:                     loss = 1666.943603515625\n","Epoch [30/30] Batch [740/798]:                     loss = 1526.541748046875\n","Epoch [30/30] Batch [750/798]:                     loss = 1549.2080078125\n","Epoch [30/30] Batch [760/798]:                     loss = 2062.697265625\n","Epoch [30/30] Batch [770/798]:                     loss = 5109.37158203125\n","Epoch [30/30] Batch [780/798]:                     loss = 5468.1318359375\n","Epoch [30/30] Batch [790/798]:                     loss = 6481.9365234375\n","Epoch [30/30]: training loss= 3903.090329, training mse loss= 3903.090329, training bce loss= nan\n","             : validation loss= 3426.142271, validation mse loss= 3426.142271, validation bce loss= nan\n","Training done! Saving logs...\n","Testing the model...\n","{'r2_0': [0.01057646245194483], 'mae_0': [24.69724531734691], 'mse_0': [2818.2801419771636], 'r2_std_0': [9.397001654645246e-05], 'mae_std_0': [16.069086135120425], 'te_mse_std_0': [940174.4879136783], 'r2_1': [0.010563191473693017], 'mae_1': [24.706948060255783], 'mse_1': [2819.663137306455], 'r2_std_1': [9.377946844553074e-05], 'mae_std_1': [16.025543270565272], 'te_mse_std_1': [939112.7985084354], 'r2_2': [0.010540044733944182], 'mae_2': [24.714857817774984], 'mse_2': [2821.424142414628], 'r2_std_2': [9.298932197445393e-05], 'mae_std_2': [15.853370386226938], 'te_mse_std_2': [931775.2885013588], 'mean_fine_tunning_time': [nan], 'std_fine_tunning_time': [nan]}\n","saving the predictions...\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/306 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["./data/skt/multi_mtgnn/test/figures/0_step\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [04:27<00:00,  1.14it/s]\n","  0%|          | 0/306 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["./data/skt/multi_mtgnn/test/figures/1_step\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [04:28<00:00,  1.14it/s]\n","  0%|          | 0/306 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["./data/skt/multi_mtgnn/test/figures/2_step\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [04:30<00:00,  1.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Test done!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#최종 r2,mse,mae 저장\n","for k, v in perf.items(): \n","    print(f'{k}: {v[0]:.4f}')\n","csv_file = os.path.join(args.model_path, 'perf.csv')\n","pd.DataFrame(perf).to_csv(csv_file, index= False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYaArcuhMTnn","executionInfo":{"status":"ok","timestamp":1666749876087,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"9ce80fce-2971-41f2-b53c-fe124779ee25"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["r2_0: 0.0106\n","mae_0: 24.6972\n","mse_0: 2818.2801\n","r2_std_0: 0.0001\n","mae_std_0: 16.0691\n","te_mse_std_0: 940174.4879\n","r2_1: 0.0106\n","mae_1: 24.7069\n","mse_1: 2819.6631\n","r2_std_1: 0.0001\n","mae_std_1: 16.0255\n","te_mse_std_1: 939112.7985\n","r2_2: 0.0105\n","mae_2: 24.7149\n","mse_2: 2821.4241\n","r2_std_2: 0.0001\n","mae_std_2: 15.8534\n","te_mse_std_2: 931775.2885\n","mean_fine_tunning_time: nan\n","std_fine_tunning_time: nan\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8hlowNe94m0O"},"execution_count":null,"outputs":[]}]}