{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"10k_OPriRSBlxfWauGHi6-b5AhK7TVRyC","authorship_tag":"ABX9TyNF1eWAsi10AsXGfEagPCWP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/gdrvie/\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cu6e0PKNF6eS","executionInfo":{"status":"ok","timestamp":1666703293045,"user_tz":-540,"elapsed":14739,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"80b5c8da-588e-4f85-a385-5a4afef329c7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrvie/\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/gdrvie/MyDrive/SKT\")\n","os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"H-NiRdidFZgN","executionInfo":{"status":"ok","timestamp":1666703293046,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"5aac89bc-7d65-4f4c-b69e-05b88ba376b4"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/gdrvie/MyDrive/SKT'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["#오류가 어디서 났는지 자세히 보여주게끔 하는 코드\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"metadata":{"id":"ywhnP6rJF5-m","executionInfo":{"status":"ok","timestamp":1666703293047,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zKup-aWTEt6Z","executionInfo":{"status":"ok","timestamp":1666703303184,"user_tz":-540,"elapsed":5331,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"outputs":[],"source":["#패키지들\n","import os\n","import sys\n","import argparse\n","import json\n","\n","import pandas as pd\n","from time import time\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","from utils.utils import *\n","from utils.torchUtils import *\n","from layers.models import *\n","from layers.graphLearningLayers import *\n","from layers.nriLayers import *\n","from utils.dataloader import *"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ztcc1OoGzCI","executionInfo":{"status":"ok","timestamp":1666703303184,"user_tz":-540,"elapsed":14,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"2dd77537-f67c-4d6d-95c2-f7aead99a253"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Oct 25 13:08:22 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0    43W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["#args\n","args = argparse.Namespace(\n","    # data path\n","    data_type='skt',\n","    data_path='./data/skt',\n","    pred_steps=3,\n","    tr=0.7,\n","    val=0.2,\n","    standardize=True, #action='store_true'\n","    exclude_TA=True,  #action='store_true'\n","    lag=7,\n","    cache_file='./data/cache.pickle',\n","    # training options\n","    batch_size=2,\n","    fine_tunning_every=12, #nri multistep에 처음생김\n","    epoch=30,\n","    epoch_online=30, #nri multistep에 처음생김\n","    lr=0.001,\n","    kl_loss_penalty=0.01,\n","    patience=5,\n","    delta=0.01,\n","    print_log_option=10,\n","    verbose=True,  #action='store_true'\n","    train_ar = True, #action='store_true'이고 아마 이게 True\n","    train_online = False, #action='store_true'이고 아마 이게 False\n","    # reg_loss_penalty=1e-2,\n","    # kl_weight=0.1,\n","    # gradient_max_norm=5,\n","\n","    # model options\n","    model_path='./data/skt/multi_NRI',\n","    num_blocks=3,\n","    k=2,\n","    top_k=4,\n","    embedding_dim=256,\n","    #alpha=3,\n","    beta=0.5,\n","    tau=0.1,\n","    model_name='latest_checkpoint.pth.tar',\n","    n_hid_encoder=256,\n","    msg_hid=256,\n","    msg_out=256,\n","    n_hid_decoder=256,\n","    save_result = True #action='store_true'\n","    #hard=True,\n","    # To test\n","    #test=False,   #학습하고 싶을땐 True로 바꿔\n","    #model_file='latest_checkpoint.pth.tar',\n","    #model_type='proto',\n","    #num_folds=1\n","\n",")"],"metadata":{"id":"2mWnySjVHRl1","executionInfo":{"status":"ok","timestamp":1666703303606,"user_tz":-540,"elapsed":10,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"AlAEguBTeaAJ","executionInfo":{"status":"ok","timestamp":1666703303608,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# make a path to save a model\n","if not os.path.exists(args.model_path):\n","    print(\"Making a path to save the model...\")\n","    os.makedirs(args.model_path, exist_ok=True)\n","else:\n","    print(\"The path already exists, skip making the path...\")\n","\n","print(f'saving the commandline arguments in the path: {args.model_path}...')\n","args_file = os.path.join(args.model_path, 'commandline_args.txt')\n","with open(args_file, 'w') as f:\n","    json.dump(args.__dict__, f, indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b434exRvecRe","executionInfo":{"status":"ok","timestamp":1666703306490,"user_tz":-540,"elapsed":2890,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"fd441f5f-b48e-4086-bd69-8386ea3abd0e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The path already exists, skip making the path...\n","saving the commandline arguments in the path: ./data/skt/multi_NRI...\n"]}]},{"cell_type":"code","source":["def main(args):\n","    # read data\n","    print(\"Loading data...\")\n","    if args.data_type == 'skt':\n","        # load gestures-data\n","        data = load_skt(args) if not args.exclude_TA else load_skt_without_TA(args)\n","    else:\n","        print(\"Unkown data type, data type should be \\\"skt\\\"\")\n","        sys.exit()\n","\n","    # define training, validation, test datasets and their dataloaders respectively\n","    train_data, valid_data, test_data \\\n","        = TimeSeriesDataset(*data['train'], lag=args.lag, pred_steps=args.pred_steps), \\\n","          TimeSeriesDataset(*data['valid'], lag=args.lag, pred_steps=args.pred_steps), \\\n","          TimeSeriesDataset(*data['test'], lag=args.lag, pred_steps=args.pred_steps)\n","    train_loader, valid_loader, test_loader \\\n","        = DataLoader(train_data, batch_size=args.batch_size, shuffle=False), \\\n","          DataLoader(valid_data, batch_size=args.batch_size, shuffle=False), \\\n","          DataLoader(test_data, batch_size=args.fine_tunning_every, shuffle=False)\n","\n","    print(\"Loading data done!\")\n","\n","    model = NRIMulti(\n","        num_heteros=args.num_heteros,\n","        num_time_series=args.num_ts,\n","        time_lags=args.lag,\n","        #device=device,\n","        tau=args.tau,\n","        n_hid_encoder=args.n_hid_encoder,\n","        msg_hid=args.msg_hid,\n","        msg_out=args.msg_out,\n","        n_hid_decoder=args.n_hid_decoder,\n","        pred_steps=args.pred_steps,\n","        device=device\n","    ).to(device)\n","\n","    # setting training args...\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), args.lr)\n","    # optimizer.load_state_dict(ckpt['optimizer'])\n","    early_stopping = EarlyStopping(\n","        patience= args.patience,\n","        verbose= args.verbose,\n","        delta = args.delta,\n","        path= args.model_path,\n","        model_name= args.model_name\n","    )\n","\n","    # train the multi-step heads using the training data\n","    if args.train_ar:\n","        train(args, model, train_loader, valid_loader, optimizer, criterion, early_stopping, device)\n","    else:\n","        print('skip training auto-regressives predictions...')\n","    # test the multi-step HeteroNRI (model) using test data\n","    # fine tune the model every 'args.fine_tunning_every'\n","    # test the fine-tuned model using the next batch of the test dataset.\n","\n","    if args.train_online:\n","        print('start online-learning...')\n","    else:\n","        print('start evaluating...')\n","    # record time elapsed of the fine-tunning\n","    # the time should be less than 5 minutes...\n","  \n","  \n","  \n","    #test  \n","    te_mse = [[] for _ in range(args.pred_steps)]; te_r2 = [[] for _ in range(args.pred_steps)]; \n","    te_mae= [[] for _ in range(args.pred_steps)]\n","    weights = []\n","    time_ellapsed = []\n","\n","    criterion_mask = nn.BCELoss()\n","    # test_loader_iter = iter(test_loader)\n","\n","    predictions = []\n","    labels = []\n","    graphs = []\n","\n","    for batch_idx, x in enumerate(test_loader): \n","\n","        x['input'], x['mask'], x['label'], x['label_mask'] \\\n","        = x['input'].to(device), x['mask'].to(device), x['label'].to(device), x['label_mask'].to(device)\n","        \n","        # test\n","        model.eval() \n","        with torch.no_grad():\n","            out = model(x, args.beta)\n","            preds = out['preds'].detach().cpu().numpy()\n","            label = x['label'].detach().cpu().numpy()\n","\n","            for t in range(args.pred_steps):\n","                te_mse[t].append(mean_squared_error(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","                te_mae[t].append(mean_absolute_error(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","                te_r2[t].append(r2_score(label[...,t,:].flatten(), preds[...,t,:].flatten()))\n","            weights.append(len(out['preds']))\n","\n","            # record labels and predictions \n","            predictions.append(out['preds'].detach().cpu()) # bs, c, t, n\n","            labels.append(x['label'].detach().cpu()) # bs, c, t, n\n","            if out['adj_mat'] is not None: \n","                graphs.append(out['adj_mat'].detach().cpu()) # bs, c, n, n or bs, n, n\n","        \n","        if args.train_online:\n","            model.train()\n","            ts = time()\n","            # feed forward\n","            print(f'[Batch: {batch_idx+1} / {len(test_loader)}] online learning...')\n","            for epoch in range(args.epoch_online):\n","                with torch.set_grad_enabled(True):\n","                    out = model(x, args.beta)\n","                    mse_loss = criterion(out['outs_label'], x['label'])\n","                    if out['outs_mask'] is not None: \n","                        bce_loss = criterion_mask(out['outs_mask'], x['label_mask'])\n","                        loss = mse_loss + bce_loss\n","                    else: \n","                        loss = mse_loss\n","                    if out['kl_loss'] is not None: \n","                        loss += args.kl_loss_penalty * out['kl_loss']\n","                    # if out['regularization_loss'] is not None: \n","                    #     loss += args.reg_loss_penalty * out['regularization_loss']\n","                # backward \n","                model.zero_grad()\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","            tf = time()\n","            time_ellapsed.append(tf-ts)\n","            print(f'[Batch: {batch_idx+1} / {len(test_loader)}] online learning done in {tf-ts:4f} sec')\n","\n","    te_mse = np.array(te_mse)\n","    te_mae = np.array(te_mae)\n","    te_r2 = np.array(te_r2)\n","    time_ellapsed = np.array(time_ellapsed) if args.train_online else float('nan')\n","   \n","    te_mse_mean = np.average(te_mse, weights= weights, axis= 1)\n","    te_r2_mean  = np.average(te_r2, weights= weights, axis= 1)\n","    te_mae_mean  = np.average(te_mae, weights=weights, axis= 1)\n","    time_ellapsed_mean = np.average(time_ellapsed, weights=weights) if args.train_online else float('nan')\n","\n","    te_mse_std = np.average((te_mse-te_mse_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    te_r2_std = np.average((te_r2-te_r2_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    te_mae_std = np.average((te_mae-te_mae_mean[:, np.newaxis])**2, weights= weights, axis= 1)\n","    time_ellapsed_std = np.average((time_ellapsed-time_ellapsed_mean)**2, weights=weights) if args.train_online else float('nan')\n","    \n","    perf = {}\n","    for t in range(args.pred_steps):\n","        perf[f'r2_{t}'] = [te_r2_mean[t]]\n","        perf[f'mae_{t}'] = [te_mae_mean[t]]\n","        perf[f'mse_{t}'] = [te_mse_mean[t]]\n","        perf[f'r2_std_{t}'] = [te_r2_std[t]]\n","        perf[f'mae_std_{t}'] = [te_mae_std[t]]\n","        perf[f'te_mse_std_{t}'] = [te_mse_std[t]]\n","    perf['mean_fine_tunning_time'] = [time_ellapsed_mean]\n","    perf['std_fine_tunning_time'] = [time_ellapsed_std]\n","\n","    print(perf)\n","\n","    if args.save_result: \n","        \n","        print('saving the predictions...')\n","\n","        predictions = torch.concat(predictions, dim=0) # num_obs, num_cells, preds_steps, num_time_series\n","        labels = torch.concat(labels, dim=0) # num_obs, num_cells, preds_steps, num_time_series   \n","\n","        #예측값 저장 및 figure 저장\n","\n","        for t in range(args.pred_steps):\n","            p = torch.permute(predictions[:,:,t, :], (1, 0, 2)) # num_cells, num_obs, num_time_series \n","            p = p.numpy()\n","            if args.cache is not None: \n","                # preds = inv_min_max_scaler(preds, args.cache, args.columns)\n","                p = inv_min_max_scaler_ver2(p, args.cache, args.columns)\n","\n","            l = torch.permute(labels[:,:,t, :], (1, 0, 2)) # num_cells, num_obs, num_time_series\n","            l = l.numpy()\n","            num_cells = l.shape[0]\n","            if args.cache is not None: \n","                # labels = inv_min_max_scaler(labels, args.cache, args.columns)\n","                l = inv_min_max_scaler_ver2(l, args.cache, args.columns)\n","        \n","            # saving figures: predictions vs labels\n","            for i in tqdm(range(num_cells), total= num_cells):\n","                enb_id = args.decoder.get(i)\n","                write_csv(args, f'test/predictions_{t}_step', f'predictions_{enb_id}.csv', p[i, ...], args.columns)\n","                write_csv(args, f'test/labels_{t}_step', f'labels_{enb_id}.csv', l[i, ...], args.columns)   \n","                \n","                fig, axes = plt.subplots(len(args.columns), 1, figsize= (10,3*len(args.columns)))\n","\n","                for j in range(len(args.columns)):\n","                    col_name = args.columns[j]\n","                    fig.axes[j].set_title(f'time-seris plot: {col_name}')\n","                    fig.axes[j].plot(p[i,:,j], label= 'prediction')\n","                    fig.axes[j].plot(l[i,:,j], label= 'label')\n","                    fig.axes[j].legend()\n","                \n","                fig.suptitle(f\"Prediction and True label plot of {enb_id}\", fontsize=20, position= (0.5, 1.0+0.05))\n","                fig.tight_layout()\n","                # make a path to save a figures \n","                fig_path = os.path.join(args.model_path, f'test/figures/{t}_step')\n","                if not os.path.exists(fig_path):\n","                    # print(\"Making a path to save figures...\")\n","                    print(f\"{fig_path}\")\n","                    os.makedirs(fig_path, exist_ok= True)\n","                # else:\n","                #     print(\"The path to save figures already exists, skip making the path...\")\n","                fig_file = os.path.join(fig_path, f'figure_{enb_id}.png')\n","                fig.savefig(fig_file)\n","                plt.close('all')\n","\n","    return perf \n","\n","\n","\n","\n","\n"],"metadata":{"id":"n5oE8kdZefYD","executionInfo":{"status":"ok","timestamp":1666703316487,"user_tz":-540,"elapsed":306,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# #NRI hyper parameters\n","# embedding_dims = [64,128,256,512]\n","# lags = [7,12,24,36,48]\n","# taus = [0.01,0.1,0.5,1.0,5.0 ]"],"metadata":{"id":"CK5-qbKbfdOS","executionInfo":{"status":"ok","timestamp":1666703318655,"user_tz":-540,"elapsed":311,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["#single step의 optimal hyper parameter로 결과 내기\n","print(f'saving the commandline arguments in the path: {args.model_path}...')\n","args_file = os.path.join(args.model_path, 'commandline_args.txt')\n","with open(args_file, 'w') as f:\n","    json.dump(args.__dict__, f, indent=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JIG6LDY8ekhO","executionInfo":{"status":"ok","timestamp":1666703319078,"user_tz":-540,"elapsed":5,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"e6013c18-940a-434b-bb00-085c51f5fb9e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["saving the commandline arguments in the path: ./data/skt/multi_NRI...\n"]}]},{"cell_type":"code","source":["perf = main(args)\n","print(\"Test done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYg5--kofLYs","executionInfo":{"status":"ok","timestamp":1666709165643,"user_tz":-540,"elapsed":5840308,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"662f45dd-9381-4d9d-c0e2-0a1531d42418"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [00:07<00:00, 39.57it/s] \n"]},{"output_type":"stream","name":"stdout","text":["the shape of X       : (306, 2293, 9)\n","Loading data done!\n"]},{"output_type":"stream","name":"stderr","text":["/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:114: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  nn.init.xavier_normal(m.weight.data)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:148: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  nn.init.xavier_normal(m.weight.data)\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n"]},{"output_type":"stream","name":"stderr","text":["/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/30] Batch [10/798]:                     loss = 306.7317810058594\n","Epoch [1/30] Batch [20/798]:                     loss = 216.354736328125\n","Epoch [1/30] Batch [30/798]:                     loss = 197.12939453125\n","Epoch [1/30] Batch [40/798]:                     loss = 163.13299560546875\n","Epoch [1/30] Batch [50/798]:                     loss = 239.72499084472656\n","Epoch [1/30] Batch [60/798]:                     loss = 364.12115478515625\n","Epoch [1/30] Batch [70/798]:                     loss = 774.9683837890625\n","Epoch [1/30] Batch [80/798]:                     loss = 585.0399169921875\n","Epoch [1/30] Batch [90/798]:                     loss = 513.418701171875\n","Epoch [1/30] Batch [100/798]:                     loss = 757.2833862304688\n","Epoch [1/30] Batch [110/798]:                     loss = 566.9803466796875\n","Epoch [1/30] Batch [120/798]:                     loss = 477.3011169433594\n","Epoch [1/30] Batch [130/798]:                     loss = 272.2059631347656\n","Epoch [1/30] Batch [140/798]:                     loss = 311.8906555175781\n","Epoch [1/30] Batch [150/798]:                     loss = 224.72311401367188\n","Epoch [1/30] Batch [160/798]:                     loss = 197.11862182617188\n","Epoch [1/30] Batch [170/798]:                     loss = 166.79714965820312\n","Epoch [1/30] Batch [180/798]:                     loss = 128.2836151123047\n","Epoch [1/30] Batch [190/798]:                     loss = 204.1798553466797\n","Epoch [1/30] Batch [200/798]:                     loss = 254.5916290283203\n","Epoch [1/30] Batch [210/798]:                     loss = 368.5075988769531\n","Epoch [1/30] Batch [220/798]:                     loss = 392.2210693359375\n","Epoch [1/30] Batch [230/798]:                     loss = 883.7744750976562\n","Epoch [1/30] Batch [240/798]:                     loss = 364.8557434082031\n","Epoch [1/30] Batch [250/798]:                     loss = 501.6979064941406\n","Epoch [1/30] Batch [260/798]:                     loss = 375.04302978515625\n","Epoch [1/30] Batch [270/798]:                     loss = 215.60009765625\n","Epoch [1/30] Batch [280/798]:                     loss = 218.58303833007812\n","Epoch [1/30] Batch [290/798]:                     loss = 124.97863006591797\n","Epoch [1/30] Batch [300/798]:                     loss = 147.3564453125\n","Epoch [1/30] Batch [310/798]:                     loss = 153.54075622558594\n","Epoch [1/30] Batch [320/798]:                     loss = 166.37350463867188\n","Epoch [1/30] Batch [330/798]:                     loss = 314.0335998535156\n","Epoch [1/30] Batch [340/798]:                     loss = 784.2467651367188\n","Epoch [1/30] Batch [350/798]:                     loss = 678.1245727539062\n","Epoch [1/30] Batch [360/798]:                     loss = 964.9337158203125\n","Epoch [1/30] Batch [370/798]:                     loss = 1151.840576171875\n","Epoch [1/30] Batch [380/798]:                     loss = 665.72998046875\n","Epoch [1/30] Batch [390/798]:                     loss = 1304.7359619140625\n","Epoch [1/30] Batch [400/798]:                     loss = 850.5603637695312\n","Epoch [1/30] Batch [410/798]:                     loss = 426.74871826171875\n","Epoch [1/30] Batch [420/798]:                     loss = 405.3212890625\n","Epoch [1/30] Batch [430/798]:                     loss = 142.8220977783203\n","Epoch [1/30] Batch [440/798]:                     loss = 188.20758056640625\n","Epoch [1/30] Batch [450/798]:                     loss = 122.94539642333984\n","Epoch [1/30] Batch [460/798]:                     loss = 184.7482147216797\n","Epoch [1/30] Batch [470/798]:                     loss = 144.5237579345703\n","Epoch [1/30] Batch [480/798]:                     loss = 604.3775634765625\n","Epoch [1/30] Batch [490/798]:                     loss = 670.4503173828125\n","Epoch [1/30] Batch [500/798]:                     loss = 985.9970092773438\n","Epoch [1/30] Batch [510/798]:                     loss = 717.0361328125\n","Epoch [1/30] Batch [520/798]:                     loss = 1047.318359375\n","Epoch [1/30] Batch [530/798]:                     loss = 646.877685546875\n","Epoch [1/30] Batch [540/798]:                     loss = 940.9869384765625\n","Epoch [1/30] Batch [550/798]:                     loss = 478.8640441894531\n","Epoch [1/30] Batch [560/798]:                     loss = 405.6482849121094\n","Epoch [1/30] Batch [570/798]:                     loss = 269.1844787597656\n","Epoch [1/30] Batch [580/798]:                     loss = 228.5701904296875\n","Epoch [1/30] Batch [590/798]:                     loss = 160.30711364746094\n","Epoch [1/30] Batch [600/798]:                     loss = 143.771484375\n","Epoch [1/30] Batch [610/798]:                     loss = 180.54640197753906\n","Epoch [1/30] Batch [620/798]:                     loss = 146.84165954589844\n","Epoch [1/30] Batch [630/798]:                     loss = 158.40496826171875\n","Epoch [1/30] Batch [640/798]:                     loss = 296.9700927734375\n","Epoch [1/30] Batch [650/798]:                     loss = 606.1813354492188\n","Epoch [1/30] Batch [660/798]:                     loss = 339.6911315917969\n","Epoch [1/30] Batch [670/798]:                     loss = 383.75006103515625\n","Epoch [1/30] Batch [680/798]:                     loss = 532.647705078125\n","Epoch [1/30] Batch [690/798]:                     loss = 202.16555786132812\n","Epoch [1/30] Batch [700/798]:                     loss = 239.72120666503906\n","Epoch [1/30] Batch [710/798]:                     loss = 256.286865234375\n","Epoch [1/30] Batch [720/798]:                     loss = 172.0298614501953\n","Epoch [1/30] Batch [730/798]:                     loss = 149.19569396972656\n","Epoch [1/30] Batch [740/798]:                     loss = 128.48419189453125\n","Epoch [1/30] Batch [750/798]:                     loss = 145.02679443359375\n","Epoch [1/30] Batch [760/798]:                     loss = 192.4773406982422\n","Epoch [1/30] Batch [770/798]:                     loss = 613.1287231445312\n","Epoch [1/30] Batch [780/798]:                     loss = 854.8471069335938\n","Epoch [1/30] Batch [790/798]:                     loss = 961.7921142578125\n","Epoch [1/30]: training loss= 459.296478, training mse loss= 459.296478, training bce loss= nan\n","            : validation loss= 427.029071, validation mse loss= 427.029071, validation bce loss= nan\n","Validation loss decreased: inf --> 427.0291. Saving model...\n","Epoch [2/30] Batch [10/798]:                     loss = 310.90313720703125\n","Epoch [2/30] Batch [20/798]:                     loss = 214.08645629882812\n","Epoch [2/30] Batch [30/798]:                     loss = 187.15464782714844\n","Epoch [2/30] Batch [40/798]:                     loss = 147.45968627929688\n","Epoch [2/30] Batch [50/798]:                     loss = 220.4852752685547\n","Epoch [2/30] Batch [60/798]:                     loss = 353.9435119628906\n","Epoch [2/30] Batch [70/798]:                     loss = 764.009033203125\n","Epoch [2/30] Batch [80/798]:                     loss = 575.5900268554688\n","Epoch [2/30] Batch [90/798]:                     loss = 504.3784484863281\n","Epoch [2/30] Batch [100/798]:                     loss = 728.9879760742188\n","Epoch [2/30] Batch [110/798]:                     loss = 512.2926025390625\n","Epoch [2/30] Batch [120/798]:                     loss = 443.4480285644531\n","Epoch [2/30] Batch [130/798]:                     loss = 238.17433166503906\n","Epoch [2/30] Batch [140/798]:                     loss = 270.51385498046875\n","Epoch [2/30] Batch [150/798]:                     loss = 198.1951141357422\n","Epoch [2/30] Batch [160/798]:                     loss = 178.54946899414062\n","Epoch [2/30] Batch [170/798]:                     loss = 149.98013305664062\n","Epoch [2/30] Batch [180/798]:                     loss = 101.0932846069336\n","Epoch [2/30] Batch [190/798]:                     loss = 201.68930053710938\n","Epoch [2/30] Batch [200/798]:                     loss = 245.92501831054688\n","Epoch [2/30] Batch [210/798]:                     loss = 358.447021484375\n","Epoch [2/30] Batch [220/798]:                     loss = 387.2394104003906\n","Epoch [2/30] Batch [230/798]:                     loss = 862.38671875\n","Epoch [2/30] Batch [240/798]:                     loss = 348.6553649902344\n","Epoch [2/30] Batch [250/798]:                     loss = 469.3580627441406\n","Epoch [2/30] Batch [260/798]:                     loss = 338.5976867675781\n","Epoch [2/30] Batch [270/798]:                     loss = 203.9178009033203\n","Epoch [2/30] Batch [280/798]:                     loss = 214.26901245117188\n","Epoch [2/30] Batch [290/798]:                     loss = 118.33980560302734\n","Epoch [2/30] Batch [300/798]:                     loss = 139.6521453857422\n","Epoch [2/30] Batch [310/798]:                     loss = 144.73080444335938\n","Epoch [2/30] Batch [320/798]:                     loss = 164.6225128173828\n","Epoch [2/30] Batch [330/798]:                     loss = 322.0047302246094\n","Epoch [2/30] Batch [340/798]:                     loss = 784.2117309570312\n","Epoch [2/30] Batch [350/798]:                     loss = 676.9844970703125\n","Epoch [2/30] Batch [360/798]:                     loss = 961.9802856445312\n","Epoch [2/30] Batch [370/798]:                     loss = 1168.0059814453125\n","Epoch [2/30] Batch [380/798]:                     loss = 695.3135375976562\n","Epoch [2/30] Batch [390/798]:                     loss = 1339.492919921875\n","Epoch [2/30] Batch [400/798]:                     loss = 914.185302734375\n","Epoch [2/30] Batch [410/798]:                     loss = 463.3572082519531\n","Epoch [2/30] Batch [420/798]:                     loss = 456.8816833496094\n","Epoch [2/30] Batch [430/798]:                     loss = 138.7335205078125\n","Epoch [2/30] Batch [440/798]:                     loss = 213.5046844482422\n","Epoch [2/30] Batch [450/798]:                     loss = 132.19483947753906\n","Epoch [2/30] Batch [460/798]:                     loss = 189.45591735839844\n","Epoch [2/30] Batch [470/798]:                     loss = 124.43887329101562\n","Epoch [2/30] Batch [480/798]:                     loss = 600.8901977539062\n","Epoch [2/30] Batch [490/798]:                     loss = 668.7044067382812\n","Epoch [2/30] Batch [500/798]:                     loss = 983.7068481445312\n","Epoch [2/30] Batch [510/798]:                     loss = 715.3589477539062\n","Epoch [2/30] Batch [520/798]:                     loss = 1049.32568359375\n","Epoch [2/30] Batch [530/798]:                     loss = 611.1292724609375\n","Epoch [2/30] Batch [540/798]:                     loss = 881.8992919921875\n","Epoch [2/30] Batch [550/798]:                     loss = 414.54986572265625\n","Epoch [2/30] Batch [560/798]:                     loss = 361.32293701171875\n","Epoch [2/30] Batch [570/798]:                     loss = 233.95921325683594\n","Epoch [2/30] Batch [580/798]:                     loss = 199.14718627929688\n","Epoch [2/30] Batch [590/798]:                     loss = 140.5377655029297\n","Epoch [2/30] Batch [600/798]:                     loss = 127.20482635498047\n","Epoch [2/30] Batch [610/798]:                     loss = 154.93084716796875\n","Epoch [2/30] Batch [620/798]:                     loss = 141.07852172851562\n","Epoch [2/30] Batch [630/798]:                     loss = 159.55035400390625\n","Epoch [2/30] Batch [640/798]:                     loss = 296.7618408203125\n","Epoch [2/30] Batch [650/798]:                     loss = 592.7542724609375\n","Epoch [2/30] Batch [660/798]:                     loss = 323.22015380859375\n","Epoch [2/30] Batch [670/798]:                     loss = 374.65789794921875\n","Epoch [2/30] Batch [680/798]:                     loss = 511.5463562011719\n","Epoch [2/30] Batch [690/798]:                     loss = 179.32171630859375\n","Epoch [2/30] Batch [700/798]:                     loss = 224.5791473388672\n","Epoch [2/30] Batch [710/798]:                     loss = 222.144287109375\n","Epoch [2/30] Batch [720/798]:                     loss = 178.2012939453125\n","Epoch [2/30] Batch [730/798]:                     loss = 139.1556854248047\n","Epoch [2/30] Batch [740/798]:                     loss = 114.55537414550781\n","Epoch [2/30] Batch [750/798]:                     loss = 144.55857849121094\n","Epoch [2/30] Batch [760/798]:                     loss = 203.35862731933594\n","Epoch [2/30] Batch [770/798]:                     loss = 614.6878051757812\n","Epoch [2/30] Batch [780/798]:                     loss = 852.9078979492188\n","Epoch [2/30] Batch [790/798]:                     loss = 951.7677001953125\n","Epoch [2/30]: training loss= 448.481026, training mse loss= 448.481026, training bce loss= nan\n","            : validation loss= 410.516293, validation mse loss= 410.516293, validation bce loss= nan\n","Validation loss decreased: 427.0291 --> 410.5163. Saving model...\n","Epoch [3/30] Batch [10/798]:                     loss = 288.0589904785156\n","Epoch [3/30] Batch [20/798]:                     loss = 199.0050506591797\n","Epoch [3/30] Batch [30/798]:                     loss = 184.47813415527344\n","Epoch [3/30] Batch [40/798]:                     loss = 152.08474731445312\n","Epoch [3/30] Batch [50/798]:                     loss = 233.81076049804688\n","Epoch [3/30] Batch [60/798]:                     loss = 352.58770751953125\n","Epoch [3/30] Batch [70/798]:                     loss = 765.869140625\n","Epoch [3/30] Batch [80/798]:                     loss = 576.744140625\n","Epoch [3/30] Batch [90/798]:                     loss = 508.4476013183594\n","Epoch [3/30] Batch [100/798]:                     loss = 750.238525390625\n","Epoch [3/30] Batch [110/798]:                     loss = 559.2188720703125\n","Epoch [3/30] Batch [120/798]:                     loss = 470.5572509765625\n","Epoch [3/30] Batch [130/798]:                     loss = 260.6175231933594\n","Epoch [3/30] Batch [140/798]:                     loss = 303.3070068359375\n","Epoch [3/30] Batch [150/798]:                     loss = 217.46173095703125\n","Epoch [3/30] Batch [160/798]:                     loss = 184.9624786376953\n","Epoch [3/30] Batch [170/798]:                     loss = 160.87588500976562\n","Epoch [3/30] Batch [180/798]:                     loss = 125.16310119628906\n","Epoch [3/30] Batch [190/798]:                     loss = 201.00143432617188\n","Epoch [3/30] Batch [200/798]:                     loss = 250.4103546142578\n","Epoch [3/30] Batch [210/798]:                     loss = 360.145263671875\n","Epoch [3/30] Batch [220/798]:                     loss = 390.7845458984375\n","Epoch [3/30] Batch [230/798]:                     loss = 878.1007080078125\n","Epoch [3/30] Batch [240/798]:                     loss = 358.66973876953125\n","Epoch [3/30] Batch [250/798]:                     loss = 493.3800048828125\n","Epoch [3/30] Batch [260/798]:                     loss = 372.65252685546875\n","Epoch [3/30] Batch [270/798]:                     loss = 238.6313934326172\n","Epoch [3/30] Batch [280/798]:                     loss = 248.28872680664062\n","Epoch [3/30] Batch [290/798]:                     loss = 138.33114624023438\n","Epoch [3/30] Batch [300/798]:                     loss = 151.95968627929688\n","Epoch [3/30] Batch [310/798]:                     loss = 157.0255126953125\n","Epoch [3/30] Batch [320/798]:                     loss = 165.93795776367188\n","Epoch [3/30] Batch [330/798]:                     loss = 312.2143249511719\n","Epoch [3/30] Batch [340/798]:                     loss = 784.2109985351562\n","Epoch [3/30] Batch [350/798]:                     loss = 677.1560668945312\n","Epoch [3/30] Batch [360/798]:                     loss = 962.447265625\n","Epoch [3/30] Batch [370/798]:                     loss = 1168.3992919921875\n","Epoch [3/30] Batch [380/798]:                     loss = 695.9434814453125\n","Epoch [3/30] Batch [390/798]:                     loss = 1339.6810302734375\n","Epoch [3/30] Batch [400/798]:                     loss = 914.043701171875\n","Epoch [3/30] Batch [410/798]:                     loss = 464.0298767089844\n","Epoch [3/30] Batch [420/798]:                     loss = 457.3974304199219\n","Epoch [3/30] Batch [430/798]:                     loss = 139.72872924804688\n","Epoch [3/30] Batch [440/798]:                     loss = 214.4940643310547\n","Epoch [3/30] Batch [450/798]:                     loss = 133.91842651367188\n","Epoch [3/30] Batch [460/798]:                     loss = 190.29905700683594\n","Epoch [3/30] Batch [470/798]:                     loss = 125.83615112304688\n","Epoch [3/30] Batch [480/798]:                     loss = 601.2415161132812\n","Epoch [3/30] Batch [490/798]:                     loss = 669.5136108398438\n","Epoch [3/30] Batch [500/798]:                     loss = 984.49853515625\n","Epoch [3/30] Batch [510/798]:                     loss = 719.2892456054688\n","Epoch [3/30] Batch [520/798]:                     loss = 1048.6632080078125\n","Epoch [3/30] Batch [530/798]:                     loss = 646.201416015625\n","Epoch [3/30] Batch [540/798]:                     loss = 940.3031005859375\n","Epoch [3/30] Batch [550/798]:                     loss = 478.1636047363281\n","Epoch [3/30] Batch [560/798]:                     loss = 405.593505859375\n","Epoch [3/30] Batch [570/798]:                     loss = 269.2641906738281\n","Epoch [3/30] Batch [580/798]:                     loss = 228.47805786132812\n","Epoch [3/30] Batch [590/798]:                     loss = 160.30352783203125\n","Epoch [3/30] Batch [600/798]:                     loss = 143.60986328125\n","Epoch [3/30] Batch [610/798]:                     loss = 180.56552124023438\n","Epoch [3/30] Batch [620/798]:                     loss = 146.708740234375\n","Epoch [3/30] Batch [630/798]:                     loss = 158.24440002441406\n","Epoch [3/30] Batch [640/798]:                     loss = 296.6097106933594\n","Epoch [3/30] Batch [650/798]:                     loss = 605.7115478515625\n","Epoch [3/30] Batch [660/798]:                     loss = 339.42877197265625\n","Epoch [3/30] Batch [670/798]:                     loss = 384.6656494140625\n","Epoch [3/30] Batch [680/798]:                     loss = 533.2962036132812\n","Epoch [3/30] Batch [690/798]:                     loss = 203.70758056640625\n","Epoch [3/30] Batch [700/798]:                     loss = 240.97879028320312\n","Epoch [3/30] Batch [710/798]:                     loss = 257.1138000488281\n","Epoch [3/30] Batch [720/798]:                     loss = 172.08392333984375\n","Epoch [3/30] Batch [730/798]:                     loss = 150.59922790527344\n","Epoch [3/30] Batch [740/798]:                     loss = 129.85556030273438\n","Epoch [3/30] Batch [750/798]:                     loss = 144.9774169921875\n","Epoch [3/30] Batch [760/798]:                     loss = 193.9617156982422\n","Epoch [3/30] Batch [770/798]:                     loss = 613.5575561523438\n","Epoch [3/30] Batch [780/798]:                     loss = 854.7321166992188\n","Epoch [3/30] Batch [790/798]:                     loss = 959.910888671875\n","Epoch [3/30]: training loss= 460.652952, training mse loss= 460.652952, training bce loss= nan\n","            : validation loss= 425.664844, validation mse loss= 425.664844, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [4/30] Batch [10/798]:                     loss = 312.7113037109375\n","Epoch [4/30] Batch [20/798]:                     loss = 215.35179138183594\n","Epoch [4/30] Batch [30/798]:                     loss = 185.2508544921875\n","Epoch [4/30] Batch [40/798]:                     loss = 144.19163513183594\n","Epoch [4/30] Batch [50/798]:                     loss = 215.4861297607422\n","Epoch [4/30] Batch [60/798]:                     loss = 350.8286437988281\n","Epoch [4/30] Batch [70/798]:                     loss = 763.1084594726562\n","Epoch [4/30] Batch [80/798]:                     loss = 573.069091796875\n","Epoch [4/30] Batch [90/798]:                     loss = 506.3467712402344\n","Epoch [4/30] Batch [100/798]:                     loss = 748.5458984375\n","Epoch [4/30] Batch [110/798]:                     loss = 557.6162109375\n","Epoch [4/30] Batch [120/798]:                     loss = 470.18255615234375\n","Epoch [4/30] Batch [130/798]:                     loss = 258.9759521484375\n","Epoch [4/30] Batch [140/798]:                     loss = 275.4731140136719\n","Epoch [4/30] Batch [150/798]:                     loss = 198.21676635742188\n","Epoch [4/30] Batch [160/798]:                     loss = 179.28042602539062\n","Epoch [4/30] Batch [170/798]:                     loss = 149.13949584960938\n","Epoch [4/30] Batch [180/798]:                     loss = 99.51649475097656\n","Epoch [4/30] Batch [190/798]:                     loss = 202.356201171875\n","Epoch [4/30] Batch [200/798]:                     loss = 252.8086395263672\n","Epoch [4/30] Batch [210/798]:                     loss = 361.0303039550781\n","Epoch [4/30] Batch [220/798]:                     loss = 395.8698425292969\n","Epoch [4/30] Batch [230/798]:                     loss = 882.9064331054688\n","Epoch [4/30] Batch [240/798]:                     loss = 362.0813293457031\n","Epoch [4/30] Batch [250/798]:                     loss = 490.6538391113281\n","Epoch [4/30] Batch [260/798]:                     loss = 361.9564514160156\n","Epoch [4/30] Batch [270/798]:                     loss = 225.46376037597656\n","Epoch [4/30] Batch [280/798]:                     loss = 233.78346252441406\n","Epoch [4/30] Batch [290/798]:                     loss = 131.44644165039062\n","Epoch [4/30] Batch [300/798]:                     loss = 151.5391082763672\n","Epoch [4/30] Batch [310/798]:                     loss = 157.43136596679688\n","Epoch [4/30] Batch [320/798]:                     loss = 168.02423095703125\n","Epoch [4/30] Batch [330/798]:                     loss = 319.9271545410156\n","Epoch [4/30] Batch [340/798]:                     loss = 774.8377685546875\n","Epoch [4/30] Batch [350/798]:                     loss = 670.3856811523438\n","Epoch [4/30] Batch [360/798]:                     loss = 960.9813232421875\n","Epoch [4/30] Batch [370/798]:                     loss = 1167.53662109375\n","Epoch [4/30] Batch [380/798]:                     loss = 687.81591796875\n","Epoch [4/30] Batch [390/798]:                     loss = 1335.497802734375\n","Epoch [4/30] Batch [400/798]:                     loss = 881.4989013671875\n","Epoch [4/30] Batch [410/798]:                     loss = 458.26806640625\n","Epoch [4/30] Batch [420/798]:                     loss = 427.946533203125\n","Epoch [4/30] Batch [430/798]:                     loss = 144.05471801757812\n","Epoch [4/30] Batch [440/798]:                     loss = 210.5657501220703\n","Epoch [4/30] Batch [450/798]:                     loss = 136.97947692871094\n","Epoch [4/30] Batch [460/798]:                     loss = 200.87197875976562\n","Epoch [4/30] Batch [470/798]:                     loss = 141.45294189453125\n","Epoch [4/30] Batch [480/798]:                     loss = 625.2481079101562\n","Epoch [4/30] Batch [490/798]:                     loss = 681.4024658203125\n","Epoch [4/30] Batch [500/798]:                     loss = 981.807373046875\n","Epoch [4/30] Batch [510/798]:                     loss = 711.8756713867188\n","Epoch [4/30] Batch [520/798]:                     loss = 1050.123046875\n","Epoch [4/30] Batch [530/798]:                     loss = 626.7122802734375\n","Epoch [4/30] Batch [540/798]:                     loss = 902.5963134765625\n","Epoch [4/30] Batch [550/798]:                     loss = 440.7486267089844\n","Epoch [4/30] Batch [560/798]:                     loss = 367.5895690917969\n","Epoch [4/30] Batch [570/798]:                     loss = 238.2802276611328\n","Epoch [4/30] Batch [580/798]:                     loss = 205.60693359375\n","Epoch [4/30] Batch [590/798]:                     loss = 142.93939208984375\n","Epoch [4/30] Batch [600/798]:                     loss = 131.5948028564453\n","Epoch [4/30] Batch [610/798]:                     loss = 160.6949920654297\n","Epoch [4/30] Batch [620/798]:                     loss = 146.72195434570312\n","Epoch [4/30] Batch [630/798]:                     loss = 163.96836853027344\n","Epoch [4/30] Batch [640/798]:                     loss = 297.2630920410156\n","Epoch [4/30] Batch [650/798]:                     loss = 607.9389038085938\n","Epoch [4/30] Batch [660/798]:                     loss = 342.0181579589844\n","Epoch [4/30] Batch [670/798]:                     loss = 386.1902770996094\n","Epoch [4/30] Batch [680/798]:                     loss = 535.3917236328125\n","Epoch [4/30] Batch [690/798]:                     loss = 203.1498260498047\n","Epoch [4/30] Batch [700/798]:                     loss = 242.6968994140625\n","Epoch [4/30] Batch [710/798]:                     loss = 257.4891357421875\n","Epoch [4/30] Batch [720/798]:                     loss = 172.99432373046875\n","Epoch [4/30] Batch [730/798]:                     loss = 151.1627197265625\n","Epoch [4/30] Batch [740/798]:                     loss = 130.82838439941406\n","Epoch [4/30] Batch [750/798]:                     loss = 146.3800048828125\n","Epoch [4/30] Batch [760/798]:                     loss = 194.45579528808594\n","Epoch [4/30] Batch [770/798]:                     loss = 613.34033203125\n","Epoch [4/30] Batch [780/798]:                     loss = 858.0598754882812\n","Epoch [4/30] Batch [790/798]:                     loss = 962.6378173828125\n","Epoch [4/30]: training loss= 456.683427, training mse loss= 456.683427, training bce loss= nan\n","            : validation loss= 423.357112, validation mse loss= 423.357112, validation bce loss= nan\n","Early stopping counter 2/5\n","Epoch [5/30] Batch [10/798]:                     loss = 288.3908996582031\n","Epoch [5/30] Batch [20/798]:                     loss = 192.2555389404297\n","Epoch [5/30] Batch [30/798]:                     loss = 180.07772827148438\n","Epoch [5/30] Batch [40/798]:                     loss = 151.79933166503906\n","Epoch [5/30] Batch [50/798]:                     loss = 231.7525177001953\n","Epoch [5/30] Batch [60/798]:                     loss = 349.54095458984375\n","Epoch [5/30] Batch [70/798]:                     loss = 763.4287109375\n","Epoch [5/30] Batch [80/798]:                     loss = 574.100830078125\n","Epoch [5/30] Batch [90/798]:                     loss = 507.7967529296875\n","Epoch [5/30] Batch [100/798]:                     loss = 749.7814331054688\n","Epoch [5/30] Batch [110/798]:                     loss = 559.2686157226562\n","Epoch [5/30] Batch [120/798]:                     loss = 470.81915283203125\n","Epoch [5/30] Batch [130/798]:                     loss = 260.5014953613281\n","Epoch [5/30] Batch [140/798]:                     loss = 303.3285217285156\n","Epoch [5/30] Batch [150/798]:                     loss = 217.85012817382812\n","Epoch [5/30] Batch [160/798]:                     loss = 185.7971954345703\n","Epoch [5/30] Batch [170/798]:                     loss = 160.3633270263672\n","Epoch [5/30] Batch [180/798]:                     loss = 125.2131118774414\n","Epoch [5/30] Batch [190/798]:                     loss = 201.638916015625\n","Epoch [5/30] Batch [200/798]:                     loss = 250.67701721191406\n","Epoch [5/30] Batch [210/798]:                     loss = 361.25732421875\n","Epoch [5/30] Batch [220/798]:                     loss = 390.37872314453125\n","Epoch [5/30] Batch [230/798]:                     loss = 869.8629760742188\n","Epoch [5/30] Batch [240/798]:                     loss = 351.61834716796875\n","Epoch [5/30] Batch [250/798]:                     loss = 472.9505310058594\n","Epoch [5/30] Batch [260/798]:                     loss = 340.3761291503906\n","Epoch [5/30] Batch [270/798]:                     loss = 206.11172485351562\n","Epoch [5/30] Batch [280/798]:                     loss = 216.37184143066406\n","Epoch [5/30] Batch [290/798]:                     loss = 118.85909271240234\n","Epoch [5/30] Batch [300/798]:                     loss = 142.08889770507812\n","Epoch [5/30] Batch [310/798]:                     loss = 146.88243103027344\n","Epoch [5/30] Batch [320/798]:                     loss = 164.1560516357422\n","Epoch [5/30] Batch [330/798]:                     loss = 322.7991943359375\n","Epoch [5/30] Batch [340/798]:                     loss = 797.6915283203125\n","Epoch [5/30] Batch [350/798]:                     loss = 688.871337890625\n","Epoch [5/30] Batch [360/798]:                     loss = 972.4375610351562\n","Epoch [5/30] Batch [370/798]:                     loss = 1177.526611328125\n","Epoch [5/30] Batch [380/798]:                     loss = 699.6978759765625\n","Epoch [5/30] Batch [390/798]:                     loss = 1349.687744140625\n","Epoch [5/30] Batch [400/798]:                     loss = 914.2293701171875\n","Epoch [5/30] Batch [410/798]:                     loss = 488.3036804199219\n","Epoch [5/30] Batch [420/798]:                     loss = 457.6078186035156\n","Epoch [5/30] Batch [430/798]:                     loss = 139.66763305664062\n","Epoch [5/30] Batch [440/798]:                     loss = 214.50634765625\n","Epoch [5/30] Batch [450/798]:                     loss = 133.82427978515625\n","Epoch [5/30] Batch [460/798]:                     loss = 190.3534698486328\n","Epoch [5/30] Batch [470/798]:                     loss = 124.57317352294922\n","Epoch [5/30] Batch [480/798]:                     loss = 601.2048950195312\n","Epoch [5/30] Batch [490/798]:                     loss = 669.4059448242188\n","Epoch [5/30] Batch [500/798]:                     loss = 983.8916015625\n","Epoch [5/30] Batch [510/798]:                     loss = 716.54638671875\n","Epoch [5/30] Batch [520/798]:                     loss = 1046.1998291015625\n","Epoch [5/30] Batch [530/798]:                     loss = 625.3015747070312\n","Epoch [5/30] Batch [540/798]:                     loss = 887.8687744140625\n","Epoch [5/30] Batch [550/798]:                     loss = 415.40576171875\n","Epoch [5/30] Batch [560/798]:                     loss = 363.10308837890625\n","Epoch [5/30] Batch [570/798]:                     loss = 236.84500122070312\n","Epoch [5/30] Batch [580/798]:                     loss = 201.6268768310547\n","Epoch [5/30] Batch [590/798]:                     loss = 142.52780151367188\n","Epoch [5/30] Batch [600/798]:                     loss = 129.8245849609375\n","Epoch [5/30] Batch [610/798]:                     loss = 157.03135681152344\n","Epoch [5/30] Batch [620/798]:                     loss = 145.14801025390625\n","Epoch [5/30] Batch [630/798]:                     loss = 160.8401641845703\n","Epoch [5/30] Batch [640/798]:                     loss = 298.24560546875\n","Epoch [5/30] Batch [650/798]:                     loss = 592.8203125\n","Epoch [5/30] Batch [660/798]:                     loss = 328.7010192871094\n","Epoch [5/30] Batch [670/798]:                     loss = 377.7626647949219\n","Epoch [5/30] Batch [680/798]:                     loss = 513.9230346679688\n","Epoch [5/30] Batch [690/798]:                     loss = 179.5449981689453\n","Epoch [5/30] Batch [700/798]:                     loss = 225.07713317871094\n","Epoch [5/30] Batch [710/798]:                     loss = 222.7307586669922\n","Epoch [5/30] Batch [720/798]:                     loss = 178.43995666503906\n","Epoch [5/30] Batch [730/798]:                     loss = 141.18508911132812\n","Epoch [5/30] Batch [740/798]:                     loss = 117.37171173095703\n","Epoch [5/30] Batch [750/798]:                     loss = 144.85960388183594\n","Epoch [5/30] Batch [760/798]:                     loss = 197.92466735839844\n","Epoch [5/30] Batch [770/798]:                     loss = 615.0342407226562\n","Epoch [5/30] Batch [780/798]:                     loss = 845.3219604492188\n","Epoch [5/30] Batch [790/798]:                     loss = 956.423095703125\n","Epoch [5/30]: training loss= 452.659147, training mse loss= 452.659147, training bce loss= nan\n","            : validation loss= 407.773562, validation mse loss= 407.773562, validation bce loss= nan\n","Validation loss decreased: 410.5163 --> 407.7736. Saving model...\n","Epoch [6/30] Batch [10/798]:                     loss = 284.53131103515625\n","Epoch [6/30] Batch [20/798]:                     loss = 194.51104736328125\n","Epoch [6/30] Batch [30/798]:                     loss = 179.44581604003906\n","Epoch [6/30] Batch [40/798]:                     loss = 150.4649658203125\n","Epoch [6/30] Batch [50/798]:                     loss = 233.77438354492188\n","Epoch [6/30] Batch [60/798]:                     loss = 352.3412780761719\n","Epoch [6/30] Batch [70/798]:                     loss = 761.7568359375\n","Epoch [6/30] Batch [80/798]:                     loss = 573.8826904296875\n","Epoch [6/30] Batch [90/798]:                     loss = 507.0093078613281\n","Epoch [6/30] Batch [100/798]:                     loss = 743.4569091796875\n","Epoch [6/30] Batch [110/798]:                     loss = 533.2528686523438\n","Epoch [6/30] Batch [120/798]:                     loss = 445.82269287109375\n","Epoch [6/30] Batch [130/798]:                     loss = 242.1305694580078\n","Epoch [6/30] Batch [140/798]:                     loss = 271.0314636230469\n","Epoch [6/30] Batch [150/798]:                     loss = 198.91380310058594\n","Epoch [6/30] Batch [160/798]:                     loss = 178.9927978515625\n","Epoch [6/30] Batch [170/798]:                     loss = 150.1965789794922\n","Epoch [6/30] Batch [180/798]:                     loss = 100.18093872070312\n","Epoch [6/30] Batch [190/798]:                     loss = 200.42449951171875\n","Epoch [6/30] Batch [200/798]:                     loss = 246.1310577392578\n","Epoch [6/30] Batch [210/798]:                     loss = 358.5435791015625\n","Epoch [6/30] Batch [220/798]:                     loss = 390.6949768066406\n","Epoch [6/30] Batch [230/798]:                     loss = 876.340576171875\n","Epoch [6/30] Batch [240/798]:                     loss = 353.4642333984375\n","Epoch [6/30] Batch [250/798]:                     loss = 487.6624755859375\n","Epoch [6/30] Batch [260/798]:                     loss = 350.65960693359375\n","Epoch [6/30] Batch [270/798]:                     loss = 205.17703247070312\n","Epoch [6/30] Batch [280/798]:                     loss = 215.435546875\n","Epoch [6/30] Batch [290/798]:                     loss = 118.28511047363281\n","Epoch [6/30] Batch [300/798]:                     loss = 139.7605438232422\n","Epoch [6/30] Batch [310/798]:                     loss = 144.85464477539062\n","Epoch [6/30] Batch [320/798]:                     loss = 163.3246307373047\n","Epoch [6/30] Batch [330/798]:                     loss = 317.02337646484375\n","Epoch [6/30] Batch [340/798]:                     loss = 785.7528076171875\n","Epoch [6/30] Batch [350/798]:                     loss = 671.1778564453125\n","Epoch [6/30] Batch [360/798]:                     loss = 947.5899658203125\n","Epoch [6/30] Batch [370/798]:                     loss = 1152.2508544921875\n","Epoch [6/30] Batch [380/798]:                     loss = 671.7291259765625\n","Epoch [6/30] Batch [390/798]:                     loss = 1301.6845703125\n","Epoch [6/30] Batch [400/798]:                     loss = 844.5382690429688\n","Epoch [6/30] Batch [410/798]:                     loss = 422.7008361816406\n","Epoch [6/30] Batch [420/798]:                     loss = 408.65496826171875\n","Epoch [6/30] Batch [430/798]:                     loss = 145.9650421142578\n","Epoch [6/30] Batch [440/798]:                     loss = 189.05967712402344\n","Epoch [6/30] Batch [450/798]:                     loss = 124.52937316894531\n","Epoch [6/30] Batch [460/798]:                     loss = 187.2388458251953\n","Epoch [6/30] Batch [470/798]:                     loss = 147.0076904296875\n","Epoch [6/30] Batch [480/798]:                     loss = 621.4453125\n","Epoch [6/30] Batch [490/798]:                     loss = 670.8909912109375\n","Epoch [6/30] Batch [500/798]:                     loss = 985.834716796875\n","Epoch [6/30] Batch [510/798]:                     loss = 716.5457763671875\n","Epoch [6/30] Batch [520/798]:                     loss = 1047.5201416015625\n","Epoch [6/30] Batch [530/798]:                     loss = 645.8348999023438\n","Epoch [6/30] Batch [540/798]:                     loss = 939.4052124023438\n","Epoch [6/30] Batch [550/798]:                     loss = 478.2639465332031\n","Epoch [6/30] Batch [560/798]:                     loss = 405.65771484375\n","Epoch [6/30] Batch [570/798]:                     loss = 270.2466735839844\n","Epoch [6/30] Batch [580/798]:                     loss = 229.51123046875\n","Epoch [6/30] Batch [590/798]:                     loss = 160.12960815429688\n","Epoch [6/30] Batch [600/798]:                     loss = 144.0265350341797\n","Epoch [6/30] Batch [610/798]:                     loss = 179.86245727539062\n","Epoch [6/30] Batch [620/798]:                     loss = 146.43215942382812\n","Epoch [6/30] Batch [630/798]:                     loss = 158.18885803222656\n","Epoch [6/30] Batch [640/798]:                     loss = 296.6251525878906\n","Epoch [6/30] Batch [650/798]:                     loss = 605.2730712890625\n","Epoch [6/30] Batch [660/798]:                     loss = 339.0589599609375\n","Epoch [6/30] Batch [670/798]:                     loss = 383.59759521484375\n","Epoch [6/30] Batch [680/798]:                     loss = 532.4608154296875\n","Epoch [6/30] Batch [690/798]:                     loss = 201.96157836914062\n","Epoch [6/30] Batch [700/798]:                     loss = 240.08595275878906\n","Epoch [6/30] Batch [710/798]:                     loss = 256.7307434082031\n","Epoch [6/30] Batch [720/798]:                     loss = 171.40866088867188\n","Epoch [6/30] Batch [730/798]:                     loss = 149.96771240234375\n","Epoch [6/30] Batch [740/798]:                     loss = 129.72036743164062\n","Epoch [6/30] Batch [750/798]:                     loss = 144.5548095703125\n","Epoch [6/30] Batch [760/798]:                     loss = 193.2914581298828\n","Epoch [6/30] Batch [770/798]:                     loss = 611.639892578125\n","Epoch [6/30] Batch [780/798]:                     loss = 853.4078979492188\n","Epoch [6/30] Batch [790/798]:                     loss = 959.1665649414062\n","Epoch [6/30]: training loss= 453.152932, training mse loss= 453.152932, training bce loss= nan\n","            : validation loss= 428.452340, validation mse loss= 428.452340, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [7/30] Batch [10/798]:                     loss = 313.1644287109375\n","Epoch [7/30] Batch [20/798]:                     loss = 217.0051727294922\n","Epoch [7/30] Batch [30/798]:                     loss = 187.4269561767578\n","Epoch [7/30] Batch [40/798]:                     loss = 146.52932739257812\n","Epoch [7/30] Batch [50/798]:                     loss = 217.45249938964844\n","Epoch [7/30] Batch [60/798]:                     loss = 351.3485107421875\n","Epoch [7/30] Batch [70/798]:                     loss = 763.2054443359375\n","Epoch [7/30] Batch [80/798]:                     loss = 571.8575439453125\n","Epoch [7/30] Batch [90/798]:                     loss = 507.51739501953125\n","Epoch [7/30] Batch [100/798]:                     loss = 731.80126953125\n","Epoch [7/30] Batch [110/798]:                     loss = 517.7228393554688\n","Epoch [7/30] Batch [120/798]:                     loss = 442.822998046875\n","Epoch [7/30] Batch [130/798]:                     loss = 239.62351989746094\n","Epoch [7/30] Batch [140/798]:                     loss = 269.8816833496094\n","Epoch [7/30] Batch [150/798]:                     loss = 198.372802734375\n","Epoch [7/30] Batch [160/798]:                     loss = 178.4281768798828\n","Epoch [7/30] Batch [170/798]:                     loss = 150.01736450195312\n","Epoch [7/30] Batch [180/798]:                     loss = 100.63632202148438\n","Epoch [7/30] Batch [190/798]:                     loss = 199.11968994140625\n","Epoch [7/30] Batch [200/798]:                     loss = 250.77822875976562\n","Epoch [7/30] Batch [210/798]:                     loss = 354.4168395996094\n","Epoch [7/30] Batch [220/798]:                     loss = 385.8504638671875\n","Epoch [7/30] Batch [230/798]:                     loss = 868.2969360351562\n","Epoch [7/30] Batch [240/798]:                     loss = 351.0104064941406\n","Epoch [7/30] Batch [250/798]:                     loss = 473.60986328125\n","Epoch [7/30] Batch [260/798]:                     loss = 345.7903747558594\n","Epoch [7/30] Batch [270/798]:                     loss = 206.58132934570312\n","Epoch [7/30] Batch [280/798]:                     loss = 214.773193359375\n","Epoch [7/30] Batch [290/798]:                     loss = 118.47389221191406\n","Epoch [7/30] Batch [300/798]:                     loss = 139.35784912109375\n","Epoch [7/30] Batch [310/798]:                     loss = 144.70489501953125\n","Epoch [7/30] Batch [320/798]:                     loss = 162.9594268798828\n","Epoch [7/30] Batch [330/798]:                     loss = 321.9266052246094\n","Epoch [7/30] Batch [340/798]:                     loss = 786.716064453125\n","Epoch [7/30] Batch [350/798]:                     loss = 675.0006713867188\n","Epoch [7/30] Batch [360/798]:                     loss = 948.1324462890625\n","Epoch [7/30] Batch [370/798]:                     loss = 1150.199462890625\n","Epoch [7/30] Batch [380/798]:                     loss = 666.9248046875\n","Epoch [7/30] Batch [390/798]:                     loss = 1298.9185791015625\n","Epoch [7/30] Batch [400/798]:                     loss = 843.8244018554688\n","Epoch [7/30] Batch [410/798]:                     loss = 421.9745788574219\n","Epoch [7/30] Batch [420/798]:                     loss = 404.0826721191406\n","Epoch [7/30] Batch [430/798]:                     loss = 145.33474731445312\n","Epoch [7/30] Batch [440/798]:                     loss = 188.9944305419922\n","Epoch [7/30] Batch [450/798]:                     loss = 123.5913314819336\n","Epoch [7/30] Batch [460/798]:                     loss = 184.85772705078125\n","Epoch [7/30] Batch [470/798]:                     loss = 142.46780395507812\n","Epoch [7/30] Batch [480/798]:                     loss = 610.4913940429688\n","Epoch [7/30] Batch [490/798]:                     loss = 667.6295166015625\n","Epoch [7/30] Batch [500/798]:                     loss = 986.5034790039062\n","Epoch [7/30] Batch [510/798]:                     loss = 717.0728759765625\n","Epoch [7/30] Batch [520/798]:                     loss = 1046.644775390625\n","Epoch [7/30] Batch [530/798]:                     loss = 645.8756713867188\n","Epoch [7/30] Batch [540/798]:                     loss = 939.4678955078125\n","Epoch [7/30] Batch [550/798]:                     loss = 477.3363342285156\n","Epoch [7/30] Batch [560/798]:                     loss = 404.7691345214844\n","Epoch [7/30] Batch [570/798]:                     loss = 269.28082275390625\n","Epoch [7/30] Batch [580/798]:                     loss = 228.98207092285156\n","Epoch [7/30] Batch [590/798]:                     loss = 158.92117309570312\n","Epoch [7/30] Batch [600/798]:                     loss = 142.67356872558594\n","Epoch [7/30] Batch [610/798]:                     loss = 179.2026824951172\n","Epoch [7/30] Batch [620/798]:                     loss = 146.4906005859375\n","Epoch [7/30] Batch [630/798]:                     loss = 157.16397094726562\n","Epoch [7/30] Batch [640/798]:                     loss = 296.4373474121094\n","Epoch [7/30] Batch [650/798]:                     loss = 605.1235961914062\n","Epoch [7/30] Batch [660/798]:                     loss = 337.0281982421875\n","Epoch [7/30] Batch [670/798]:                     loss = 382.83441162109375\n","Epoch [7/30] Batch [680/798]:                     loss = 527.5682983398438\n","Epoch [7/30] Batch [690/798]:                     loss = 197.9333038330078\n","Epoch [7/30] Batch [700/798]:                     loss = 233.32196044921875\n","Epoch [7/30] Batch [710/798]:                     loss = 238.56097412109375\n","Epoch [7/30] Batch [720/798]:                     loss = 182.4238739013672\n","Epoch [7/30] Batch [730/798]:                     loss = 140.78140258789062\n","Epoch [7/30] Batch [740/798]:                     loss = 117.0721206665039\n","Epoch [7/30] Batch [750/798]:                     loss = 149.11532592773438\n","Epoch [7/30] Batch [760/798]:                     loss = 208.6248321533203\n","Epoch [7/30] Batch [770/798]:                     loss = 640.7384033203125\n","Epoch [7/30] Batch [780/798]:                     loss = 846.9141845703125\n","Epoch [7/30] Batch [790/798]:                     loss = 962.9190063476562\n","Epoch [7/30]: training loss= 450.628065, training mse loss= 450.628065, training bce loss= nan\n","            : validation loss= 410.613466, validation mse loss= 410.613466, validation bce loss= nan\n","Early stopping counter 2/5\n","Epoch [8/30] Batch [10/798]:                     loss = 287.7395935058594\n","Epoch [8/30] Batch [20/798]:                     loss = 196.2845916748047\n","Epoch [8/30] Batch [30/798]:                     loss = 178.24156188964844\n","Epoch [8/30] Batch [40/798]:                     loss = 148.39776611328125\n","Epoch [8/30] Batch [50/798]:                     loss = 232.95242309570312\n","Epoch [8/30] Batch [60/798]:                     loss = 356.1145324707031\n","Epoch [8/30] Batch [70/798]:                     loss = 765.3538818359375\n","Epoch [8/30] Batch [80/798]:                     loss = 571.7518920898438\n","Epoch [8/30] Batch [90/798]:                     loss = 504.0708923339844\n","Epoch [8/30] Batch [100/798]:                     loss = 731.0332641601562\n","Epoch [8/30] Batch [110/798]:                     loss = 517.1605224609375\n","Epoch [8/30] Batch [120/798]:                     loss = 442.9024658203125\n","Epoch [8/30] Batch [130/798]:                     loss = 239.5978240966797\n","Epoch [8/30] Batch [140/798]:                     loss = 271.12908935546875\n","Epoch [8/30] Batch [150/798]:                     loss = 198.64849853515625\n","Epoch [8/30] Batch [160/798]:                     loss = 177.71627807617188\n","Epoch [8/30] Batch [170/798]:                     loss = 149.5021209716797\n","Epoch [8/30] Batch [180/798]:                     loss = 100.82662200927734\n","Epoch [8/30] Batch [190/798]:                     loss = 199.8566131591797\n","Epoch [8/30] Batch [200/798]:                     loss = 253.75868225097656\n","Epoch [8/30] Batch [210/798]:                     loss = 358.7837829589844\n","Epoch [8/30] Batch [220/798]:                     loss = 387.8774108886719\n","Epoch [8/30] Batch [230/798]:                     loss = 871.1655883789062\n","Epoch [8/30] Batch [240/798]:                     loss = 353.3106689453125\n","Epoch [8/30] Batch [250/798]:                     loss = 476.876708984375\n","Epoch [8/30] Batch [260/798]:                     loss = 350.5355529785156\n","Epoch [8/30] Batch [270/798]:                     loss = 211.47047424316406\n","Epoch [8/30] Batch [280/798]:                     loss = 217.39077758789062\n","Epoch [8/30] Batch [290/798]:                     loss = 118.79608154296875\n","Epoch [8/30] Batch [300/798]:                     loss = 139.95120239257812\n","Epoch [8/30] Batch [310/798]:                     loss = 145.6695098876953\n","Epoch [8/30] Batch [320/798]:                     loss = 164.56787109375\n","Epoch [8/30] Batch [330/798]:                     loss = 326.08416748046875\n","Epoch [8/30] Batch [340/798]:                     loss = 794.1094970703125\n","Epoch [8/30] Batch [350/798]:                     loss = 669.2675170898438\n","Epoch [8/30] Batch [360/798]:                     loss = 944.5741577148438\n","Epoch [8/30] Batch [370/798]:                     loss = 1148.1868896484375\n","Epoch [8/30] Batch [380/798]:                     loss = 667.6980590820312\n","Epoch [8/30] Batch [390/798]:                     loss = 1299.063720703125\n","Epoch [8/30] Batch [400/798]:                     loss = 844.277099609375\n","Epoch [8/30] Batch [410/798]:                     loss = 424.26605224609375\n","Epoch [8/30] Batch [420/798]:                     loss = 403.4591064453125\n","Epoch [8/30] Batch [430/798]:                     loss = 144.99131774902344\n","Epoch [8/30] Batch [440/798]:                     loss = 189.18191528320312\n","Epoch [8/30] Batch [450/798]:                     loss = 122.43561553955078\n","Epoch [8/30] Batch [460/798]:                     loss = 184.2606201171875\n","Epoch [8/30] Batch [470/798]:                     loss = 142.05477905273438\n","Epoch [8/30] Batch [480/798]:                     loss = 613.036865234375\n","Epoch [8/30] Batch [490/798]:                     loss = 672.3980102539062\n","Epoch [8/30] Batch [500/798]:                     loss = 983.2421264648438\n","Epoch [8/30] Batch [510/798]:                     loss = 708.2144165039062\n","Epoch [8/30] Batch [520/798]:                     loss = 1052.483642578125\n","Epoch [8/30] Batch [530/798]:                     loss = 613.22119140625\n","Epoch [8/30] Batch [540/798]:                     loss = 881.5663452148438\n","Epoch [8/30] Batch [550/798]:                     loss = 422.76422119140625\n","Epoch [8/30] Batch [560/798]:                     loss = 369.0193176269531\n","Epoch [8/30] Batch [570/798]:                     loss = 238.4591827392578\n","Epoch [8/30] Batch [580/798]:                     loss = 201.17706298828125\n","Epoch [8/30] Batch [590/798]:                     loss = 141.30296325683594\n","Epoch [8/30] Batch [600/798]:                     loss = 127.88662719726562\n","Epoch [8/30] Batch [610/798]:                     loss = 157.0175018310547\n","Epoch [8/30] Batch [620/798]:                     loss = 142.28201293945312\n","Epoch [8/30] Batch [630/798]:                     loss = 160.52865600585938\n","Epoch [8/30] Batch [640/798]:                     loss = 301.65606689453125\n","Epoch [8/30] Batch [650/798]:                     loss = 593.0933227539062\n","Epoch [8/30] Batch [660/798]:                     loss = 327.3841247558594\n","Epoch [8/30] Batch [670/798]:                     loss = 376.1191101074219\n","Epoch [8/30] Batch [680/798]:                     loss = 512.0343627929688\n","Epoch [8/30] Batch [690/798]:                     loss = 184.68255615234375\n","Epoch [8/30] Batch [700/798]:                     loss = 227.05335998535156\n","Epoch [8/30] Batch [710/798]:                     loss = 229.16749572753906\n","Epoch [8/30] Batch [720/798]:                     loss = 179.17068481445312\n","Epoch [8/30] Batch [730/798]:                     loss = 139.4739227294922\n","Epoch [8/30] Batch [740/798]:                     loss = 115.40277862548828\n","Epoch [8/30] Batch [750/798]:                     loss = 144.0294189453125\n","Epoch [8/30] Batch [760/798]:                     loss = 203.46163940429688\n","Epoch [8/30] Batch [770/798]:                     loss = 636.5992431640625\n","Epoch [8/30] Batch [780/798]:                     loss = 847.9119262695312\n","Epoch [8/30] Batch [790/798]:                     loss = 962.7144775390625\n","Epoch [8/30]: training loss= 445.705513, training mse loss= 445.705513, training bce loss= nan\n","            : validation loss= 406.678692, validation mse loss= 406.678692, validation bce loss= nan\n","Validation loss decreased: 407.7736 --> 406.6787. Saving model...\n","Epoch [9/30] Batch [10/798]:                     loss = 285.48175048828125\n","Epoch [9/30] Batch [20/798]:                     loss = 196.14947509765625\n","Epoch [9/30] Batch [30/798]:                     loss = 178.4319305419922\n","Epoch [9/30] Batch [40/798]:                     loss = 147.77349853515625\n","Epoch [9/30] Batch [50/798]:                     loss = 236.9054718017578\n","Epoch [9/30] Batch [60/798]:                     loss = 360.77630615234375\n","Epoch [9/30] Batch [70/798]:                     loss = 774.1524047851562\n","Epoch [9/30] Batch [80/798]:                     loss = 569.6666870117188\n","Epoch [9/30] Batch [90/798]:                     loss = 502.1046142578125\n","Epoch [9/30] Batch [100/798]:                     loss = 731.1024780273438\n","Epoch [9/30] Batch [110/798]:                     loss = 516.04443359375\n","Epoch [9/30] Batch [120/798]:                     loss = 443.1883544921875\n","Epoch [9/30] Batch [130/798]:                     loss = 239.84364318847656\n","Epoch [9/30] Batch [140/798]:                     loss = 270.63702392578125\n","Epoch [9/30] Batch [150/798]:                     loss = 198.265625\n","Epoch [9/30] Batch [160/798]:                     loss = 177.6462860107422\n","Epoch [9/30] Batch [170/798]:                     loss = 149.6241455078125\n","Epoch [9/30] Batch [180/798]:                     loss = 100.32660675048828\n","Epoch [9/30] Batch [190/798]:                     loss = 200.4937286376953\n","Epoch [9/30] Batch [200/798]:                     loss = 248.57296752929688\n","Epoch [9/30] Batch [210/798]:                     loss = 360.3973083496094\n","Epoch [9/30] Batch [220/798]:                     loss = 387.5215759277344\n","Epoch [9/30] Batch [230/798]:                     loss = 862.7346801757812\n","Epoch [9/30] Batch [240/798]:                     loss = 347.85894775390625\n","Epoch [9/30] Batch [250/798]:                     loss = 473.989013671875\n","Epoch [9/30] Batch [260/798]:                     loss = 345.8036193847656\n","Epoch [9/30] Batch [270/798]:                     loss = 208.99130249023438\n","Epoch [9/30] Batch [280/798]:                     loss = 218.98934936523438\n","Epoch [9/30] Batch [290/798]:                     loss = 119.95551300048828\n","Epoch [9/30] Batch [300/798]:                     loss = 140.17579650878906\n","Epoch [9/30] Batch [310/798]:                     loss = 145.02723693847656\n","Epoch [9/30] Batch [320/798]:                     loss = 165.2432403564453\n","Epoch [9/30] Batch [330/798]:                     loss = 330.02947998046875\n","Epoch [9/30] Batch [340/798]:                     loss = 791.0597534179688\n","Epoch [9/30] Batch [350/798]:                     loss = 669.3508911132812\n","Epoch [9/30] Batch [360/798]:                     loss = 946.2255859375\n","Epoch [9/30] Batch [370/798]:                     loss = 1146.7579345703125\n","Epoch [9/30] Batch [380/798]:                     loss = 664.3757934570312\n","Epoch [9/30] Batch [390/798]:                     loss = 1299.2950439453125\n","Epoch [9/30] Batch [400/798]:                     loss = 844.5781860351562\n","Epoch [9/30] Batch [410/798]:                     loss = 422.7523498535156\n","Epoch [9/30] Batch [420/798]:                     loss = 409.73345947265625\n","Epoch [9/30] Batch [430/798]:                     loss = 145.19190979003906\n","Epoch [9/30] Batch [440/798]:                     loss = 188.8841552734375\n","Epoch [9/30] Batch [450/798]:                     loss = 123.2833251953125\n","Epoch [9/30] Batch [460/798]:                     loss = 185.2293701171875\n","Epoch [9/30] Batch [470/798]:                     loss = 143.1300048828125\n","Epoch [9/30] Batch [480/798]:                     loss = 634.514404296875\n","Epoch [9/30] Batch [490/798]:                     loss = 677.7561645507812\n","Epoch [9/30] Batch [500/798]:                     loss = 984.0131225585938\n","Epoch [9/30] Batch [510/798]:                     loss = 709.4126586914062\n","Epoch [9/30] Batch [520/798]:                     loss = 1053.38525390625\n","Epoch [9/30] Batch [530/798]:                     loss = 612.7098999023438\n","Epoch [9/30] Batch [540/798]:                     loss = 880.93212890625\n","Epoch [9/30] Batch [550/798]:                     loss = 422.7832336425781\n","Epoch [9/30] Batch [560/798]:                     loss = 369.6260070800781\n","Epoch [9/30] Batch [570/798]:                     loss = 239.2788543701172\n","Epoch [9/30] Batch [580/798]:                     loss = 202.6144256591797\n","Epoch [9/30] Batch [590/798]:                     loss = 140.6790008544922\n","Epoch [9/30] Batch [600/798]:                     loss = 127.7608871459961\n","Epoch [9/30] Batch [610/798]:                     loss = 156.49981689453125\n","Epoch [9/30] Batch [620/798]:                     loss = 142.2733612060547\n","Epoch [9/30] Batch [630/798]:                     loss = 160.2144317626953\n","Epoch [9/30] Batch [640/798]:                     loss = 302.12420654296875\n","Epoch [9/30] Batch [650/798]:                     loss = 593.298828125\n","Epoch [9/30] Batch [660/798]:                     loss = 327.60003662109375\n","Epoch [9/30] Batch [670/798]:                     loss = 376.20806884765625\n","Epoch [9/30] Batch [680/798]:                     loss = 511.73870849609375\n","Epoch [9/30] Batch [690/798]:                     loss = 185.0880584716797\n","Epoch [9/30] Batch [700/798]:                     loss = 226.96693420410156\n","Epoch [9/30] Batch [710/798]:                     loss = 228.02098083496094\n","Epoch [9/30] Batch [720/798]:                     loss = 179.83079528808594\n","Epoch [9/30] Batch [730/798]:                     loss = 139.1611328125\n","Epoch [9/30] Batch [740/798]:                     loss = 115.2125473022461\n","Epoch [9/30] Batch [750/798]:                     loss = 143.46351623535156\n","Epoch [9/30] Batch [760/798]:                     loss = 204.49069213867188\n","Epoch [9/30] Batch [770/798]:                     loss = 640.4957885742188\n","Epoch [9/30] Batch [780/798]:                     loss = 846.3340454101562\n","Epoch [9/30] Batch [790/798]:                     loss = 957.3977661132812\n","Epoch [9/30]: training loss= 445.721276, training mse loss= 445.721276, training bce loss= nan\n","            : validation loss= 407.104902, validation mse loss= 407.104902, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [10/30] Batch [10/798]:                     loss = 279.5204772949219\n","Epoch [10/30] Batch [20/798]:                     loss = 190.61898803710938\n","Epoch [10/30] Batch [30/798]:                     loss = 178.0870819091797\n","Epoch [10/30] Batch [40/798]:                     loss = 149.792236328125\n","Epoch [10/30] Batch [50/798]:                     loss = 238.78602600097656\n","Epoch [10/30] Batch [60/798]:                     loss = 362.7475280761719\n","Epoch [10/30] Batch [70/798]:                     loss = 780.7498168945312\n","Epoch [10/30] Batch [80/798]:                     loss = 565.18408203125\n","Epoch [10/30] Batch [90/798]:                     loss = 501.6719665527344\n","Epoch [10/30] Batch [100/798]:                     loss = 728.5191650390625\n","Epoch [10/30] Batch [110/798]:                     loss = 516.0704345703125\n","Epoch [10/30] Batch [120/798]:                     loss = 444.78125\n","Epoch [10/30] Batch [130/798]:                     loss = 242.15516662597656\n","Epoch [10/30] Batch [140/798]:                     loss = 282.62359619140625\n","Epoch [10/30] Batch [150/798]:                     loss = 200.52870178222656\n","Epoch [10/30] Batch [160/798]:                     loss = 179.52581787109375\n","Epoch [10/30] Batch [170/798]:                     loss = 150.18882751464844\n","Epoch [10/30] Batch [180/798]:                     loss = 102.01799774169922\n","Epoch [10/30] Batch [190/798]:                     loss = 201.905029296875\n","Epoch [10/30] Batch [200/798]:                     loss = 257.7127380371094\n","Epoch [10/30] Batch [210/798]:                     loss = 355.2884826660156\n","Epoch [10/30] Batch [220/798]:                     loss = 389.64404296875\n","Epoch [10/30] Batch [230/798]:                     loss = 858.239013671875\n","Epoch [10/30] Batch [240/798]:                     loss = 348.5470886230469\n","Epoch [10/30] Batch [250/798]:                     loss = 471.21075439453125\n","Epoch [10/30] Batch [260/798]:                     loss = 345.0061950683594\n","Epoch [10/30] Batch [270/798]:                     loss = 208.44119262695312\n","Epoch [10/30] Batch [280/798]:                     loss = 218.82066345214844\n","Epoch [10/30] Batch [290/798]:                     loss = 120.6891098022461\n","Epoch [10/30] Batch [300/798]:                     loss = 140.35067749023438\n","Epoch [10/30] Batch [310/798]:                     loss = 145.0910186767578\n","Epoch [10/30] Batch [320/798]:                     loss = 165.62049865722656\n","Epoch [10/30] Batch [330/798]:                     loss = 338.30279541015625\n","Epoch [10/30] Batch [340/798]:                     loss = 814.3905029296875\n","Epoch [10/30] Batch [350/798]:                     loss = 662.4205932617188\n","Epoch [10/30] Batch [360/798]:                     loss = 946.33349609375\n","Epoch [10/30] Batch [370/798]:                     loss = 1149.96484375\n","Epoch [10/30] Batch [380/798]:                     loss = 666.7675170898438\n","Epoch [10/30] Batch [390/798]:                     loss = 1299.15283203125\n","Epoch [10/30] Batch [400/798]:                     loss = 842.7608642578125\n","Epoch [10/30] Batch [410/798]:                     loss = 424.9700012207031\n","Epoch [10/30] Batch [420/798]:                     loss = 409.42376708984375\n","Epoch [10/30] Batch [430/798]:                     loss = 145.9774932861328\n","Epoch [10/30] Batch [440/798]:                     loss = 192.17813110351562\n","Epoch [10/30] Batch [450/798]:                     loss = 122.93495178222656\n","Epoch [10/30] Batch [460/798]:                     loss = 183.94915771484375\n","Epoch [10/30] Batch [470/798]:                     loss = 142.66546630859375\n","Epoch [10/30] Batch [480/798]:                     loss = 659.3487548828125\n","Epoch [10/30] Batch [490/798]:                     loss = 672.4540405273438\n","Epoch [10/30] Batch [500/798]:                     loss = 982.8657836914062\n","Epoch [10/30] Batch [510/798]:                     loss = 706.49951171875\n","Epoch [10/30] Batch [520/798]:                     loss = 1054.8544921875\n","Epoch [10/30] Batch [530/798]:                     loss = 612.4572143554688\n","Epoch [10/30] Batch [540/798]:                     loss = 881.4702758789062\n","Epoch [10/30] Batch [550/798]:                     loss = 421.4854431152344\n","Epoch [10/30] Batch [560/798]:                     loss = 369.25152587890625\n","Epoch [10/30] Batch [570/798]:                     loss = 241.0071563720703\n","Epoch [10/30] Batch [580/798]:                     loss = 204.82247924804688\n","Epoch [10/30] Batch [590/798]:                     loss = 144.27098083496094\n","Epoch [10/30] Batch [600/798]:                     loss = 131.49159240722656\n","Epoch [10/30] Batch [610/798]:                     loss = 161.1693878173828\n","Epoch [10/30] Batch [620/798]:                     loss = 141.54933166503906\n","Epoch [10/30] Batch [630/798]:                     loss = 164.63075256347656\n","Epoch [10/30] Batch [640/798]:                     loss = 308.287109375\n","Epoch [10/30] Batch [650/798]:                     loss = 596.2112426757812\n","Epoch [10/30] Batch [660/798]:                     loss = 324.8856201171875\n","Epoch [10/30] Batch [670/798]:                     loss = 376.63641357421875\n","Epoch [10/30] Batch [680/798]:                     loss = 513.1876831054688\n","Epoch [10/30] Batch [690/798]:                     loss = 181.7155303955078\n","Epoch [10/30] Batch [700/798]:                     loss = 225.7752227783203\n","Epoch [10/30] Batch [710/798]:                     loss = 224.5275421142578\n","Epoch [10/30] Batch [720/798]:                     loss = 178.3740234375\n","Epoch [10/30] Batch [730/798]:                     loss = 138.9489288330078\n","Epoch [10/30] Batch [740/798]:                     loss = 115.55396270751953\n","Epoch [10/30] Batch [750/798]:                     loss = 143.72227478027344\n","Epoch [10/30] Batch [760/798]:                     loss = 205.90338134765625\n","Epoch [10/30] Batch [770/798]:                     loss = 638.3413696289062\n","Epoch [10/30] Batch [780/798]:                     loss = 847.150390625\n","Epoch [10/30] Batch [790/798]:                     loss = 957.98828125\n","Epoch [10/30]: training loss= 447.077100, training mse loss= 447.077100, training bce loss= nan\n","             : validation loss= 407.997320, validation mse loss= 407.997320, validation bce loss= nan\n","Early stopping counter 2/5\n","Epoch [11/30] Batch [10/798]:                     loss = 285.79412841796875\n","Epoch [11/30] Batch [20/798]:                     loss = 197.44659423828125\n","Epoch [11/30] Batch [30/798]:                     loss = 181.78050231933594\n","Epoch [11/30] Batch [40/798]:                     loss = 150.32261657714844\n","Epoch [11/30] Batch [50/798]:                     loss = 241.1166229248047\n","Epoch [11/30] Batch [60/798]:                     loss = 368.0623474121094\n","Epoch [11/30] Batch [70/798]:                     loss = 785.4121704101562\n","Epoch [11/30] Batch [80/798]:                     loss = 566.0325927734375\n","Epoch [11/30] Batch [90/798]:                     loss = 503.87664794921875\n","Epoch [11/30] Batch [100/798]:                     loss = 728.2930297851562\n","Epoch [11/30] Batch [110/798]:                     loss = 513.8839111328125\n","Epoch [11/30] Batch [120/798]:                     loss = 442.56524658203125\n","Epoch [11/30] Batch [130/798]:                     loss = 240.86117553710938\n","Epoch [11/30] Batch [140/798]:                     loss = 278.9391784667969\n","Epoch [11/30] Batch [150/798]:                     loss = 200.15956115722656\n","Epoch [11/30] Batch [160/798]:                     loss = 179.18084716796875\n","Epoch [11/30] Batch [170/798]:                     loss = 150.79969787597656\n","Epoch [11/30] Batch [180/798]:                     loss = 104.68946075439453\n","Epoch [11/30] Batch [190/798]:                     loss = 200.62222290039062\n","Epoch [11/30] Batch [200/798]:                     loss = 257.8010559082031\n","Epoch [11/30] Batch [210/798]:                     loss = 355.45257568359375\n","Epoch [11/30] Batch [220/798]:                     loss = 391.1295166015625\n","Epoch [11/30] Batch [230/798]:                     loss = 857.3754272460938\n","Epoch [11/30] Batch [240/798]:                     loss = 347.76104736328125\n","Epoch [11/30] Batch [250/798]:                     loss = 470.4669494628906\n","Epoch [11/30] Batch [260/798]:                     loss = 339.8600158691406\n","Epoch [11/30] Batch [270/798]:                     loss = 211.66891479492188\n","Epoch [11/30] Batch [280/798]:                     loss = 224.6512908935547\n","Epoch [11/30] Batch [290/798]:                     loss = 123.74962615966797\n","Epoch [11/30] Batch [300/798]:                     loss = 141.46173095703125\n","Epoch [11/30] Batch [310/798]:                     loss = 145.77037048339844\n","Epoch [11/30] Batch [320/798]:                     loss = 166.7104034423828\n","Epoch [11/30] Batch [330/798]:                     loss = 334.8951416015625\n","Epoch [11/30] Batch [340/798]:                     loss = 802.6634521484375\n","Epoch [11/30] Batch [350/798]:                     loss = 670.4237670898438\n","Epoch [11/30] Batch [360/798]:                     loss = 950.9964599609375\n","Epoch [11/30] Batch [370/798]:                     loss = 1150.7059326171875\n","Epoch [11/30] Batch [380/798]:                     loss = 667.7146606445312\n","Epoch [11/30] Batch [390/798]:                     loss = 1303.6988525390625\n","Epoch [11/30] Batch [400/798]:                     loss = 850.8333129882812\n","Epoch [11/30] Batch [410/798]:                     loss = 425.69512939453125\n","Epoch [11/30] Batch [420/798]:                     loss = 414.4332275390625\n","Epoch [11/30] Batch [430/798]:                     loss = 145.4158935546875\n","Epoch [11/30] Batch [440/798]:                     loss = 193.02561950683594\n","Epoch [11/30] Batch [450/798]:                     loss = 124.2629623413086\n","Epoch [11/30] Batch [460/798]:                     loss = 185.44500732421875\n","Epoch [11/30] Batch [470/798]:                     loss = 144.4267578125\n","Epoch [11/30] Batch [480/798]:                     loss = 676.3074340820312\n","Epoch [11/30] Batch [490/798]:                     loss = 692.0321044921875\n","Epoch [11/30] Batch [500/798]:                     loss = 995.1503295898438\n","Epoch [11/30] Batch [510/798]:                     loss = 709.0618896484375\n","Epoch [11/30] Batch [520/798]:                     loss = 1056.2646484375\n","Epoch [11/30] Batch [530/798]:                     loss = 614.2307739257812\n","Epoch [11/30] Batch [540/798]:                     loss = 885.8740234375\n","Epoch [11/30] Batch [550/798]:                     loss = 425.0039367675781\n","Epoch [11/30] Batch [560/798]:                     loss = 372.08074951171875\n","Epoch [11/30] Batch [570/798]:                     loss = 243.3706817626953\n","Epoch [11/30] Batch [580/798]:                     loss = 207.74876403808594\n","Epoch [11/30] Batch [590/798]:                     loss = 146.6403350830078\n","Epoch [11/30] Batch [600/798]:                     loss = 133.11985778808594\n","Epoch [11/30] Batch [610/798]:                     loss = 163.34881591796875\n","Epoch [11/30] Batch [620/798]:                     loss = 142.37396240234375\n","Epoch [11/30] Batch [630/798]:                     loss = 164.57044982910156\n","Epoch [11/30] Batch [640/798]:                     loss = 310.033935546875\n","Epoch [11/30] Batch [650/798]:                     loss = 597.9690551757812\n","Epoch [11/30] Batch [660/798]:                     loss = 323.0633850097656\n","Epoch [11/30] Batch [670/798]:                     loss = 378.2099609375\n","Epoch [11/30] Batch [680/798]:                     loss = 515.269775390625\n","Epoch [11/30] Batch [690/798]:                     loss = 182.4536590576172\n","Epoch [11/30] Batch [700/798]:                     loss = 226.612060546875\n","Epoch [11/30] Batch [710/798]:                     loss = 227.7521209716797\n","Epoch [11/30] Batch [720/798]:                     loss = 178.4241180419922\n","Epoch [11/30] Batch [730/798]:                     loss = 140.4597625732422\n","Epoch [11/30] Batch [740/798]:                     loss = 117.77851104736328\n","Epoch [11/30] Batch [750/798]:                     loss = 142.74771118164062\n","Epoch [11/30] Batch [760/798]:                     loss = 210.6491241455078\n","Epoch [11/30] Batch [770/798]:                     loss = 670.5403442382812\n","Epoch [11/30] Batch [780/798]:                     loss = 846.2671508789062\n","Epoch [11/30] Batch [790/798]:                     loss = 960.0680541992188\n","Epoch [11/30]: training loss= 448.475780, training mse loss= 448.475780, training bce loss= nan\n","             : validation loss= 404.365924, validation mse loss= 404.365924, validation bce loss= nan\n","Validation loss decreased: 406.6787 --> 404.3659. Saving model...\n","Epoch [12/30] Batch [10/798]:                     loss = 286.2025146484375\n","Epoch [12/30] Batch [20/798]:                     loss = 196.79010009765625\n","Epoch [12/30] Batch [30/798]:                     loss = 179.25726318359375\n","Epoch [12/30] Batch [40/798]:                     loss = 147.6321563720703\n","Epoch [12/30] Batch [50/798]:                     loss = 237.41529846191406\n","Epoch [12/30] Batch [60/798]:                     loss = 365.89453125\n","Epoch [12/30] Batch [70/798]:                     loss = 782.0010375976562\n","Epoch [12/30] Batch [80/798]:                     loss = 567.5142211914062\n","Epoch [12/30] Batch [90/798]:                     loss = 503.9472961425781\n","Epoch [12/30] Batch [100/798]:                     loss = 729.5156860351562\n","Epoch [12/30] Batch [110/798]:                     loss = 516.3930053710938\n","Epoch [12/30] Batch [120/798]:                     loss = 443.42034912109375\n","Epoch [12/30] Batch [130/798]:                     loss = 243.60447692871094\n","Epoch [12/30] Batch [140/798]:                     loss = 277.03839111328125\n","Epoch [12/30] Batch [150/798]:                     loss = 201.4960479736328\n","Epoch [12/30] Batch [160/798]:                     loss = 182.9281463623047\n","Epoch [12/30] Batch [170/798]:                     loss = 153.0572967529297\n","Epoch [12/30] Batch [180/798]:                     loss = 101.63042449951172\n","Epoch [12/30] Batch [190/798]:                     loss = 206.07363891601562\n","Epoch [12/30] Batch [200/798]:                     loss = 261.1772766113281\n","Epoch [12/30] Batch [210/798]:                     loss = 358.82012939453125\n","Epoch [12/30] Batch [220/798]:                     loss = 392.5411071777344\n","Epoch [12/30] Batch [230/798]:                     loss = 860.3413696289062\n","Epoch [12/30] Batch [240/798]:                     loss = 350.6050109863281\n","Epoch [12/30] Batch [250/798]:                     loss = 473.25018310546875\n","Epoch [12/30] Batch [260/798]:                     loss = 343.9678955078125\n","Epoch [12/30] Batch [270/798]:                     loss = 208.90440368652344\n","Epoch [12/30] Batch [280/798]:                     loss = 221.41717529296875\n","Epoch [12/30] Batch [290/798]:                     loss = 122.87601470947266\n","Epoch [12/30] Batch [300/798]:                     loss = 142.45114135742188\n","Epoch [12/30] Batch [310/798]:                     loss = 147.09246826171875\n","Epoch [12/30] Batch [320/798]:                     loss = 166.58023071289062\n","Epoch [12/30] Batch [330/798]:                     loss = 341.55206298828125\n","Epoch [12/30] Batch [340/798]:                     loss = 814.5575561523438\n","Epoch [12/30] Batch [350/798]:                     loss = 663.41650390625\n","Epoch [12/30] Batch [360/798]:                     loss = 946.7432861328125\n","Epoch [12/30] Batch [370/798]:                     loss = 1150.424560546875\n","Epoch [12/30] Batch [380/798]:                     loss = 667.5381469726562\n","Epoch [12/30] Batch [390/798]:                     loss = 1301.8018798828125\n","Epoch [12/30] Batch [400/798]:                     loss = 846.8857421875\n","Epoch [12/30] Batch [410/798]:                     loss = 425.47991943359375\n","Epoch [12/30] Batch [420/798]:                     loss = 412.6133117675781\n","Epoch [12/30] Batch [430/798]:                     loss = 145.2686767578125\n","Epoch [12/30] Batch [440/798]:                     loss = 192.67945861816406\n","Epoch [12/30] Batch [450/798]:                     loss = 123.44488525390625\n","Epoch [12/30] Batch [460/798]:                     loss = 185.0019073486328\n","Epoch [12/30] Batch [470/798]:                     loss = 146.54820251464844\n","Epoch [12/30] Batch [480/798]:                     loss = 676.2127075195312\n","Epoch [12/30] Batch [490/798]:                     loss = 692.1904907226562\n","Epoch [12/30] Batch [500/798]:                     loss = 993.5579833984375\n","Epoch [12/30] Batch [510/798]:                     loss = 708.2725830078125\n","Epoch [12/30] Batch [520/798]:                     loss = 1053.7213134765625\n","Epoch [12/30] Batch [530/798]:                     loss = 613.9555053710938\n","Epoch [12/30] Batch [540/798]:                     loss = 886.69091796875\n","Epoch [12/30] Batch [550/798]:                     loss = 427.6876220703125\n","Epoch [12/30] Batch [560/798]:                     loss = 372.37225341796875\n","Epoch [12/30] Batch [570/798]:                     loss = 242.75912475585938\n","Epoch [12/30] Batch [580/798]:                     loss = 206.4138641357422\n","Epoch [12/30] Batch [590/798]:                     loss = 145.5215301513672\n","Epoch [12/30] Batch [600/798]:                     loss = 132.57872009277344\n","Epoch [12/30] Batch [610/798]:                     loss = 162.6820831298828\n","Epoch [12/30] Batch [620/798]:                     loss = 142.57972717285156\n","Epoch [12/30] Batch [630/798]:                     loss = 165.9143524169922\n","Epoch [12/30] Batch [640/798]:                     loss = 311.9679260253906\n","Epoch [12/30] Batch [650/798]:                     loss = 597.2156372070312\n","Epoch [12/30] Batch [660/798]:                     loss = 322.7179870605469\n","Epoch [12/30] Batch [670/798]:                     loss = 378.6758728027344\n","Epoch [12/30] Batch [680/798]:                     loss = 516.27392578125\n","Epoch [12/30] Batch [690/798]:                     loss = 182.1123504638672\n","Epoch [12/30] Batch [700/798]:                     loss = 225.80389404296875\n","Epoch [12/30] Batch [710/798]:                     loss = 224.78067016601562\n","Epoch [12/30] Batch [720/798]:                     loss = 180.307373046875\n","Epoch [12/30] Batch [730/798]:                     loss = 139.85003662109375\n","Epoch [12/30] Batch [740/798]:                     loss = 116.26634979248047\n","Epoch [12/30] Batch [750/798]:                     loss = 146.18316650390625\n","Epoch [12/30] Batch [760/798]:                     loss = 219.20870971679688\n","Epoch [12/30] Batch [770/798]:                     loss = 673.9437255859375\n","Epoch [12/30] Batch [780/798]:                     loss = 849.3872680664062\n","Epoch [12/30] Batch [790/798]:                     loss = 959.63037109375\n","Epoch [12/30]: training loss= 449.372884, training mse loss= 449.372884, training bce loss= nan\n","             : validation loss= 407.634424, validation mse loss= 407.634424, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [13/30] Batch [10/798]:                     loss = 283.5098876953125\n","Epoch [13/30] Batch [20/798]:                     loss = 193.83546447753906\n","Epoch [13/30] Batch [30/798]:                     loss = 182.51661682128906\n","Epoch [13/30] Batch [40/798]:                     loss = 152.33424377441406\n","Epoch [13/30] Batch [50/798]:                     loss = 245.02667236328125\n","Epoch [13/30] Batch [60/798]:                     loss = 372.18719482421875\n","Epoch [13/30] Batch [70/798]:                     loss = 789.6506958007812\n","Epoch [13/30] Batch [80/798]:                     loss = 570.284423828125\n","Epoch [13/30] Batch [90/798]:                     loss = 507.865966796875\n","Epoch [13/30] Batch [100/798]:                     loss = 730.2058715820312\n","Epoch [13/30] Batch [110/798]:                     loss = 515.55126953125\n","Epoch [13/30] Batch [120/798]:                     loss = 447.26568603515625\n","Epoch [13/30] Batch [130/798]:                     loss = 247.30276489257812\n","Epoch [13/30] Batch [140/798]:                     loss = 287.0340576171875\n","Epoch [13/30] Batch [150/798]:                     loss = 205.34510803222656\n","Epoch [13/30] Batch [160/798]:                     loss = 183.03932189941406\n","Epoch [13/30] Batch [170/798]:                     loss = 154.42523193359375\n","Epoch [13/30] Batch [180/798]:                     loss = 107.54580688476562\n","Epoch [13/30] Batch [190/798]:                     loss = 204.86868286132812\n","Epoch [13/30] Batch [200/798]:                     loss = 259.6454772949219\n","Epoch [13/30] Batch [210/798]:                     loss = 358.4053039550781\n","Epoch [13/30] Batch [220/798]:                     loss = 394.89581298828125\n","Epoch [13/30] Batch [230/798]:                     loss = 860.6980590820312\n","Epoch [13/30] Batch [240/798]:                     loss = 351.446044921875\n","Epoch [13/30] Batch [250/798]:                     loss = 473.0847473144531\n","Epoch [13/30] Batch [260/798]:                     loss = 344.6548767089844\n","Epoch [13/30] Batch [270/798]:                     loss = 209.14202880859375\n","Epoch [13/30] Batch [280/798]:                     loss = 221.53170776367188\n","Epoch [13/30] Batch [290/798]:                     loss = 127.62469482421875\n","Epoch [13/30] Batch [300/798]:                     loss = 148.17916870117188\n","Epoch [13/30] Batch [310/798]:                     loss = 152.06552124023438\n","Epoch [13/30] Batch [320/798]:                     loss = 168.0336151123047\n","Epoch [13/30] Batch [330/798]:                     loss = 340.58026123046875\n","Epoch [13/30] Batch [340/798]:                     loss = 806.39453125\n","Epoch [13/30] Batch [350/798]:                     loss = 673.7882080078125\n","Epoch [13/30] Batch [360/798]:                     loss = 952.0208129882812\n","Epoch [13/30] Batch [370/798]:                     loss = 1154.0389404296875\n","Epoch [13/30] Batch [380/798]:                     loss = 675.3300170898438\n","Epoch [13/30] Batch [390/798]:                     loss = 1305.724365234375\n","Epoch [13/30] Batch [400/798]:                     loss = 847.60302734375\n","Epoch [13/30] Batch [410/798]:                     loss = 426.58074951171875\n","Epoch [13/30] Batch [420/798]:                     loss = 413.8958740234375\n","Epoch [13/30] Batch [430/798]:                     loss = 145.9535675048828\n","Epoch [13/30] Batch [440/798]:                     loss = 195.13641357421875\n","Epoch [13/30] Batch [450/798]:                     loss = 127.37109375\n","Epoch [13/30] Batch [460/798]:                     loss = 188.06634521484375\n","Epoch [13/30] Batch [470/798]:                     loss = 145.062744140625\n","Epoch [13/30] Batch [480/798]:                     loss = 657.59814453125\n","Epoch [13/30] Batch [490/798]:                     loss = 691.5642700195312\n","Epoch [13/30] Batch [500/798]:                     loss = 987.3328857421875\n","Epoch [13/30] Batch [510/798]:                     loss = 710.4471435546875\n","Epoch [13/30] Batch [520/798]:                     loss = 1052.2503662109375\n","Epoch [13/30] Batch [530/798]:                     loss = 616.1689453125\n","Epoch [13/30] Batch [540/798]:                     loss = 885.644775390625\n","Epoch [13/30] Batch [550/798]:                     loss = 426.3162841796875\n","Epoch [13/30] Batch [560/798]:                     loss = 370.60772705078125\n","Epoch [13/30] Batch [570/798]:                     loss = 241.42156982421875\n","Epoch [13/30] Batch [580/798]:                     loss = 207.3003387451172\n","Epoch [13/30] Batch [590/798]:                     loss = 144.44775390625\n","Epoch [13/30] Batch [600/798]:                     loss = 130.64300537109375\n","Epoch [13/30] Batch [610/798]:                     loss = 158.93605041503906\n","Epoch [13/30] Batch [620/798]:                     loss = 144.77584838867188\n","Epoch [13/30] Batch [630/798]:                     loss = 168.21058654785156\n","Epoch [13/30] Batch [640/798]:                     loss = 310.3752746582031\n","Epoch [13/30] Batch [650/798]:                     loss = 598.7201538085938\n","Epoch [13/30] Batch [660/798]:                     loss = 326.81488037109375\n","Epoch [13/30] Batch [670/798]:                     loss = 378.0321350097656\n","Epoch [13/30] Batch [680/798]:                     loss = 516.0686645507812\n","Epoch [13/30] Batch [690/798]:                     loss = 188.9239959716797\n","Epoch [13/30] Batch [700/798]:                     loss = 231.01800537109375\n","Epoch [13/30] Batch [710/798]:                     loss = 233.1393585205078\n","Epoch [13/30] Batch [720/798]:                     loss = 179.0789031982422\n","Epoch [13/30] Batch [730/798]:                     loss = 141.15281677246094\n","Epoch [13/30] Batch [740/798]:                     loss = 119.01016998291016\n","Epoch [13/30] Batch [750/798]:                     loss = 145.78720092773438\n","Epoch [13/30] Batch [760/798]:                     loss = 216.3794708251953\n","Epoch [13/30] Batch [770/798]:                     loss = 671.0520629882812\n","Epoch [13/30] Batch [780/798]:                     loss = 848.589111328125\n","Epoch [13/30] Batch [790/798]:                     loss = 959.1324462890625\n","Epoch [13/30]: training loss= 450.622250, training mse loss= 450.622250, training bce loss= nan\n","             : validation loss= 402.819226, validation mse loss= 402.819226, validation bce loss= nan\n","Validation loss decreased: 404.3659 --> 402.8192. Saving model...\n","Epoch [14/30] Batch [10/798]:                     loss = 281.271728515625\n","Epoch [14/30] Batch [20/798]:                     loss = 193.79574584960938\n","Epoch [14/30] Batch [30/798]:                     loss = 178.78323364257812\n","Epoch [14/30] Batch [40/798]:                     loss = 149.39419555664062\n","Epoch [14/30] Batch [50/798]:                     loss = 242.95089721679688\n","Epoch [14/30] Batch [60/798]:                     loss = 371.03594970703125\n","Epoch [14/30] Batch [70/798]:                     loss = 789.1699829101562\n","Epoch [14/30] Batch [80/798]:                     loss = 568.5169067382812\n","Epoch [14/30] Batch [90/798]:                     loss = 506.2302551269531\n","Epoch [14/30] Batch [100/798]:                     loss = 732.0781860351562\n","Epoch [14/30] Batch [110/798]:                     loss = 518.9712524414062\n","Epoch [14/30] Batch [120/798]:                     loss = 447.01959228515625\n","Epoch [14/30] Batch [130/798]:                     loss = 246.05633544921875\n","Epoch [14/30] Batch [140/798]:                     loss = 285.3492736816406\n","Epoch [14/30] Batch [150/798]:                     loss = 202.78965759277344\n","Epoch [14/30] Batch [160/798]:                     loss = 181.98776245117188\n","Epoch [14/30] Batch [170/798]:                     loss = 152.573486328125\n","Epoch [14/30] Batch [180/798]:                     loss = 106.38172912597656\n","Epoch [14/30] Batch [190/798]:                     loss = 204.6457061767578\n","Epoch [14/30] Batch [200/798]:                     loss = 261.358642578125\n","Epoch [14/30] Batch [210/798]:                     loss = 358.9620056152344\n","Epoch [14/30] Batch [220/798]:                     loss = 392.80450439453125\n","Epoch [14/30] Batch [230/798]:                     loss = 859.9915771484375\n","Epoch [14/30] Batch [240/798]:                     loss = 352.20745849609375\n","Epoch [14/30] Batch [250/798]:                     loss = 474.8255920410156\n","Epoch [14/30] Batch [260/798]:                     loss = 346.7148132324219\n","Epoch [14/30] Batch [270/798]:                     loss = 211.82907104492188\n","Epoch [14/30] Batch [280/798]:                     loss = 223.99505615234375\n","Epoch [14/30] Batch [290/798]:                     loss = 124.95584106445312\n","Epoch [14/30] Batch [300/798]:                     loss = 144.0498504638672\n","Epoch [14/30] Batch [310/798]:                     loss = 149.2119903564453\n","Epoch [14/30] Batch [320/798]:                     loss = 167.59300231933594\n","Epoch [14/30] Batch [330/798]:                     loss = 344.1611022949219\n","Epoch [14/30] Batch [340/798]:                     loss = 825.4959106445312\n","Epoch [14/30] Batch [350/798]:                     loss = 668.8489990234375\n","Epoch [14/30] Batch [360/798]:                     loss = 952.6415405273438\n","Epoch [14/30] Batch [370/798]:                     loss = 1160.9375\n","Epoch [14/30] Batch [380/798]:                     loss = 669.9599609375\n","Epoch [14/30] Batch [390/798]:                     loss = 1304.497802734375\n","Epoch [14/30] Batch [400/798]:                     loss = 845.4647827148438\n","Epoch [14/30] Batch [410/798]:                     loss = 425.9197692871094\n","Epoch [14/30] Batch [420/798]:                     loss = 411.4718322753906\n","Epoch [14/30] Batch [430/798]:                     loss = 147.36737060546875\n","Epoch [14/30] Batch [440/798]:                     loss = 195.14907836914062\n","Epoch [14/30] Batch [450/798]:                     loss = 124.4246826171875\n","Epoch [14/30] Batch [460/798]:                     loss = 185.4926300048828\n","Epoch [14/30] Batch [470/798]:                     loss = 140.77774047851562\n","Epoch [14/30] Batch [480/798]:                     loss = 678.6727294921875\n","Epoch [14/30] Batch [490/798]:                     loss = 692.1876220703125\n","Epoch [14/30] Batch [500/798]:                     loss = 997.1085815429688\n","Epoch [14/30] Batch [510/798]:                     loss = 709.4561767578125\n","Epoch [14/30] Batch [520/798]:                     loss = 1055.742919921875\n","Epoch [14/30] Batch [530/798]:                     loss = 622.7015380859375\n","Epoch [14/30] Batch [540/798]:                     loss = 891.660400390625\n","Epoch [14/30] Batch [550/798]:                     loss = 429.1050109863281\n","Epoch [14/30] Batch [560/798]:                     loss = 377.4710388183594\n","Epoch [14/30] Batch [570/798]:                     loss = 249.3934326171875\n","Epoch [14/30] Batch [580/798]:                     loss = 214.12095642089844\n","Epoch [14/30] Batch [590/798]:                     loss = 150.5895233154297\n","Epoch [14/30] Batch [600/798]:                     loss = 137.2285919189453\n","Epoch [14/30] Batch [610/798]:                     loss = 168.40675354003906\n","Epoch [14/30] Batch [620/798]:                     loss = 145.1861572265625\n","Epoch [14/30] Batch [630/798]:                     loss = 165.49224853515625\n","Epoch [14/30] Batch [640/798]:                     loss = 311.55010986328125\n","Epoch [14/30] Batch [650/798]:                     loss = 601.6649780273438\n","Epoch [14/30] Batch [660/798]:                     loss = 328.33123779296875\n","Epoch [14/30] Batch [670/798]:                     loss = 381.5155029296875\n","Epoch [14/30] Batch [680/798]:                     loss = 517.5230102539062\n","Epoch [14/30] Batch [690/798]:                     loss = 186.32196044921875\n","Epoch [14/30] Batch [700/798]:                     loss = 229.34445190429688\n","Epoch [14/30] Batch [710/798]:                     loss = 232.26907348632812\n","Epoch [14/30] Batch [720/798]:                     loss = 178.0320587158203\n","Epoch [14/30] Batch [730/798]:                     loss = 143.13916015625\n","Epoch [14/30] Batch [740/798]:                     loss = 123.35771942138672\n","Epoch [14/30] Batch [750/798]:                     loss = 147.63644409179688\n","Epoch [14/30] Batch [760/798]:                     loss = 206.38832092285156\n","Epoch [14/30] Batch [770/798]:                     loss = 655.9462280273438\n","Epoch [14/30] Batch [780/798]:                     loss = 849.1412963867188\n","Epoch [14/30] Batch [790/798]:                     loss = 960.307861328125\n","Epoch [14/30]: training loss= 451.366669, training mse loss= 451.366669, training bce loss= nan\n","             : validation loss= 407.488544, validation mse loss= 407.488544, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [15/30] Batch [10/798]:                     loss = 289.6834411621094\n","Epoch [15/30] Batch [20/798]:                     loss = 201.3513641357422\n","Epoch [15/30] Batch [30/798]:                     loss = 183.9355010986328\n","Epoch [15/30] Batch [40/798]:                     loss = 151.40484619140625\n","Epoch [15/30] Batch [50/798]:                     loss = 239.09451293945312\n","Epoch [15/30] Batch [60/798]:                     loss = 369.6164245605469\n","Epoch [15/30] Batch [70/798]:                     loss = 790.1194458007812\n","Epoch [15/30] Batch [80/798]:                     loss = 571.4837646484375\n","Epoch [15/30] Batch [90/798]:                     loss = 508.5206604003906\n","Epoch [15/30] Batch [100/798]:                     loss = 732.9546508789062\n","Epoch [15/30] Batch [110/798]:                     loss = 518.4591064453125\n","Epoch [15/30] Batch [120/798]:                     loss = 446.4639587402344\n","Epoch [15/30] Batch [130/798]:                     loss = 244.66519165039062\n","Epoch [15/30] Batch [140/798]:                     loss = 283.9024658203125\n","Epoch [15/30] Batch [150/798]:                     loss = 203.3196258544922\n","Epoch [15/30] Batch [160/798]:                     loss = 184.31283569335938\n","Epoch [15/30] Batch [170/798]:                     loss = 155.8485870361328\n","Epoch [15/30] Batch [180/798]:                     loss = 112.53143310546875\n","Epoch [15/30] Batch [190/798]:                     loss = 211.7450408935547\n","Epoch [15/30] Batch [200/798]:                     loss = 270.2090148925781\n","Epoch [15/30] Batch [210/798]:                     loss = 367.5976867675781\n","Epoch [15/30] Batch [220/798]:                     loss = 402.54388427734375\n","Epoch [15/30] Batch [230/798]:                     loss = 865.8422241210938\n","Epoch [15/30] Batch [240/798]:                     loss = 356.4629211425781\n","Epoch [15/30] Batch [250/798]:                     loss = 475.6981506347656\n","Epoch [15/30] Batch [260/798]:                     loss = 346.3438415527344\n","Epoch [15/30] Batch [270/798]:                     loss = 211.62069702148438\n","Epoch [15/30] Batch [280/798]:                     loss = 224.30453491210938\n","Epoch [15/30] Batch [290/798]:                     loss = 126.82572174072266\n","Epoch [15/30] Batch [300/798]:                     loss = 146.33399963378906\n","Epoch [15/30] Batch [310/798]:                     loss = 150.83639526367188\n","Epoch [15/30] Batch [320/798]:                     loss = 169.25645446777344\n","Epoch [15/30] Batch [330/798]:                     loss = 346.5728454589844\n","Epoch [15/30] Batch [340/798]:                     loss = 823.009765625\n","Epoch [15/30] Batch [350/798]:                     loss = 665.982666015625\n","Epoch [15/30] Batch [360/798]:                     loss = 951.2258911132812\n","Epoch [15/30] Batch [370/798]:                     loss = 1159.3304443359375\n","Epoch [15/30] Batch [380/798]:                     loss = 670.5728759765625\n","Epoch [15/30] Batch [390/798]:                     loss = 1305.3336181640625\n","Epoch [15/30] Batch [400/798]:                     loss = 846.3414916992188\n","Epoch [15/30] Batch [410/798]:                     loss = 426.31488037109375\n","Epoch [15/30] Batch [420/798]:                     loss = 413.7672424316406\n","Epoch [15/30] Batch [430/798]:                     loss = 145.9799346923828\n","Epoch [15/30] Batch [440/798]:                     loss = 197.71002197265625\n","Epoch [15/30] Batch [450/798]:                     loss = 127.4035415649414\n","Epoch [15/30] Batch [460/798]:                     loss = 187.5244598388672\n","Epoch [15/30] Batch [470/798]:                     loss = 139.7359619140625\n","Epoch [15/30] Batch [480/798]:                     loss = 671.177490234375\n","Epoch [15/30] Batch [490/798]:                     loss = 693.0166015625\n","Epoch [15/30] Batch [500/798]:                     loss = 998.869873046875\n","Epoch [15/30] Batch [510/798]:                     loss = 711.7029418945312\n","Epoch [15/30] Batch [520/798]:                     loss = 1059.0653076171875\n","Epoch [15/30] Batch [530/798]:                     loss = 616.0621948242188\n","Epoch [15/30] Batch [540/798]:                     loss = 886.41259765625\n","Epoch [15/30] Batch [550/798]:                     loss = 426.4800720214844\n","Epoch [15/30] Batch [560/798]:                     loss = 371.72332763671875\n","Epoch [15/30] Batch [570/798]:                     loss = 243.019287109375\n","Epoch [15/30] Batch [580/798]:                     loss = 207.16326904296875\n","Epoch [15/30] Batch [590/798]:                     loss = 144.74404907226562\n","Epoch [15/30] Batch [600/798]:                     loss = 131.104248046875\n","Epoch [15/30] Batch [610/798]:                     loss = 159.98678588867188\n","Epoch [15/30] Batch [620/798]:                     loss = 145.2788848876953\n","Epoch [15/30] Batch [630/798]:                     loss = 171.68072509765625\n","Epoch [15/30] Batch [640/798]:                     loss = 312.7158203125\n","Epoch [15/30] Batch [650/798]:                     loss = 600.1285400390625\n","Epoch [15/30] Batch [660/798]:                     loss = 324.94342041015625\n","Epoch [15/30] Batch [670/798]:                     loss = 379.3093566894531\n","Epoch [15/30] Batch [680/798]:                     loss = 515.6741333007812\n","Epoch [15/30] Batch [690/798]:                     loss = 183.0924530029297\n","Epoch [15/30] Batch [700/798]:                     loss = 227.26693725585938\n","Epoch [15/30] Batch [710/798]:                     loss = 228.26243591308594\n","Epoch [15/30] Batch [720/798]:                     loss = 178.096923828125\n","Epoch [15/30] Batch [730/798]:                     loss = 140.89376831054688\n","Epoch [15/30] Batch [740/798]:                     loss = 117.24781799316406\n","Epoch [15/30] Batch [750/798]:                     loss = 145.69407653808594\n","Epoch [15/30] Batch [760/798]:                     loss = 215.45010375976562\n","Epoch [15/30] Batch [770/798]:                     loss = 679.3495483398438\n","Epoch [15/30] Batch [780/798]:                     loss = 845.9517211914062\n","Epoch [15/30] Batch [790/798]:                     loss = 961.23828125\n","Epoch [15/30]: training loss= 451.934901, training mse loss= 451.934901, training bce loss= nan\n","             : validation loss= 404.403262, validation mse loss= 404.403262, validation bce loss= nan\n","Early stopping counter 2/5\n","Epoch [16/30] Batch [10/798]:                     loss = 292.7507019042969\n","Epoch [16/30] Batch [20/798]:                     loss = 201.68716430664062\n","Epoch [16/30] Batch [30/798]:                     loss = 182.552978515625\n","Epoch [16/30] Batch [40/798]:                     loss = 150.40992736816406\n","Epoch [16/30] Batch [50/798]:                     loss = 239.86756896972656\n","Epoch [16/30] Batch [60/798]:                     loss = 372.5542297363281\n","Epoch [16/30] Batch [70/798]:                     loss = 792.494140625\n","Epoch [16/30] Batch [80/798]:                     loss = 574.4007568359375\n","Epoch [16/30] Batch [90/798]:                     loss = 509.0950622558594\n","Epoch [16/30] Batch [100/798]:                     loss = 734.4109497070312\n","Epoch [16/30] Batch [110/798]:                     loss = 519.1803588867188\n","Epoch [16/30] Batch [120/798]:                     loss = 448.15423583984375\n","Epoch [16/30] Batch [130/798]:                     loss = 247.0670928955078\n","Epoch [16/30] Batch [140/798]:                     loss = 284.5683288574219\n","Epoch [16/30] Batch [150/798]:                     loss = 203.17822265625\n","Epoch [16/30] Batch [160/798]:                     loss = 183.08485412597656\n","Epoch [16/30] Batch [170/798]:                     loss = 154.14767456054688\n","Epoch [16/30] Batch [180/798]:                     loss = 107.35505676269531\n","Epoch [16/30] Batch [190/798]:                     loss = 206.2461395263672\n","Epoch [16/30] Batch [200/798]:                     loss = 260.75433349609375\n","Epoch [16/30] Batch [210/798]:                     loss = 358.436767578125\n","Epoch [16/30] Batch [220/798]:                     loss = 393.2958068847656\n","Epoch [16/30] Batch [230/798]:                     loss = 861.4490966796875\n","Epoch [16/30] Batch [240/798]:                     loss = 351.56494140625\n","Epoch [16/30] Batch [250/798]:                     loss = 474.2401428222656\n","Epoch [16/30] Batch [260/798]:                     loss = 345.81768798828125\n","Epoch [16/30] Batch [270/798]:                     loss = 210.62985229492188\n","Epoch [16/30] Batch [280/798]:                     loss = 222.52554321289062\n","Epoch [16/30] Batch [290/798]:                     loss = 123.22581481933594\n","Epoch [16/30] Batch [300/798]:                     loss = 141.90798950195312\n","Epoch [16/30] Batch [310/798]:                     loss = 146.94309997558594\n","Epoch [16/30] Batch [320/798]:                     loss = 165.13790893554688\n","Epoch [16/30] Batch [330/798]:                     loss = 338.45831298828125\n","Epoch [16/30] Batch [340/798]:                     loss = 816.8206787109375\n","Epoch [16/30] Batch [350/798]:                     loss = 660.4301147460938\n","Epoch [16/30] Batch [360/798]:                     loss = 946.257080078125\n","Epoch [16/30] Batch [370/798]:                     loss = 1152.9180908203125\n","Epoch [16/30] Batch [380/798]:                     loss = 664.9244384765625\n","Epoch [16/30] Batch [390/798]:                     loss = 1301.621337890625\n","Epoch [16/30] Batch [400/798]:                     loss = 843.9696044921875\n","Epoch [16/30] Batch [410/798]:                     loss = 424.0518798828125\n","Epoch [16/30] Batch [420/798]:                     loss = 411.9906921386719\n","Epoch [16/30] Batch [430/798]:                     loss = 144.26113891601562\n","Epoch [16/30] Batch [440/798]:                     loss = 194.44493103027344\n","Epoch [16/30] Batch [450/798]:                     loss = 124.78038787841797\n","Epoch [16/30] Batch [460/798]:                     loss = 184.95236206054688\n","Epoch [16/30] Batch [470/798]:                     loss = 139.79388427734375\n","Epoch [16/30] Batch [480/798]:                     loss = 672.8432006835938\n","Epoch [16/30] Batch [490/798]:                     loss = 689.6232299804688\n","Epoch [16/30] Batch [500/798]:                     loss = 995.5115356445312\n","Epoch [16/30] Batch [510/798]:                     loss = 708.5471801757812\n","Epoch [16/30] Batch [520/798]:                     loss = 1056.105224609375\n","Epoch [16/30] Batch [530/798]:                     loss = 612.7194213867188\n","Epoch [16/30] Batch [540/798]:                     loss = 882.0130615234375\n","Epoch [16/30] Batch [550/798]:                     loss = 422.6108093261719\n","Epoch [16/30] Batch [560/798]:                     loss = 369.7822570800781\n","Epoch [16/30] Batch [570/798]:                     loss = 241.2456512451172\n","Epoch [16/30] Batch [580/798]:                     loss = 207.46630859375\n","Epoch [16/30] Batch [590/798]:                     loss = 144.2183837890625\n","Epoch [16/30] Batch [600/798]:                     loss = 131.4170684814453\n","Epoch [16/30] Batch [610/798]:                     loss = 163.1940460205078\n","Epoch [16/30] Batch [620/798]:                     loss = 140.84605407714844\n","Epoch [16/30] Batch [630/798]:                     loss = 162.139892578125\n","Epoch [16/30] Batch [640/798]:                     loss = 308.1951904296875\n","Epoch [16/30] Batch [650/798]:                     loss = 596.1285400390625\n","Epoch [16/30] Batch [660/798]:                     loss = 322.9297180175781\n","Epoch [16/30] Batch [670/798]:                     loss = 377.9898986816406\n","Epoch [16/30] Batch [680/798]:                     loss = 517.3935546875\n","Epoch [16/30] Batch [690/798]:                     loss = 186.0341339111328\n","Epoch [16/30] Batch [700/798]:                     loss = 227.35733032226562\n","Epoch [16/30] Batch [710/798]:                     loss = 229.11756896972656\n","Epoch [16/30] Batch [720/798]:                     loss = 177.4922332763672\n","Epoch [16/30] Batch [730/798]:                     loss = 140.40457153320312\n","Epoch [16/30] Batch [740/798]:                     loss = 119.36823272705078\n","Epoch [16/30] Batch [750/798]:                     loss = 144.08840942382812\n","Epoch [16/30] Batch [760/798]:                     loss = 208.44046020507812\n","Epoch [16/30] Batch [770/798]:                     loss = 670.9481201171875\n","Epoch [16/30] Batch [780/798]:                     loss = 847.14501953125\n","Epoch [16/30] Batch [790/798]:                     loss = 960.648681640625\n","Epoch [16/30]: training loss= 449.866345, training mse loss= 449.866345, training bce loss= nan\n","             : validation loss= 402.296435, validation mse loss= 402.296435, validation bce loss= nan\n","Validation loss decreased: 402.8192 --> 402.2964. Saving model...\n","Epoch [17/30] Batch [10/798]:                     loss = 279.92572021484375\n","Epoch [17/30] Batch [20/798]:                     loss = 193.3077392578125\n","Epoch [17/30] Batch [30/798]:                     loss = 180.68580627441406\n","Epoch [17/30] Batch [40/798]:                     loss = 150.04981994628906\n","Epoch [17/30] Batch [50/798]:                     loss = 242.3872528076172\n","Epoch [17/30] Batch [60/798]:                     loss = 371.0851135253906\n","Epoch [17/30] Batch [70/798]:                     loss = 789.1583862304688\n","Epoch [17/30] Batch [80/798]:                     loss = 569.9422607421875\n","Epoch [17/30] Batch [90/798]:                     loss = 506.7940368652344\n","Epoch [17/30] Batch [100/798]:                     loss = 731.9662475585938\n","Epoch [17/30] Batch [110/798]:                     loss = 516.4800415039062\n","Epoch [17/30] Batch [120/798]:                     loss = 445.538330078125\n","Epoch [17/30] Batch [130/798]:                     loss = 242.52955627441406\n","Epoch [17/30] Batch [140/798]:                     loss = 280.4972229003906\n","Epoch [17/30] Batch [150/798]:                     loss = 199.9570770263672\n","Epoch [17/30] Batch [160/798]:                     loss = 179.8785858154297\n","Epoch [17/30] Batch [170/798]:                     loss = 152.18695068359375\n","Epoch [17/30] Batch [180/798]:                     loss = 107.29966735839844\n","Epoch [17/30] Batch [190/798]:                     loss = 202.7509765625\n","Epoch [17/30] Batch [200/798]:                     loss = 258.0302429199219\n","Epoch [17/30] Batch [210/798]:                     loss = 357.5827331542969\n","Epoch [17/30] Batch [220/798]:                     loss = 392.7093200683594\n","Epoch [17/30] Batch [230/798]:                     loss = 859.7266235351562\n","Epoch [17/30] Batch [240/798]:                     loss = 351.7781066894531\n","Epoch [17/30] Batch [250/798]:                     loss = 472.1649169921875\n","Epoch [17/30] Batch [260/798]:                     loss = 342.4894714355469\n","Epoch [17/30] Batch [270/798]:                     loss = 212.6540069580078\n","Epoch [17/30] Batch [280/798]:                     loss = 223.0142364501953\n","Epoch [17/30] Batch [290/798]:                     loss = 125.4790267944336\n","Epoch [17/30] Batch [300/798]:                     loss = 145.3704376220703\n","Epoch [17/30] Batch [310/798]:                     loss = 149.10020446777344\n","Epoch [17/30] Batch [320/798]:                     loss = 168.7080078125\n","Epoch [17/30] Batch [330/798]:                     loss = 347.73822021484375\n","Epoch [17/30] Batch [340/798]:                     loss = 823.0512084960938\n","Epoch [17/30] Batch [350/798]:                     loss = 663.58154296875\n","Epoch [17/30] Batch [360/798]:                     loss = 947.45068359375\n","Epoch [17/30] Batch [370/798]:                     loss = 1156.2391357421875\n","Epoch [17/30] Batch [380/798]:                     loss = 672.3950805664062\n","Epoch [17/30] Batch [390/798]:                     loss = 1321.683349609375\n","Epoch [17/30] Batch [400/798]:                     loss = 861.9268188476562\n","Epoch [17/30] Batch [410/798]:                     loss = 441.6028747558594\n","Epoch [17/30] Batch [420/798]:                     loss = 422.67657470703125\n","Epoch [17/30] Batch [430/798]:                     loss = 153.20736694335938\n","Epoch [17/30] Batch [440/798]:                     loss = 199.6847686767578\n","Epoch [17/30] Batch [450/798]:                     loss = 128.84629821777344\n","Epoch [17/30] Batch [460/798]:                     loss = 190.13490295410156\n","Epoch [17/30] Batch [470/798]:                     loss = 147.59051513671875\n","Epoch [17/30] Batch [480/798]:                     loss = 684.1967163085938\n","Epoch [17/30] Batch [490/798]:                     loss = 701.732177734375\n","Epoch [17/30] Batch [500/798]:                     loss = 1005.2967529296875\n","Epoch [17/30] Batch [510/798]:                     loss = 716.0210571289062\n","Epoch [17/30] Batch [520/798]:                     loss = 1062.7265625\n","Epoch [17/30] Batch [530/798]:                     loss = 619.4669189453125\n","Epoch [17/30] Batch [540/798]:                     loss = 888.93115234375\n","Epoch [17/30] Batch [550/798]:                     loss = 431.0574951171875\n","Epoch [17/30] Batch [560/798]:                     loss = 377.14398193359375\n","Epoch [17/30] Batch [570/798]:                     loss = 247.0478973388672\n","Epoch [17/30] Batch [580/798]:                     loss = 208.79144287109375\n","Epoch [17/30] Batch [590/798]:                     loss = 148.5647430419922\n","Epoch [17/30] Batch [600/798]:                     loss = 134.99386596679688\n","Epoch [17/30] Batch [610/798]:                     loss = 165.66552734375\n","Epoch [17/30] Batch [620/798]:                     loss = 145.01882934570312\n","Epoch [17/30] Batch [630/798]:                     loss = 168.6331787109375\n","Epoch [17/30] Batch [640/798]:                     loss = 313.0424499511719\n","Epoch [17/30] Batch [650/798]:                     loss = 600.2943725585938\n","Epoch [17/30] Batch [660/798]:                     loss = 325.88531494140625\n","Epoch [17/30] Batch [670/798]:                     loss = 380.9269714355469\n","Epoch [17/30] Batch [680/798]:                     loss = 517.8084106445312\n","Epoch [17/30] Batch [690/798]:                     loss = 184.868408203125\n","Epoch [17/30] Batch [700/798]:                     loss = 229.1885528564453\n","Epoch [17/30] Batch [710/798]:                     loss = 230.1765594482422\n","Epoch [17/30] Batch [720/798]:                     loss = 179.70094299316406\n","Epoch [17/30] Batch [730/798]:                     loss = 143.3187255859375\n","Epoch [17/30] Batch [740/798]:                     loss = 120.1758804321289\n","Epoch [17/30] Batch [750/798]:                     loss = 146.81300354003906\n","Epoch [17/30] Batch [760/798]:                     loss = 217.71209716796875\n","Epoch [17/30] Batch [770/798]:                     loss = 684.7669067382812\n","Epoch [17/30] Batch [780/798]:                     loss = 848.4215698242188\n","Epoch [17/30] Batch [790/798]:                     loss = 964.3488159179688\n","Epoch [17/30]: training loss= 452.189613, training mse loss= 452.189613, training bce loss= nan\n","             : validation loss= 404.431971, validation mse loss= 404.431971, validation bce loss= nan\n","Early stopping counter 1/5\n","Epoch [18/30] Batch [10/798]:                     loss = 281.8002624511719\n","Epoch [18/30] Batch [20/798]:                     loss = 194.09747314453125\n","Epoch [18/30] Batch [30/798]:                     loss = 182.20082092285156\n","Epoch [18/30] Batch [40/798]:                     loss = 153.2778778076172\n","Epoch [18/30] Batch [50/798]:                     loss = 246.03623962402344\n","Epoch [18/30] Batch [60/798]:                     loss = 375.56329345703125\n","Epoch [18/30] Batch [70/798]:                     loss = 794.7653198242188\n","Epoch [18/30] Batch [80/798]:                     loss = 574.0118408203125\n","Epoch [18/30] Batch [90/798]:                     loss = 509.88470458984375\n","Epoch [18/30] Batch [100/798]:                     loss = 733.673828125\n","Epoch [18/30] Batch [110/798]:                     loss = 516.2446899414062\n","Epoch [18/30] Batch [120/798]:                     loss = 446.103515625\n","Epoch [18/30] Batch [130/798]:                     loss = 245.1418914794922\n","Epoch [18/30] Batch [140/798]:                     loss = 282.337158203125\n","Epoch [18/30] Batch [150/798]:                     loss = 202.761474609375\n","Epoch [18/30] Batch [160/798]:                     loss = 182.1648406982422\n","Epoch [18/30] Batch [170/798]:                     loss = 153.6060333251953\n","Epoch [18/30] Batch [180/798]:                     loss = 106.16542053222656\n","Epoch [18/30] Batch [190/798]:                     loss = 204.47317504882812\n","Epoch [18/30] Batch [200/798]:                     loss = 260.38629150390625\n","Epoch [18/30] Batch [210/798]:                     loss = 358.9813232421875\n","Epoch [18/30] Batch [220/798]:                     loss = 395.83135986328125\n","Epoch [18/30] Batch [230/798]:                     loss = 860.7996826171875\n","Epoch [18/30] Batch [240/798]:                     loss = 352.94635009765625\n","Epoch [18/30] Batch [250/798]:                     loss = 473.6559753417969\n","Epoch [18/30] Batch [260/798]:                     loss = 343.2408447265625\n","Epoch [18/30] Batch [270/798]:                     loss = 210.22445678710938\n","Epoch [18/30] Batch [280/798]:                     loss = 222.41839599609375\n","Epoch [18/30] Batch [290/798]:                     loss = 125.04633331298828\n","Epoch [18/30] Batch [300/798]:                     loss = 144.491455078125\n","Epoch [18/30] Batch [310/798]:                     loss = 148.58047485351562\n","Epoch [18/30] Batch [320/798]:                     loss = 167.46969604492188\n","Epoch [18/30] Batch [330/798]:                     loss = 344.859375\n","Epoch [18/30] Batch [340/798]:                     loss = 823.9392700195312\n","Epoch [18/30] Batch [350/798]:                     loss = 663.5477294921875\n","Epoch [18/30] Batch [360/798]:                     loss = 949.391845703125\n","Epoch [18/30] Batch [370/798]:                     loss = 1157.08984375\n","Epoch [18/30] Batch [380/798]:                     loss = 668.5763549804688\n","Epoch [18/30] Batch [390/798]:                     loss = 1302.587646484375\n","Epoch [18/30] Batch [400/798]:                     loss = 844.1277465820312\n","Epoch [18/30] Batch [410/798]:                     loss = 427.1974792480469\n","Epoch [18/30] Batch [420/798]:                     loss = 413.1300354003906\n","Epoch [18/30] Batch [430/798]:                     loss = 147.40151977539062\n","Epoch [18/30] Batch [440/798]:                     loss = 196.96881103515625\n","Epoch [18/30] Batch [450/798]:                     loss = 127.69570922851562\n","Epoch [18/30] Batch [460/798]:                     loss = 187.382568359375\n","Epoch [18/30] Batch [470/798]:                     loss = 142.80050659179688\n","Epoch [18/30] Batch [480/798]:                     loss = 676.1980590820312\n","Epoch [18/30] Batch [490/798]:                     loss = 693.6378784179688\n","Epoch [18/30] Batch [500/798]:                     loss = 999.8878173828125\n","Epoch [18/30] Batch [510/798]:                     loss = 714.0\n","Epoch [18/30] Batch [520/798]:                     loss = 1060.79638671875\n","Epoch [18/30] Batch [530/798]:                     loss = 618.9609985351562\n","Epoch [18/30] Batch [540/798]:                     loss = 889.5455322265625\n","Epoch [18/30] Batch [550/798]:                     loss = 422.01641845703125\n","Epoch [18/30] Batch [560/798]:                     loss = 370.4998474121094\n","Epoch [18/30] Batch [570/798]:                     loss = 242.43011474609375\n","Epoch [18/30] Batch [580/798]:                     loss = 206.1028594970703\n","Epoch [18/30] Batch [590/798]:                     loss = 146.96176147460938\n","Epoch [18/30] Batch [600/798]:                     loss = 133.7098388671875\n","Epoch [18/30] Batch [610/798]:                     loss = 164.2859649658203\n","Epoch [18/30] Batch [620/798]:                     loss = 143.54058837890625\n","Epoch [18/30] Batch [630/798]:                     loss = 167.655029296875\n","Epoch [18/30] Batch [640/798]:                     loss = 312.4539489746094\n","Epoch [18/30] Batch [650/798]:                     loss = 599.0601806640625\n","Epoch [18/30] Batch [660/798]:                     loss = 326.1566467285156\n","Epoch [18/30] Batch [670/798]:                     loss = 386.9280700683594\n","Epoch [18/30] Batch [680/798]:                     loss = 528.75732421875\n","Epoch [18/30] Batch [690/798]:                     loss = 195.66758728027344\n","Epoch [18/30] Batch [700/798]:                     loss = 235.07659912109375\n","Epoch [18/30] Batch [710/798]:                     loss = 236.8064422607422\n","Epoch [18/30] Batch [720/798]:                     loss = 183.5894775390625\n","Epoch [18/30] Batch [730/798]:                     loss = 147.20950317382812\n","Epoch [18/30] Batch [740/798]:                     loss = 126.81710052490234\n","Epoch [18/30] Batch [750/798]:                     loss = 152.48851013183594\n","Epoch [18/30] Batch [760/798]:                     loss = 216.6683349609375\n","Epoch [18/30] Batch [770/798]:                     loss = 673.7395629882812\n","Epoch [18/30] Batch [780/798]:                     loss = 850.063720703125\n","Epoch [18/30] Batch [790/798]:                     loss = 963.9833374023438\n","Epoch [18/30]: training loss= 452.123960, training mse loss= 452.123960, training bce loss= nan\n","             : validation loss= 407.050190, validation mse loss= 407.050190, validation bce loss= nan\n","Early stopping counter 2/5\n","Epoch [19/30] Batch [10/798]:                     loss = 289.1956481933594\n","Epoch [19/30] Batch [20/798]:                     loss = 202.1465301513672\n","Epoch [19/30] Batch [30/798]:                     loss = 184.15106201171875\n","Epoch [19/30] Batch [40/798]:                     loss = 154.01060485839844\n","Epoch [19/30] Batch [50/798]:                     loss = 249.08358764648438\n","Epoch [19/30] Batch [60/798]:                     loss = 378.7373046875\n","Epoch [19/30] Batch [70/798]:                     loss = 801.6510620117188\n","Epoch [19/30] Batch [80/798]:                     loss = 579.0922241210938\n","Epoch [19/30] Batch [90/798]:                     loss = 515.3065185546875\n","Epoch [19/30] Batch [100/798]:                     loss = 738.5633544921875\n","Epoch [19/30] Batch [110/798]:                     loss = 523.07373046875\n","Epoch [19/30] Batch [120/798]:                     loss = 452.2738037109375\n","Epoch [19/30] Batch [130/798]:                     loss = 249.37672424316406\n","Epoch [19/30] Batch [140/798]:                     loss = 285.038818359375\n","Epoch [19/30] Batch [150/798]:                     loss = 204.93795776367188\n","Epoch [19/30] Batch [160/798]:                     loss = 185.56300354003906\n","Epoch [19/30] Batch [170/798]:                     loss = 157.6768798828125\n","Epoch [19/30] Batch [180/798]:                     loss = 111.14537048339844\n","Epoch [19/30] Batch [190/798]:                     loss = 209.8158721923828\n","Epoch [19/30] Batch [200/798]:                     loss = 264.6728515625\n","Epoch [19/30] Batch [210/798]:                     loss = 363.0628967285156\n","Epoch [19/30] Batch [220/798]:                     loss = 399.82275390625\n","Epoch [19/30] Batch [230/798]:                     loss = 865.5364379882812\n","Epoch [19/30] Batch [240/798]:                     loss = 358.33984375\n","Epoch [19/30] Batch [250/798]:                     loss = 479.1345520019531\n","Epoch [19/30] Batch [260/798]:                     loss = 348.0331726074219\n","Epoch [19/30] Batch [270/798]:                     loss = 214.47293090820312\n","Epoch [19/30] Batch [280/798]:                     loss = 225.28079223632812\n","Epoch [19/30] Batch [290/798]:                     loss = 128.40968322753906\n","Epoch [19/30] Batch [300/798]:                     loss = 148.54234313964844\n","Epoch [19/30] Batch [310/798]:                     loss = 153.2880401611328\n","Epoch [19/30] Batch [320/798]:                     loss = 172.3770751953125\n","Epoch [19/30] Batch [330/798]:                     loss = 351.2253112792969\n","Epoch [19/30] Batch [340/798]:                     loss = 826.985595703125\n","Epoch [19/30] Batch [350/798]:                     loss = 667.1390991210938\n","Epoch [19/30] Batch [360/798]:                     loss = 953.8513793945312\n","Epoch [19/30] Batch [370/798]:                     loss = 1160.51611328125\n","Epoch [19/30] Batch [380/798]:                     loss = 672.8585205078125\n","Epoch [19/30] Batch [390/798]:                     loss = 1307.7032470703125\n","Epoch [19/30] Batch [400/798]:                     loss = 851.7013549804688\n","Epoch [19/30] Batch [410/798]:                     loss = 432.0903015136719\n","Epoch [19/30] Batch [420/798]:                     loss = 418.6698303222656\n","Epoch [19/30] Batch [430/798]:                     loss = 151.29376220703125\n","Epoch [19/30] Batch [440/798]:                     loss = 199.53233337402344\n","Epoch [19/30] Batch [450/798]:                     loss = 131.0261688232422\n","Epoch [19/30] Batch [460/798]:                     loss = 191.55430603027344\n","Epoch [19/30] Batch [470/798]:                     loss = 145.7705078125\n","Epoch [19/30] Batch [480/798]:                     loss = 678.6981811523438\n","Epoch [19/30] Batch [490/798]:                     loss = 695.0537109375\n","Epoch [19/30] Batch [500/798]:                     loss = 1000.371337890625\n","Epoch [19/30] Batch [510/798]:                     loss = 715.6926879882812\n","Epoch [19/30] Batch [520/798]:                     loss = 1060.5872802734375\n","Epoch [19/30] Batch [530/798]:                     loss = 618.5702514648438\n","Epoch [19/30] Batch [540/798]:                     loss = 890.0847778320312\n","Epoch [19/30] Batch [550/798]:                     loss = 428.2039489746094\n","Epoch [19/30] Batch [560/798]:                     loss = 377.2572021484375\n","Epoch [19/30] Batch [570/798]:                     loss = 248.30979919433594\n","Epoch [19/30] Batch [580/798]:                     loss = 209.9132843017578\n","Epoch [19/30] Batch [590/798]:                     loss = 151.5764923095703\n","Epoch [19/30] Batch [600/798]:                     loss = 139.00279235839844\n","Epoch [19/30] Batch [610/798]:                     loss = 170.1060791015625\n","Epoch [19/30] Batch [620/798]:                     loss = 148.54730224609375\n","Epoch [19/30] Batch [630/798]:                     loss = 171.665771484375\n","Epoch [19/30] Batch [640/798]:                     loss = 315.11676025390625\n","Epoch [19/30] Batch [650/798]:                     loss = 602.974853515625\n","Epoch [19/30] Batch [660/798]:                     loss = 329.307373046875\n","Epoch [19/30] Batch [670/798]:                     loss = 384.4413146972656\n","Epoch [19/30] Batch [680/798]:                     loss = 520.8413696289062\n","Epoch [19/30] Batch [690/798]:                     loss = 189.48081970214844\n","Epoch [19/30] Batch [700/798]:                     loss = 232.38270568847656\n","Epoch [19/30] Batch [710/798]:                     loss = 234.0330047607422\n","Epoch [19/30] Batch [720/798]:                     loss = 183.5256805419922\n","Epoch [19/30] Batch [730/798]:                     loss = 146.62403869628906\n","Epoch [19/30] Batch [740/798]:                     loss = 124.43248748779297\n","Epoch [19/30] Batch [750/798]:                     loss = 149.42694091796875\n","Epoch [19/30] Batch [760/798]:                     loss = 215.71665954589844\n","Epoch [19/30] Batch [770/798]:                     loss = 678.9666137695312\n","Epoch [19/30] Batch [780/798]:                     loss = 849.8897705078125\n","Epoch [19/30] Batch [790/798]:                     loss = 963.81396484375\n","Epoch [19/30]: training loss= 455.253059, training mse loss= 455.253059, training bce loss= nan\n","             : validation loss= 404.772367, validation mse loss= 404.772367, validation bce loss= nan\n","Early stopping counter 3/5\n","Epoch [20/30] Batch [10/798]:                     loss = 283.744140625\n","Epoch [20/30] Batch [20/798]:                     loss = 197.91561889648438\n","Epoch [20/30] Batch [30/798]:                     loss = 182.89308166503906\n","Epoch [20/30] Batch [40/798]:                     loss = 152.2296142578125\n","Epoch [20/30] Batch [50/798]:                     loss = 244.077392578125\n","Epoch [20/30] Batch [60/798]:                     loss = 374.1597900390625\n","Epoch [20/30] Batch [70/798]:                     loss = 791.8052978515625\n","Epoch [20/30] Batch [80/798]:                     loss = 572.5111083984375\n","Epoch [20/30] Batch [90/798]:                     loss = 510.2222900390625\n","Epoch [20/30] Batch [100/798]:                     loss = 733.6524658203125\n","Epoch [20/30] Batch [110/798]:                     loss = 518.686767578125\n","Epoch [20/30] Batch [120/798]:                     loss = 448.57940673828125\n","Epoch [20/30] Batch [130/798]:                     loss = 252.23594665527344\n","Epoch [20/30] Batch [140/798]:                     loss = 298.9403381347656\n","Epoch [20/30] Batch [150/798]:                     loss = 215.80685424804688\n","Epoch [20/30] Batch [160/798]:                     loss = 195.308349609375\n","Epoch [20/30] Batch [170/798]:                     loss = 166.94998168945312\n","Epoch [20/30] Batch [180/798]:                     loss = 125.3196029663086\n","Epoch [20/30] Batch [190/798]:                     loss = 212.32725524902344\n","Epoch [20/30] Batch [200/798]:                     loss = 268.1051330566406\n","Epoch [20/30] Batch [210/798]:                     loss = 371.7485656738281\n","Epoch [20/30] Batch [220/798]:                     loss = 409.673828125\n","Epoch [20/30] Batch [230/798]:                     loss = 876.7733154296875\n","Epoch [20/30] Batch [240/798]:                     loss = 367.6177062988281\n","Epoch [20/30] Batch [250/798]:                     loss = 487.875732421875\n","Epoch [20/30] Batch [260/798]:                     loss = 359.26751708984375\n","Epoch [20/30] Batch [270/798]:                     loss = 227.3502655029297\n","Epoch [20/30] Batch [280/798]:                     loss = 236.77378845214844\n","Epoch [20/30] Batch [290/798]:                     loss = 139.33961486816406\n","Epoch [20/30] Batch [300/798]:                     loss = 157.0164337158203\n","Epoch [20/30] Batch [310/798]:                     loss = 161.21173095703125\n","Epoch [20/30] Batch [320/798]:                     loss = 180.1392364501953\n","Epoch [20/30] Batch [330/798]:                     loss = 354.3627624511719\n","Epoch [20/30] Batch [340/798]:                     loss = 838.6991577148438\n","Epoch [20/30] Batch [350/798]:                     loss = 673.683837890625\n","Epoch [20/30] Batch [360/798]:                     loss = 962.4091796875\n","Epoch [20/30] Batch [370/798]:                     loss = 1168.593505859375\n","Epoch [20/30] Batch [380/798]:                     loss = 681.7123413085938\n","Epoch [20/30] Batch [390/798]:                     loss = 1315.3326416015625\n","Epoch [20/30] Batch [400/798]:                     loss = 860.6510620117188\n","Epoch [20/30] Batch [410/798]:                     loss = 440.37603759765625\n","Epoch [20/30] Batch [420/798]:                     loss = 427.936767578125\n","Epoch [20/30] Batch [430/798]:                     loss = 156.16635131835938\n","Epoch [20/30] Batch [440/798]:                     loss = 207.07583618164062\n","Epoch [20/30] Batch [450/798]:                     loss = 137.3080596923828\n","Epoch [20/30] Batch [460/798]:                     loss = 197.3041534423828\n","Epoch [20/30] Batch [470/798]:                     loss = 149.27113342285156\n","Epoch [20/30] Batch [480/798]:                     loss = 680.7431640625\n","Epoch [20/30] Batch [490/798]:                     loss = 699.3927612304688\n","Epoch [20/30] Batch [500/798]:                     loss = 1005.4373168945312\n","Epoch [20/30] Batch [510/798]:                     loss = 718.4383544921875\n","Epoch [20/30] Batch [520/798]:                     loss = 1066.1875\n","Epoch [20/30] Batch [530/798]:                     loss = 623.638671875\n","Epoch [20/30] Batch [540/798]:                     loss = 894.711181640625\n","Epoch [20/30] Batch [550/798]:                     loss = 429.4437561035156\n","Epoch [20/30] Batch [560/798]:                     loss = 379.0195617675781\n","Epoch [20/30] Batch [570/798]:                     loss = 251.05003356933594\n","Epoch [20/30] Batch [580/798]:                     loss = 213.25013732910156\n","Epoch [20/30] Batch [590/798]:                     loss = 154.348876953125\n","Epoch [20/30] Batch [600/798]:                     loss = 141.3271942138672\n","Epoch [20/30] Batch [610/798]:                     loss = 174.3179168701172\n","Epoch [20/30] Batch [620/798]:                     loss = 150.8621826171875\n","Epoch [20/30] Batch [630/798]:                     loss = 174.3671875\n","Epoch [20/30] Batch [640/798]:                     loss = 317.2149963378906\n","Epoch [20/30] Batch [650/798]:                     loss = 606.3978881835938\n","Epoch [20/30] Batch [660/798]:                     loss = 331.6456298828125\n","Epoch [20/30] Batch [670/798]:                     loss = 387.0048522949219\n","Epoch [20/30] Batch [680/798]:                     loss = 523.8784790039062\n","Epoch [20/30] Batch [690/798]:                     loss = 192.4136505126953\n","Epoch [20/30] Batch [700/798]:                     loss = 235.47857666015625\n","Epoch [20/30] Batch [710/798]:                     loss = 236.97242736816406\n","Epoch [20/30] Batch [720/798]:                     loss = 185.57901000976562\n","Epoch [20/30] Batch [730/798]:                     loss = 149.86862182617188\n","Epoch [20/30] Batch [740/798]:                     loss = 126.80255126953125\n","Epoch [20/30] Batch [750/798]:                     loss = 152.22671508789062\n","Epoch [20/30] Batch [760/798]:                     loss = 220.34628295898438\n","Epoch [20/30] Batch [770/798]:                     loss = 682.7159423828125\n","Epoch [20/30] Batch [780/798]:                     loss = 850.7499389648438\n","Epoch [20/30] Batch [790/798]:                     loss = 965.4596557617188\n","Epoch [20/30]: training loss= 459.285817, training mse loss= 459.285817, training bce loss= nan\n","             : validation loss= 406.691219, validation mse loss= 406.691219, validation bce loss= nan\n","Early stopping counter 4/5\n","Epoch [21/30] Batch [10/798]:                     loss = 285.96405029296875\n","Epoch [21/30] Batch [20/798]:                     loss = 200.3097381591797\n","Epoch [21/30] Batch [30/798]:                     loss = 185.12225341796875\n","Epoch [21/30] Batch [40/798]:                     loss = 155.8204803466797\n","Epoch [21/30] Batch [50/798]:                     loss = 248.05931091308594\n","Epoch [21/30] Batch [60/798]:                     loss = 376.4967956542969\n","Epoch [21/30] Batch [70/798]:                     loss = 793.7860107421875\n","Epoch [21/30] Batch [80/798]:                     loss = 574.9888916015625\n","Epoch [21/30] Batch [90/798]:                     loss = 512.126953125\n","Epoch [21/30] Batch [100/798]:                     loss = 737.3876953125\n","Epoch [21/30] Batch [110/798]:                     loss = 521.5416259765625\n","Epoch [21/30] Batch [120/798]:                     loss = 451.65081787109375\n","Epoch [21/30] Batch [130/798]:                     loss = 250.1729736328125\n","Epoch [21/30] Batch [140/798]:                     loss = 287.3831481933594\n","Epoch [21/30] Batch [150/798]:                     loss = 207.36351013183594\n","Epoch [21/30] Batch [160/798]:                     loss = 189.96730041503906\n","Epoch [21/30] Batch [170/798]:                     loss = 165.20449829101562\n","Epoch [21/30] Batch [180/798]:                     loss = 124.25821685791016\n","Epoch [21/30] Batch [190/798]:                     loss = 212.00584411621094\n","Epoch [21/30] Batch [200/798]:                     loss = 264.4973449707031\n","Epoch [21/30] Batch [210/798]:                     loss = 366.7438049316406\n","Epoch [21/30] Batch [220/798]:                     loss = 401.3501892089844\n","Epoch [21/30] Batch [230/798]:                     loss = 868.3997802734375\n","Epoch [21/30] Batch [240/798]:                     loss = 359.90240478515625\n","Epoch [21/30] Batch [250/798]:                     loss = 482.1550598144531\n","Epoch [21/30] Batch [260/798]:                     loss = 353.3895263671875\n","Epoch [21/30] Batch [270/798]:                     loss = 220.79150390625\n","Epoch [21/30] Batch [280/798]:                     loss = 230.46173095703125\n","Epoch [21/30] Batch [290/798]:                     loss = 131.4925079345703\n","Epoch [21/30] Batch [300/798]:                     loss = 150.35435485839844\n","Epoch [21/30] Batch [310/798]:                     loss = 154.28489685058594\n","Epoch [21/30] Batch [320/798]:                     loss = 172.07369995117188\n","Epoch [21/30] Batch [330/798]:                     loss = 344.5657958984375\n","Epoch [21/30] Batch [340/798]:                     loss = 818.36083984375\n","Epoch [21/30] Batch [350/798]:                     loss = 666.7648315429688\n","Epoch [21/30] Batch [360/798]:                     loss = 949.93408203125\n","Epoch [21/30] Batch [370/798]:                     loss = 1154.7874755859375\n","Epoch [21/30] Batch [380/798]:                     loss = 672.1643676757812\n","Epoch [21/30] Batch [390/798]:                     loss = 1306.1593017578125\n","Epoch [21/30] Batch [400/798]:                     loss = 854.9213256835938\n","Epoch [21/30] Batch [410/798]:                     loss = 433.42633056640625\n","Epoch [21/30] Batch [420/798]:                     loss = 422.5973205566406\n","Epoch [21/30] Batch [430/798]:                     loss = 149.1453399658203\n","Epoch [21/30] Batch [440/798]:                     loss = 199.99435424804688\n","Epoch [21/30] Batch [450/798]:                     loss = 130.41107177734375\n","Epoch [21/30] Batch [460/798]:                     loss = 190.75038146972656\n","Epoch [21/30] Batch [470/798]:                     loss = 144.11880493164062\n","Epoch [21/30] Batch [480/798]:                     loss = 675.844482421875\n","Epoch [21/30] Batch [490/798]:                     loss = 693.6699829101562\n","Epoch [21/30] Batch [500/798]:                     loss = 999.0306396484375\n","Epoch [21/30] Batch [510/798]:                     loss = 712.8433837890625\n","Epoch [21/30] Batch [520/798]:                     loss = 1059.075927734375\n","Epoch [21/30] Batch [530/798]:                     loss = 618.0299072265625\n","Epoch [21/30] Batch [540/798]:                     loss = 889.748779296875\n","Epoch [21/30] Batch [550/798]:                     loss = 425.1768493652344\n","Epoch [21/30] Batch [560/798]:                     loss = 375.4144287109375\n","Epoch [21/30] Batch [570/798]:                     loss = 247.40919494628906\n","Epoch [21/30] Batch [580/798]:                     loss = 209.3306427001953\n","Epoch [21/30] Batch [590/798]:                     loss = 150.19058227539062\n","Epoch [21/30] Batch [600/798]:                     loss = 137.5918731689453\n","Epoch [21/30] Batch [610/798]:                     loss = 168.85208129882812\n","Epoch [21/30] Batch [620/798]:                     loss = 147.21237182617188\n","Epoch [21/30] Batch [630/798]:                     loss = 170.21531677246094\n","Epoch [21/30] Batch [640/798]:                     loss = 315.2048034667969\n","Epoch [21/30] Batch [650/798]:                     loss = 600.697509765625\n","Epoch [21/30] Batch [660/798]:                     loss = 327.90478515625\n","Epoch [21/30] Batch [670/798]:                     loss = 388.46490478515625\n","Epoch [21/30] Batch [680/798]:                     loss = 525.347900390625\n","Epoch [21/30] Batch [690/798]:                     loss = 190.94265747070312\n","Epoch [21/30] Batch [700/798]:                     loss = 236.04075622558594\n","Epoch [21/30] Batch [710/798]:                     loss = 240.9344940185547\n","Epoch [21/30] Batch [720/798]:                     loss = 186.81988525390625\n","Epoch [21/30] Batch [730/798]:                     loss = 154.8354949951172\n","Epoch [21/30] Batch [740/798]:                     loss = 129.56675720214844\n","Epoch [21/30] Batch [750/798]:                     loss = 153.82752990722656\n","Epoch [21/30] Batch [760/798]:                     loss = 219.1075439453125\n","Epoch [21/30] Batch [770/798]:                     loss = 675.5987548828125\n","Epoch [21/30] Batch [780/798]:                     loss = 853.5048828125\n","Epoch [21/30] Batch [790/798]:                     loss = 971.3157958984375\n","Epoch [21/30]: training loss= 455.832375, training mse loss= 455.832375, training bce loss= nan\n","             : validation loss= 409.989047, validation mse loss= 409.989047, validation bce loss= nan\n","Early stopping counter 5/5\n","Training done! Saving logs...\n","start evaluating...\n"]},{"output_type":"stream","name":"stderr","text":["/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n","/content/gdrvie/MyDrive/SKT/layers/nriLayers.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  soft_max_1d = F.softmax(trans_input)\n"]},{"output_type":"stream","name":"stdout","text":["{'r2_0': [0.8882805588878341], 'mae_0': [6.428371996901154], 'mse_0': [312.2692759932436], 'r2_std_0': [0.0002818646246640451], 'mae_std_0': [0.44523016580936353], 'te_mse_std_0': [10509.925725986517], 'r2_1': [0.8877035878865028], 'mae_1': [6.893153812011443], 'mse_1': [311.41612561256096], 'r2_std_1': [0.0003531381098438325], 'mae_std_1': [0.38104827377095807], 'te_mse_std_1': [9386.002321093332], 'r2_2': [0.886171665701003], 'mae_2': [7.352809549996216], 'mse_2': [318.09875453759105], 'r2_std_2': [0.00031670012511800987], 'mae_std_2': [0.38987253000845945], 'te_mse_std_2': [10649.577121642009], 'mean_fine_tunning_time': [nan], 'std_fine_tunning_time': [nan]}\n","saving the predictions...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 306/306 [07:58<00:00,  1.56s/it]\n","100%|██████████| 306/306 [07:51<00:00,  1.54s/it]\n","100%|██████████| 306/306 [08:01<00:00,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Test done!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#최종 r2,mse,mae 저장\n","for k, v in perf.items(): \n","    print(f'{k}: {v[0]:.4f}')\n","csv_file = os.path.join(args.model_path, 'perf.csv')\n","pd.DataFrame(perf).to_csv(csv_file, index= False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVXM3SHStRZy","executionInfo":{"status":"ok","timestamp":1666709165651,"user_tz":-540,"elapsed":150,"user":{"displayName":"Ki Di","userId":"01376303036382304863"}},"outputId":"f22d5513-a572-4e07-dde3-4dea3b934e0d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["r2_0: 0.8883\n","mae_0: 6.4284\n","mse_0: 312.2693\n","r2_std_0: 0.0003\n","mae_std_0: 0.4452\n","te_mse_std_0: 10509.9257\n","r2_1: 0.8877\n","mae_1: 6.8932\n","mse_1: 311.4161\n","r2_std_1: 0.0004\n","mae_std_1: 0.3810\n","te_mse_std_1: 9386.0023\n","r2_2: 0.8862\n","mae_2: 7.3528\n","mse_2: 318.0988\n","r2_std_2: 0.0003\n","mae_std_2: 0.3899\n","te_mse_std_2: 10649.5771\n","mean_fine_tunning_time: nan\n","std_fine_tunning_time: nan\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"W_qk_aTOP_3R"},"execution_count":null,"outputs":[]}]}